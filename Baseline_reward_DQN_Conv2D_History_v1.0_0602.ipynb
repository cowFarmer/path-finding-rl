{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"lRpfPDrdk9wB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654163718421,"user_tz":-540,"elapsed":16908,"user":{"displayName":"한태양","userId":"17014755298990153535"}},"outputId":"b65f6c49-edd0-4c2c-bcb7-f9bb0e597f80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","__file__ = '/content/drive/MyDrive/Colab_Notebooks/RL/path-finding-rl/data'"]},{"cell_type":"markdown","metadata":{"id":"UHFqTLgjMNbJ"},"source":["## Env\n","- 경로 이동 제한 사항 반영: del apply_action() 수정\n","- def grid_box(self):  # 그리드 박스 초기화 용도로 정의\n","- 그리드값 변경\n","  - target_gridval (목표물)  = 10, 9, 8, ...\n","  - curloc_gridval (현 위치) = 1\n","  - default_gridval (기본)   = 0\n","  - rack_gridval (선반)      = -1\n","  - obs_gridval (장애물)     = -10\n","- 종료 조건\n","  - **그리드월드 밖**으로 나가는 경우 **종료하지 않음**\n","  - **장애물**에 부딪히는 경우 **종료**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"tQRnyv3UlBcu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654164169936,"user_tz":-540,"elapsed":1321,"user":{"displayName":"한태양","userId":"17014755298990153535"}},"outputId":"33eb90cd-ff02-4607-ce20-45c26bb0af0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -1 -10 10\n"]}],"source":["from string import ascii_uppercase\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","###################################\n","# 보상 Reward\n","move_reward = -1  # 0.1\n","obs_reward = -10  # 0.1\n","goal_reward = 10  # 10\n","###################################\n","# 그리드값\n","# target_gridval = 10, 9, 8... # 목표물: 10, 9, 8, ...\n","curloc_gridval  = 1            # 현 위치: 1\n","default_gridval = 0            # 기본: 0\n","rack_gridval    = -1           # 선반: -1\n","obs_gridval     = -10          # 장애물: -10\n","###################################\n","# train / test 모드 지정\n","train_mode = True\n","###################################\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","class Simulator:\n","    def __init__(self):\n","        # Load train or test data\n","        #if train_mode:  # 훈련 데이타 읽기\n","        #    self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","        #    print('data/factory_order_train.csv used')\n","        #else:           # 테스트 데이터 읽기\n","        #    self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_test.csv\"))\n","        #    print('data/factory_order_test.csv used')\n","        ##########################################################################################\n","        self.height = 10  # 그리드 높이\n","        self.width = 9    #  그리드 너비\n","        self.inds = list(ascii_uppercase)[:17]  # A ~ Q alphabet list\n","\n","    def set_box(self):  # 선반 위치, 목적지 그리드값 설정. 목적지 리스트 생성\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = rack_gridval\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = rack_gridval\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = target_gridval\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","            self.local_target.append([getattr(order_box, \"row\"), getattr(order_box, \"col\")])  # 목적지 리스트 생성: local_target\n","        # self.local_target.sort() # 불필요. 인풋 데이터 A, B, C순 정렬되어 있음... 정렬 필요시 코드 바꾸어야 함\n","        self.local_target.append([9,4]) # 목적지 리스트에 최종 목적지(9,4) 추가\n","        self.local_target_original = self.local_target.copy()  # gif 생성을 위해 추가.  에피소드의 경로 저장\n","        self.target_length = len(self.local_target_original)  # 목적지 그리드값 10, 9, 8, ... 지정 위해 추가\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","\n","    def set_obstacle(self):  # 장애물 위치 그리드값 설정 = obs_gridval\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = obs_gridval\n","\n","    def grid_box(self):  # 그리드 박스 초기화 용도로 정의: 선반, 목적지, 장애물 그리드값 초기화\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = rack_gridval\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = rack_gridval\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = target_gridval\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","        self.set_obstacle()  # 장애물 위치 그리드값 설정 = obs_gridval\n","        \n","    def reset(self, epi):  # 에피소드 시작시 12개 값 초기화\n","        self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        ####################################################################################\n","        coin = random.randint(0,16)\n","        self.items = [chr(coin+65)]  # A~Q 까지 1개 랜덤으로 받아서 items에 1개 넣어주기, 반복 횟수는 main의 epochs 조절하기\n","        ####################################################################################\n","        self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        self.terminal_location = None                             #4. 최초 목적지\n","        self.local_target = []                                    #5. 목적지 리스트 초기화\n","        self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        self.grid = np.zeros((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(default_gridval) 초기화\n","        self.set_box()                                            #8. 선반 위치, 목적지 그리드값 설정. 목적지 리스트 생성\n","        self.set_obstacle()                                       #9. 장애물 그리드값 설정\n","        ######################\n","        # print('최초 그리드맵:')  # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = curloc_gridval  #11. 현재 위치(출발점) 그리드값 세팅\n","        self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        self.goal_ob_reward = False                      # (추가) #13. 최종 목적지 도착 여부 = False\n","        ######################\n","        # print('현 위치 추가:')   # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        \"\"\"\n","        writer : 휘정\n","        move_reward가 현재 +로 주어져 있음.\n","        이 경우, 움직이기만 해도 +보상을 받기 때문에 골을 찾으러 가지않고, 그냥 돌아다니는 경우 (내 실험상) 무조건 발생.\n","        이를 방지하기 위해, 이동했던 경로를 저장해두고, 새로운 이동좌표가 기존에 왔던 곳이면 받았던 보상을 뺏어버리는 식으로 학습시킴.\n","        \"\"\"\n","        # self.move_reward = 0                                        #14. 타겟위치에 따른 보상크기를 차등으로 주기위해, distance_reward값을 저장\n","        self.move_track = []                                        #15. 이동한 경로를 저장하는 변수\n","        self.isFinal =  False                                       #16. 처음으로 9,4에도착하면 moveTrack을 초기화해주기 위해 필요한 변수\n","        self.prior_distance = 99999999999999999                             # \n","        ######################\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):  # action에 따른 새 좌표값 반환\n","        new_x = cur_x\n","        new_y = cur_y\n","        if action == 0:          # up\n","            #new_x = cur_x - 1\n","            new_x = self.move_up(cur_x, cur_y, new_x, new_y)\n","        elif action == 1:        # down\n","            #new_x = cur_x + 1\n","            new_x = self.move_down(cur_x, cur_y, new_x, new_y)\n","        elif action == 2:        # left\n","            #new_y = cur_y - 1\n","            new_y = self.move_left(cur_x, cur_y, new_x, new_y)\n","        else:                    # right\n","            #new_y = cur_y + 1\n","            new_y = self.move_right(cur_x, cur_y, new_x, new_y)\n","        return int(new_x), int(new_y)\n","    \n","    # >>> 현재 위치에서 이동이 불가한 위치 추가 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def move_up(self, cur_x, cur_y, new_x, new_y):  # action == 0:\n","        if cur_x in [6,5,4,3,2] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x - 1\n","        return new_x\n","    def move_down(self, cur_x, cur_y, new_x, new_y): # action == 1:\n","        if cur_x in [1,2,3,4,5] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x + 1\n","        return new_x\n","    def move_left(self, cur_x, cur_y, new_x, new_y): # left elif action == 2:\n","        if cur_y in [1,2,3,4,5,6,7,8] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y - 1\n","        return new_y\n","    def move_right(self, cur_x, cur_y, new_x, new_y): # right else: action == 3:\n","        if cur_y in [0,1,2,3,4,5,6,7] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y + 1\n","        return new_y\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):  # action에 의해 이동시 얻는 보상\n","        if any(out_of_boundary):                                        # 바깥으로 나가는 경우\n","            reward = obs_reward\n","        else:\n","            if self.grid[new_x][new_y] in [rack_gridval, obs_gridval]:  # 선반이나 장애물에 부딪히는 경우\n","                reward = obs_reward\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  # 현재 목적지에 도달한 경우\n","                reward = goal_reward\n","            # 그냥 움직이는 경우 \n","            else: \n","                cur_distance = self.get_distance()\n","                if self.prior_distance > cur_distance :                         # 이전의 거리 > 현재의 거리 = 현재 아이템에 더 가깝다 => + 보상얻기\n","                    reward = self.get_positive_distance_reward(cur_distance)\n","                    # print('posi', reward)\n","                else:                                                           # 이전의 거리 < 현재의 거리 = 현재 아이템에서 멀어졌다 => - 보상얻기\n","                    reward = self.get_negative_distance_reward(cur_distance)\n","                    # print('nega', reward)\n","                self.prior_distance = cur_distance      \n","\n","                if [new_x, new_y] not in self.move_track:                           # 이동한 곳이 처음온 곳이면 move_track에 추가\n","                    self.move_track.append([new_x, new_y])\n","                else:                                                           # 이동한 곳이 이미 왔던 곳이면 기존 보상에서 -1\n","                    reward = reward -1\n","        return reward\n","\n","    def get_ending_reward(self, new_x, new_y, out_of_boundary):  # action에 의해 이동시 얻는 보상\n","        # print('loc_tar',self.local_target)\n","        # print('ter',self.terminal_location)\n","        # print('x y',new_x, new_y)\n","        # print(self.grid[new_x][new_y])\n","\n","        ending_reward = 1000\n","        if any(out_of_boundary):                          # 바깥으로 나가는 경우\n","            reward = obs_reward                               # -10점\n","            # print('바깥', obs_reward)\n","        else:\n","            if self.grid[new_x][new_y] in [-1, -10]:              # 장애물에 부딪히는 경우 혹은 빈상자에 간 경우\n","                reward = obs_reward                           # -10점\n","                self.grid[new_x][new_y] = curloc_gridval  # 현 위치(장애물)에서 종료되었음을 표시(1). 원래는 장애물(-10) 이었음\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  # 현재 목적지에 도달한 경우\n","                # self.move_track.append(self.curloc)\n","                reward = goal_reward\n","                if [new_x, new_y] ==[9,4]:\n","                    reward = ending_reward\n","                    print('FINISH ㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜㅜ')\n","            # 그냥 움직이는 경우 \n","            else:\n","                cur_distance = self.get_distance()\n","                if self.prior_distance > cur_distance :                         # 이전의 거리 > 현재의 거리 = 현재 아이템에 더 가깝다 => + 보상얻기\n","                    reward = self.get_positive_distance_reward(cur_distance)\n","                    # print('posi', reward)\n","                else:                                                           # 이전의 거리 < 현재의 거리 = 현재 아이템에서 멀어졌다 => - 보상얻기\n","                    reward = self.get_negative_distance_reward(cur_distance)\n","                    # print('nega', reward)\n","                self.prior_distance = cur_distance      \n","                \n","                if [new_x, new_y] not in self.move_track:                           # 이동한 곳이 처음온 곳이면 move_track에 추가\n","                    self.move_track.append([new_x, new_y])\n","                else:                                                           # 이동한 곳이 이미 왔던 곳이면 기존 보상에서 -1\n","                    reward = reward -1\n","        return reward\n","\n","    def step(self, action):  # action에 따른 이동 실행\n","        self.terminal_location = self.local_target[0]           # 목적지 리스트의 첫 번째 요소를 목적지로 설정\n","        cur_x, cur_y = self.curloc                              # 현재 위치 기억\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 다음 위치 받기\n","        self.actions.append((cur_x, cur_y))                     # 현재 위치(지나온 위치)를 경로 리스트에 추가\n","        self.goal_ob_reward = False                             # 최종 목적지에 도착한 경우에 True. self.reset()에서 초기화\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # 그리드월드 밖으로 나갔는가? OB = True\n","\n","        if any(out_of_boundary) or self.grid[new_x][new_y] == rack_gridval:  #1. 바깥으로 나가는 경우, 빈 선반에 부딪치는 경우 종료하지 않음\n","            #print('OB')\n","            reward = self.get_reward(new_x, new_y, out_of_boundary)\n","            new_x = cur_x  # 종료 처리할 경우 코멘트 처리\n","            new_y = cur_y  # 종료 처리할 경우 코멘트 처리\n","            ######################################################종료 처리할 경우 실행\n","            #self.done = True                             # while loop 종료\n","            #self.grid_box()                              # 선반, 목적지, 장애물 그리드값 초기화\n","            #self.actions.append((new_x,new_y))           # 경로 리스트에 최종 위치 추가. (주의) 좌표값이 그리드월드를 벗어나는 값 발생!\n","            ######################################################\n","        else:\n","            if self.grid[new_x][new_y] == obs_gridval:  #2. 장애물에 부딪히는 경우 종료\n","                # print('장애물')\n","                # reward = self.get_reward(new_x, new_y, out_of_boundary)\n","                # new_x = cur_x  # 종료 처리할 경우 코멘트 처리\n","                # new_y = cur_y  # 종료 처리할 경우 코멘트 처리\n","                ###################################################종료 처리할 경우 실행\n","                self.done = True                           # while loop 종료\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                # self.grid[new_x][new_y] = curloc_gridval   # 현 위치(장애물)에서 종료되었음을 표시. 원래는 장애물이었음\n","                self.actions.append((new_x,new_y))         # 경로 리스트에 최종 위치 추가\n","                ###################################################\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  #3. 현재 목적지에 도착한 경우 다음 목적지 설정\n","                #print('목적지 도착')\n","                reward = self.get_reward(new_x, new_y, out_of_boundary)\n","                if [new_x, new_y] == [9, 4]:            #3-1. 최종 목적지에 도착한 경우 종료\n","                    self.done = True                       # while loop 종료\n","                    self.goal_ob_reward = True             # True = 1 (OB, 장애물 종료 처리 할 경우 self.done=True가 많아지므로 이때 사용)\n","                    self.actions.append((new_x,new_y))     # 경로 리스트에 최종 위치 추가\n","\n","                self.local_target.remove(self.local_target[0])  # 다음 목적지 설정. 최종 목적지에 도착한 경우에는 마지막 요소인 [9,4]를  제거\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                # self.grid[new_x][new_y] = curloc_gridval   # 현 위치(목적지)에서 종료되었음을 표시. 원래는 목적지이었음\n","                self.curloc = [new_x, new_y]               # 새로 도착한 위치를 현재 위치로 변경\n","            else:                                       #4. 그냥 움직이는 경우\n","                #print('이동')\n","                # reward = self.get_reward(new_x, new_y, out_of_boundary)\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치에서 종료되었음을 표시\n","                self.curloc = [new_x,new_y]\n","\n","        ############################### 보상받는 처리 시작 ###############################\n","        # 마지막 위치에서 위로 올라가는 행동을 한다면 - 보상받음 태양\n","        if self.terminal_location == [9, 4] or len(self.local_target)==0:\n","            if not self.isFinal:\n","                # 마지막으로 갈 때는 이때까지 지나온 길 초기화\n","                self.move_track = []\n","                self.isFinal = True\n","            reward = self.get_ending_reward(new_x, new_y, out_of_boundary)\n","            if action==0:\n","            # if goal_ob_reward != 'goal' and action == 0:\n","                reward = obs_reward\n","        else : \n","            reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        ############################### 보상받는 처리 끝 ###############################            \n","\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done\n","\n","    # >>> distance를 입력으로 받아 + 보상을 계산하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< \n","    def get_positive_distance_reward(self, distance):\n","        positive_reward = (20 - distance) / 200\n","        return positive_reward\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    # >>> distance를 입력으로 받아 - 보상을 계산하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def get_negative_distance_reward(self, distance):\n","        negative_reward = -(distance / 200)\n","        return negative_reward\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    \n","\n","    # >>> 현재 위치에서 타겟까지의 거리를 구하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def get_distance(self): \n","        cur_x, cur_y, target_x, target_y = self.get_current_state()\n","        \n","        distance = (abs(cur_x-target_x) + abs(cur_y-target_y))\n","        # print(cur_x, cur_y, target_x, target_y)\n","        # print('distance : ', distance)\n","        return distance\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    \n","\n","    # >>> 현재 상황에 대한 정보를 구하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def get_current_state(self):\n","        if len(self.local_target) != 0:\n","            state = (self.curloc[0], self.curloc[1], self.local_target[0][0], self.local_target[0][1])\n","        else :\n","            state = (self.curloc[0], self.curloc[1], self.terminal_location[0], self.terminal_location[1])\n","        return np.array(state)\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<"]},{"cell_type":"markdown","metadata":{"id":"JfJoLw7BM2xw"},"source":["## Agent\n","- def test_action(self, obs): 추가"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"uwgy-OBbk1b4","executionInfo":{"status":"ok","timestamp":1654164172570,"user_tz":-540,"elapsed":540,"user":{"displayName":"한태양","userId":"17014755298990153535"}}},"outputs":[],"source":["import collections\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ReplayBuffer():  # 리플레이 버퍼: 경험 저장소\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","    \n","    def put(self, transition):  # 리플레이 버퍼(메모리)를 경험(transition)으로 채우기\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):  # memory.sample(batch_size)로 사용\n","        mini_batch = random.sample(self.buffer, n)  # 미니배치 샘플링\n","        '''\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","        for transition in mini_batch:  # 경험의 각 요소들을 각각의 미니배치로 구성\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","        '''\n","        state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in mini_batch])\n","        action_batch = torch.Tensor([a for (s1,a,r,s2,d) in mini_batch])\n","        reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in mini_batch])\n","        state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in mini_batch])\n","        done_batch = torch.Tensor([d for (s1,a,r,s2,d) in mini_batch])\n","\n","        return state1_batch, action_batch, reward_batch, state2_batch, done_batch\n","    \n","    def size(self):  # 메모리 크기\n","        return len(self.buffer)\n","\n","class Qnet(nn.Module):\n","    '''##### Linear 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        in_size = 90  # input 크기\n","        L1 = 250\n","        L2 = 150\n","        out_size  = 4\n","\n","        self.fc1 = nn.Linear(in_size, L1)\n","        self.fc2 = nn.Linear(L1, L2)\n","        self.fc3 = nn.Linear(L2, out_size)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","    '''\n","    ##### Convolution 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(4, 32, 3, 1) #### 1이었던 부분을 4로 변경\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n","        self.fc1 = nn.Linear(768, 512)\n","        self.fc2 = nn.Linear(512, 4)\n","\n","    def forward(self, x):\n","        x = x.to(device)               # (N, 4, 10, 9)\n","        x = F.relu(self.conv1(x))      # (N, 32, 8, 7)\n","        x = F.relu(self.conv2(x))      # (N, 64, 6, 5)\n","        x = F.relu(self.conv3(x))\n","        x = torch.flatten(x, 1)        # (N, 64, 30)\n","        x = F.relu(self.fc1(x))        # (N, 1920) -> (N, 128)\n","        x = self.fc2(x)                # (N, 128)  -> (N, 4)\n","        return x\n","\n","    def sample_action(self, obs, epsilon):  # Qnet()을 실행해서 모든 action에 대한 Qval을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        coin = random.random()  # e-greedy로 action 선택\n","        if coin < epsilon:\n","            return random.randint(0,3)\n","        else : \n","            return out.argmax().item()\n","    \n","    def test_action(self, obs):  # Qnet()을 실행해서 모든 action에 대한 Qvalue을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        return out.argmax().item()\n","\n","def train(q, q_target, memory, optimizer):  # 메모리에 경험이 미니배치 크기 이상 쌓이면 미니배치 훈련 시작\n","    '''\n","    for i in range(10):\n","        s, a, r, cr, s_prime, done_mask, gr = memory.sample(batch_size)\n","        q_out = q(s)\n","        q_a = q_out.gather(1,a.to(device))\n","        \n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","        target = r.to(device) + gamma * max_q_prime * done_mask.to(device)\n","        \n","        loss = F.smooth_l1_loss(q_a, target)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    '''\n","    s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 메모리에서 미니배치를 뽑기: batch_size=1600\n","    Q1 = q(s)  # Qnet의 Qvalue\n","    X = Q1.gather(dim=1,index=a.long().unsqueeze(dim=1).to(device)).squeeze() # Qnet의 Qvalue 변환\n","    with torch.no_grad():\n","        Q2 = q_target(s_prime)\n","    Y = r.to(device) + gamma * done_mask.to(device) * torch.max(Q2,dim=1)[0]\n","    loss = F.smooth_l1_loss(X, Y.detach())\n","    #loss = nn.MSELoss(X.to(torch.float32), Y.detach().to(torch.float32))  # 손실함수 변경. target.detach()로 변경\n","    optimizer.zero_grad()  # gradient값 초기화\n","    loss.backward()  # 자동미분\n","    optimizer.step()  # gradient 업데이트\n","    return loss.item()"]},{"cell_type":"markdown","metadata":{"id":"wK35ngIkNM6I"},"source":["## Main: DQN...\n","  - GPU를 사용할 경우 구현\n","  - 학습한 모델 불러올 수 있는 코드 추가\n","    - '#' 제거하고 실행시키면 됨"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"j9qFCqbbjf3F","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1654164204854,"user_tz":-540,"elapsed":11140,"user":{"displayName":"한태양","userId":"17014755298990153535"}},"outputId":"7662eb07-1f93-4dc3-f4ad-cbc5688e3bd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor in cpu\n","0 0 1 1 [8 4 4 8] 0 0.06 누적보상 :  0.06\n","0 0 2 2 [8 5 4 8] 3 0.065 누적보상 :  0.125\n","0 0 3 3 [7 5 4 8] 0 0.07 누적보상 :  0.195\n","0 0 4 4 [6 5 4 8] 0 0.075 누적보상 :  0.27\n","0 0 5 5 [6 5 4 8] 2 -10 누적보상 :  -9.73\n","0 1 6 1 [8 4 0 7] 0 0.045 누적보상 :  0.045\n","0 1 7 2 [8 3 0 7] 2 -0.06 누적보상 :  -0.015\n","0 1 8 3 [8 4 0 7] 3 -0.955 누적보상 :  -0.97\n","0 1 9 4 [8 5 0 7] 3 0.05 누적보상 :  -0.9199999999999999\n","0 1 10 5 [8 6 0 7] 3 0.055 누적보상 :  -0.8649999999999999\n","0 1 11 6 [7 6 0 7] 0 0.06 누적보상 :  -0.8049999999999999\n","0 1 12 7 [7 7 0 7] 3 0.065 누적보상 :  -0.74\n","0 1 13 8 [8 7 0 7] 1 -0.04 누적보상 :  -0.78\n","0 1 14 9 [8 7 0 7] 1 -10 누적보상 :  -10.78\n","0 2 15 1 [8 4 0 1] 0 0.045 누적보상 :  0.045\n","0 2 16 2 [7 4 0 1] 0 0.05 누적보상 :  0.095\n","0 2 17 3 [7 3 0 1] 2 0.055 누적보상 :  0.15\n","0 2 18 4 [6 3 0 1] 0 0.06 누적보상 :  0.21\n","0 2 19 5 [6 3 0 1] 2 -10 누적보상 :  -9.79\n","0 3 20 1 [8 4 0 8] 0 0.04 누적보상 :  0.04\n","0 3 21 2 [8 3 0 8] 2 -0.065 누적보상 :  -0.025\n","0 3 22 3 [8 2 0 8] 2 -0.07 누적보상 :  -0.095\n","0 3 23 4 [8 3 0 8] 3 -0.965 누적보상 :  -1.06\n","0 3 24 5 [8 2 0 8] 2 -1.07 누적보상 :  -2.13\n","0 3 25 6 [8 3 0 8] 3 -0.965 누적보상 :  -3.0949999999999998\n","0 3 26 7 [7 3 0 8] 0 0.04 누적보상 :  -3.0549999999999997\n","0 3 27 8 [7 4 0 8] 3 0.045 누적보상 :  -3.01\n","0 3 28 9 [8 4 0 8] 1 -1.06 누적보상 :  -4.07\n","0 3 29 10 [8 5 0 8] 3 0.045 누적보상 :  -4.025\n","0 3 30 11 [8 5 0 8] 1 -10 누적보상 :  -14.025\n","0 4 31 1 [8 4 0 3] 0 0.055 누적보상 :  0.055\n","0 4 32 2 [9 4 0 3] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 4 33 3 [8 4 0 3] 0 -0.945 누적보상 :  -0.94\n","0 4 34 4 [8 3 0 3] 2 0.06 누적보상 :  -0.8799999999999999\n","0 4 35 5 [8 4 0 3] 3 -1.045 누적보상 :  -1.9249999999999998\n","0 4 36 6 [7 4 0 3] 0 0.06 누적보상 :  -1.8649999999999998\n","0 4 37 7 [7 4 0 3] 0 -10 누적보상 :  -11.865\n","0 5 38 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 5 39 2 [9 4 3 0] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 5 40 3 [8 4 3 0] 0 -0.945 누적보상 :  -0.94\n","0 5 41 4 [8 5 3 0] 3 -0.05 누적보상 :  -0.99\n","0 5 42 5 [7 5 3 0] 0 0.055 누적보상 :  -0.9349999999999999\n","0 5 43 6 [7 6 3 0] 3 -0.05 누적보상 :  -0.985\n","0 5 44 7 [8 6 3 0] 1 -0.055 누적보상 :  -1.04\n","0 5 45 8 [8 7 3 0] 3 -0.06 누적보상 :  -1.1\n","0 5 46 9 [8 8 3 0] 3 -0.065 누적보상 :  -1.165\n","0 5 47 10 [8 7 3 0] 2 -0.96 누적보상 :  -2.125\n","0 5 48 11 [8 7 3 0] 1 -10 누적보상 :  -12.125\n","0 6 49 1 [8 4 0 2] 0 0.05 누적보상 :  0.05\n","0 6 50 2 [8 5 0 2] 3 -0.055 누적보상 :  -0.0049999999999999975\n","0 6 51 3 [8 4 0 2] 2 -0.95 누적보상 :  -0.955\n","0 6 52 4 [8 5 0 2] 3 -1.055 누적보상 :  -2.01\n","0 6 53 5 [7 5 0 2] 0 0.05 누적보상 :  -1.9599999999999997\n","0 6 54 6 [6 5 0 2] 0 0.055 누적보상 :  -1.9049999999999998\n","0 6 55 7 [6 5 0 2] 2 -10 누적보상 :  -11.905\n","0 7 56 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 7 57 2 [9 4 3 0] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 7 58 3 [8 4 3 0] 0 -0.945 누적보상 :  -0.94\n","0 7 59 4 [8 5 3 0] 3 -0.05 누적보상 :  -0.99\n","0 7 60 5 [8 4 3 0] 2 -0.945 누적보상 :  -1.935\n","0 7 61 6 [7 4 3 0] 0 0.06 누적보상 :  -1.875\n","0 7 62 7 [7 4 3 0] 0 -10 누적보상 :  -11.875\n","0 8 63 1 [8 4 5 0] 0 0.065 누적보상 :  0.065\n","0 8 64 2 [8 5 5 0] 3 -0.04 누적보상 :  0.025\n","0 8 65 3 [8 6 5 0] 3 -0.045 누적보상 :  -0.019999999999999997\n","0 8 66 4 [8 5 5 0] 2 -0.94 누적보상 :  -0.96\n","0 8 67 5 [8 5 5 0] 1 -10 누적보상 :  -10.96\n","0 9 68 1 [8 4 0 0] 0 0.04 누적보상 :  0.04\n","0 9 69 2 [8 5 0 0] 3 -0.065 누적보상 :  -0.025\n","0 9 70 3 [8 6 0 0] 3 -0.07 누적보상 :  -0.095\n","0 9 71 4 [8 5 0 0] 2 -0.965 누적보상 :  -1.06\n","0 9 72 5 [8 4 0 0] 2 -0.96 누적보상 :  -2.02\n","0 9 73 6 [7 4 0 0] 0 0.045 누적보상 :  -1.975\n","0 9 74 7 [7 5 0 0] 3 -0.06 누적보상 :  -2.035\n","0 9 75 8 [6 5 0 0] 0 0.045 누적보상 :  -1.9900000000000002\n","0 9 76 9 [6 5 0 0] 2 -10 누적보상 :  -11.99\n","0 10 77 1 [8 4 5 0] 0 0.065 누적보상 :  0.065\n","0 10 78 2 [8 5 5 0] 3 -0.04 누적보상 :  0.025\n","0 10 79 3 [7 5 5 0] 0 0.065 누적보상 :  0.09\n","0 10 80 4 [6 5 5 0] 0 0.07 누적보상 :  0.16\n","0 10 81 5 [7 5 5 0] 1 -1.035 누적보상 :  -0.8749999999999999\n","0 10 82 6 [6 5 5 0] 0 -0.9299999999999999 누적보상 :  -1.8049999999999997\n","0 10 83 7 [6 5 5 0] 2 -10 누적보상 :  -11.805\n","0 11 84 1 [8 4 0 0] 0 0.04 누적보상 :  0.04\n","0 11 85 2 [7 4 0 0] 0 0.045 누적보상 :  0.08499999999999999\n","0 11 86 3 [7 3 0 0] 2 0.05 누적보상 :  0.135\n","0 11 87 4 [7 2 0 0] 2 0.055 누적보상 :  0.19\n","0 11 88 5 [8 2 0 0] 1 -0.05 누적보상 :  0.14\n","0 11 89 6 [8 2 0 0] 1 -10 누적보상 :  -9.86\n","0 12 90 1 [8 4 0 7] 0 0.045 누적보상 :  0.045\n","0 12 91 2 [8 3 0 7] 2 -0.06 누적보상 :  -0.015\n","0 12 92 3 [8 4 0 7] 3 -0.955 누적보상 :  -0.97\n","0 12 93 4 [7 4 0 7] 0 0.05 누적보상 :  -0.9199999999999999\n","0 12 94 5 [8 4 0 7] 1 -1.055 누적보상 :  -1.9749999999999999\n","0 12 95 6 [9 4 0 7] 1 -0.06 누적보상 :  -2.0349999999999997\n","0 12 96 7 [8 4 0 7] 0 -0.955 누적보상 :  -2.9899999999999998\n","0 12 97 8 [8 5 0 7] 3 0.05 누적보상 :  -2.94\n","0 12 98 9 [8 4 0 7] 2 -1.055 누적보상 :  -3.995\n","0 12 99 10 [8 3 0 7] 2 -1.06 누적보상 :  -5.055\n","0 12 100 11 [8 3 0 7] 1 -10 누적보상 :  -15.055\n","종료: done = True ... j = 100  move = 11\n","0 13 101 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 13 102 2 [8 5 4 0] 3 -0.045 누적보상 :  0.015\n","0 13 103 3 [8 5 4 0] 1 -10 누적보상 :  -9.985\n","0 14 104 1 [8 4 0 5] 0 0.055 누적보상 :  0.055\n","0 14 105 2 [9 4 0 5] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 14 106 3 [8 4 0 5] 0 -0.945 누적보상 :  -0.94\n","0 14 107 4 [7 4 0 5] 0 0.06 누적보상 :  -0.8799999999999999\n","0 14 108 5 [8 4 0 5] 1 -1.045 누적보상 :  -1.9249999999999998\n","0 14 109 6 [9 4 0 5] 1 -1.05 누적보상 :  -2.9749999999999996\n","0 14 110 7 [8 4 0 5] 0 -0.945 누적보상 :  -3.9199999999999995\n","0 14 111 8 [7 4 0 5] 0 -0.94 누적보상 :  -4.859999999999999\n","0 14 112 9 [8 4 0 5] 1 -1.045 누적보상 :  -5.904999999999999\n","0 14 113 10 [8 5 0 5] 3 0.06 누적보상 :  -5.845\n","0 14 114 11 [7 5 0 5] 0 0.065 누적보상 :  -5.779999999999999\n","0 14 115 12 [8 5 0 5] 1 -1.04 누적보상 :  -6.819999999999999\n","0 14 116 13 [7 5 0 5] 0 -0.935 누적보상 :  -7.754999999999999\n","0 14 117 14 [8 5 0 5] 1 -1.04 누적보상 :  -8.794999999999998\n","0 14 118 15 [8 4 0 5] 2 -1.045 누적보상 :  -9.839999999999998\n","0 14 119 16 [8 5 0 5] 3 -0.94 누적보상 :  -10.779999999999998\n","0 14 120 17 [7 5 0 5] 0 -0.935 누적보상 :  -11.714999999999998\n","0 14 121 18 [7 6 0 5] 3 -0.04 누적보상 :  -11.754999999999997\n","0 14 122 19 [7 7 0 5] 3 -0.045 누적보상 :  -11.799999999999997\n","0 14 123 20 [7 6 0 5] 2 -0.94 누적보상 :  -12.739999999999997\n","0 14 124 21 [7 5 0 5] 2 -0.935 누적보상 :  -13.674999999999997\n","0 14 125 22 [6 5 0 5] 0 0.07 누적보상 :  -13.604999999999997\n","0 14 126 23 [6 5 0 5] 2 -10 누적보상 :  -23.604999999999997\n","0 15 127 1 [8 4 0 7] 0 0.045 누적보상 :  0.045\n","0 15 128 2 [9 4 0 7] 1 -0.06 누적보상 :  -0.015\n","0 15 129 3 [8 4 0 7] 0 -0.955 누적보상 :  -0.97\n","0 15 130 4 [7 4 0 7] 0 0.05 누적보상 :  -0.9199999999999999\n","0 15 131 5 [7 3 0 7] 2 -0.055 누적보상 :  -0.975\n","0 15 132 6 [7 2 0 7] 2 -0.06 누적보상 :  -1.035\n","0 15 133 7 [8 2 0 7] 1 -0.065 누적보상 :  -1.0999999999999999\n","0 15 134 8 [8 2 0 7] 1 -10 누적보상 :  -11.1\n","0 16 135 1 [8 4 0 8] 0 0.04 누적보상 :  0.04\n","0 16 136 2 [9 4 0 8] 1 -0.065 누적보상 :  -0.025\n","0 16 137 3 [8 4 0 8] 0 -0.96 누적보상 :  -0.985\n","0 16 138 4 [9 4 0 8] 1 -1.065 누적보상 :  -2.05\n","0 16 139 5 [8 4 0 8] 0 -0.96 누적보상 :  -3.01\n","0 16 140 6 [8 5 0 8] 3 0.045 누적보상 :  -2.965\n","0 16 141 7 [8 6 0 8] 3 0.05 누적보상 :  -2.915\n","0 16 142 8 [8 6 0 8] 1 -10 누적보상 :  -12.915\n","0 17 143 1 [8 4 0 6] 0 0.05 누적보상 :  0.05\n","0 17 144 2 [8 3 0 6] 2 -0.055 누적보상 :  -0.0049999999999999975\n","0 17 145 3 [7 3 0 6] 0 0.05 누적보상 :  0.045000000000000005\n","0 17 146 4 [8 3 0 6] 1 -1.055 누적보상 :  -1.01\n","0 17 147 5 [8 2 0 6] 2 -0.06 누적보상 :  -1.07\n","0 17 148 6 [8 1 0 6] 2 -0.065 누적보상 :  -1.135\n","0 17 149 7 [8 2 0 6] 3 -0.96 누적보상 :  -2.0949999999999998\n","0 17 150 8 [7 2 0 6] 0 0.045 누적보상 :  -2.05\n","0 17 151 9 [8 2 0 6] 1 -1.06 누적보상 :  -3.11\n","0 17 152 10 [7 2 0 6] 0 -0.955 누적보상 :  -4.0649999999999995\n","0 17 153 11 [7 2 0 6] 0 -10 누적보상 :  -14.065\n","0 18 154 1 [8 4 5 0] 0 0.065 누적보상 :  0.065\n","0 18 155 2 [7 4 5 0] 0 0.07 누적보상 :  0.135\n","0 18 156 3 [8 4 5 0] 1 -1.035 누적보상 :  -0.8999999999999999\n","0 18 157 4 [7 4 5 0] 0 -0.9299999999999999 누적보상 :  -1.8299999999999998\n","0 18 158 5 [8 4 5 0] 1 -1.035 누적보상 :  -2.8649999999999998\n","0 18 159 6 [8 5 5 0] 3 -0.04 누적보상 :  -2.905\n","0 18 160 7 [7 5 5 0] 0 0.065 누적보상 :  -2.84\n","0 18 161 8 [6 5 5 0] 0 0.07 누적보상 :  -2.77\n","0 18 162 9 [6 5 5 0] 2 -10 누적보상 :  -12.77\n","0 19 163 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 19 164 2 [8 3 4 0] 2 0.065 누적보상 :  0.125\n","0 19 165 3 [8 2 4 0] 2 0.07 누적보상 :  0.195\n","0 19 166 4 [8 3 4 0] 3 -1.035 누적보상 :  -0.8399999999999999\n","0 19 167 5 [8 2 4 0] 2 -0.9299999999999999 누적보상 :  -1.7699999999999998\n","0 19 168 6 [8 2 4 0] 1 -10 누적보상 :  -11.77\n","0 20 169 1 [8 4 0 0] 0 0.04 누적보상 :  0.04\n","0 20 170 2 [8 3 0 0] 2 0.045 누적보상 :  0.08499999999999999\n","0 20 171 3 [8 3 0 0] 1 -10 누적보상 :  -9.915\n","0 21 172 1 [8 4 0 1] 0 0.045 누적보상 :  0.045\n","0 21 173 2 [8 5 0 1] 3 -0.06 누적보상 :  -0.015\n","0 21 174 3 [8 6 0 1] 3 -0.065 누적보상 :  -0.08\n","0 21 175 4 [8 6 0 1] 1 -10 누적보상 :  -10.08\n","0 22 176 1 [8 4 3 8] 0 0.055 누적보상 :  0.055\n","0 22 177 2 [9 4 3 8] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 22 178 3 [8 4 3 8] 0 -0.945 누적보상 :  -0.94\n","0 22 179 4 [7 4 3 8] 0 0.06 누적보상 :  -0.8799999999999999\n","0 22 180 5 [7 4 3 8] 0 -10 누적보상 :  -10.879999999999999\n","0 23 181 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 23 182 2 [8 5 4 0] 3 -0.045 누적보상 :  0.015\n","0 23 183 3 [8 5 4 0] 1 -10 누적보상 :  -9.985\n","0 24 184 1 [8 4 0 7] 0 0.045 누적보상 :  0.045\n","0 24 185 2 [7 4 0 7] 0 0.05 누적보상 :  0.095\n","0 24 186 3 [7 4 0 7] 0 -10 누적보상 :  -9.905\n","0 25 187 1 [8 4 0 4] 0 0.06 누적보상 :  0.06\n","0 25 188 2 [8 3 0 4] 2 -0.045 누적보상 :  0.015\n","0 25 189 3 [8 3 0 4] 1 -10 누적보상 :  -9.985\n","0 26 190 1 [8 4 5 8] 0 0.065 누적보상 :  0.065\n","0 26 191 2 [9 4 5 8] 1 -0.04 누적보상 :  0.025\n","0 26 192 3 [8 4 5 8] 0 -0.935 누적보상 :  -0.91\n","0 26 193 4 [9 4 5 8] 1 -1.04 누적보상 :  -1.9500000000000002\n","0 26 194 5 [8 4 5 8] 0 -0.935 누적보상 :  -2.8850000000000002\n","0 26 195 6 [9 4 5 8] 1 -1.04 누적보상 :  -3.9250000000000003\n","0 26 196 7 [8 4 5 8] 0 -0.935 누적보상 :  -4.86\n","0 26 197 8 [9 4 5 8] 1 -1.04 누적보상 :  -5.9\n","0 26 198 9 [8 4 5 8] 0 -0.935 누적보상 :  -6.835000000000001\n","0 26 199 10 [8 3 5 8] 2 -0.04 누적보상 :  -6.875000000000001\n","0 26 200 11 [7 3 5 8] 0 0.065 누적보상 :  -6.8100000000000005\n","0 26 201 12 [6 3 5 8] 0 0.07 누적보상 :  -6.74\n","0 26 202 13 [6 3 5 8] 3 -10 누적보상 :  -16.740000000000002\n","0 27 203 1 [8 4 2 8] 0 0.05 누적보상 :  0.05\n","0 27 204 2 [8 3 2 8] 2 -0.055 누적보상 :  -0.0049999999999999975\n","0 27 205 3 [7 3 2 8] 0 0.05 누적보상 :  0.045000000000000005\n","0 27 206 4 [7 4 2 8] 3 0.055 누적보상 :  0.1\n","0 27 207 5 [8 4 2 8] 1 -1.05 누적보상 :  -0.9500000000000001\n","0 27 208 6 [7 4 2 8] 0 -0.945 누적보상 :  -1.895\n","0 27 209 7 [7 5 2 8] 3 0.06 누적보상 :  -1.835\n","0 27 210 8 [7 4 2 8] 2 -1.045 누적보상 :  -2.88\n","0 27 211 9 [8 4 2 8] 1 -1.05 누적보상 :  -3.9299999999999997\n","0 27 212 10 [8 5 2 8] 3 0.055 누적보상 :  -3.8749999999999996\n","0 27 213 11 [8 6 2 8] 3 0.06 누적보상 :  -3.8149999999999995\n","0 27 214 12 [7 6 2 8] 0 0.065 누적보상 :  -3.7499999999999996\n","0 27 215 13 [7 6 2 8] 0 -10 누적보상 :  -13.75\n","0 28 216 1 [8 4 4 8] 0 0.06 누적보상 :  0.06\n","0 28 217 2 [7 4 4 8] 0 0.065 누적보상 :  0.125\n","0 28 218 3 [7 3 4 8] 2 -0.04 누적보상 :  0.08499999999999999\n","0 28 219 4 [8 3 4 8] 1 -0.045 누적보상 :  0.039999999999999994\n","0 28 220 5 [7 3 4 8] 0 -0.94 누적보상 :  -0.8999999999999999\n","0 28 221 6 [6 3 4 8] 0 0.065 누적보상 :  -0.835\n","0 28 222 7 [7 3 4 8] 1 -1.04 누적보상 :  -1.875\n","0 28 223 8 [8 3 4 8] 1 -1.045 누적보상 :  -2.92\n","0 28 224 9 [8 3 4 8] 1 -10 누적보상 :  -12.92\n","0 29 225 1 [8 4 0 2] 0 0.05 누적보상 :  0.05\n","0 29 226 2 [8 3 0 2] 2 0.055 누적보상 :  0.10500000000000001\n","0 29 227 3 [8 3 0 2] 1 -10 누적보상 :  -9.895\n","0 30 228 1 [8 4 0 1] 0 0.045 누적보상 :  0.045\n","0 30 229 2 [8 5 0 1] 3 -0.06 누적보상 :  -0.015\n","0 30 230 3 [8 6 0 1] 3 -0.065 누적보상 :  -0.08\n","0 30 231 4 [8 5 0 1] 2 -0.96 누적보상 :  -1.04\n","0 30 232 5 [8 6 0 1] 3 -1.065 누적보상 :  -2.105\n","0 30 233 6 [8 7 0 1] 3 -0.07 누적보상 :  -2.175\n","0 30 234 7 [8 6 0 1] 2 -0.965 누적보상 :  -3.1399999999999997\n","0 30 235 8 [8 5 0 1] 2 -0.96 누적보상 :  -4.1\n","0 30 236 9 [7 5 0 1] 0 0.045 누적보상 :  -4.055\n","0 30 237 10 [7 4 0 1] 2 0.05 누적보상 :  -4.005\n","0 30 238 11 [7 5 0 1] 3 -1.055 누적보상 :  -5.06\n","0 30 239 12 [7 4 0 1] 2 -0.95 누적보상 :  -6.01\n","0 30 240 13 [7 4 0 1] 0 -10 누적보상 :  -16.009999999999998\n","0 31 241 1 [8 4 0 7] 0 0.045 누적보상 :  0.045\n","0 31 242 2 [8 3 0 7] 2 -0.06 누적보상 :  -0.015\n","0 31 243 3 [8 2 0 7] 2 -0.065 누적보상 :  -0.08\n","0 31 244 4 [8 2 0 7] 1 -10 누적보상 :  -10.08\n","0 32 245 1 [8 4 0 3] 0 0.055 누적보상 :  0.055\n","0 32 246 2 [8 5 0 3] 3 -0.05 누적보상 :  0.0049999999999999975\n","0 32 247 3 [8 4 0 3] 2 -0.945 누적보상 :  -0.94\n","0 32 248 4 [8 5 0 3] 3 -1.05 누적보상 :  -1.99\n","0 32 249 5 [7 5 0 3] 0 0.055 누적보상 :  -1.935\n","0 32 250 6 [7 6 0 3] 3 -0.05 누적보상 :  -1.985\n","0 32 251 7 [7 5 0 3] 2 -0.945 누적보상 :  -2.93\n","0 32 252 8 [8 5 0 3] 1 -1.05 누적보상 :  -3.9800000000000004\n","0 32 253 9 [8 5 0 3] 1 -10 누적보상 :  -13.98\n","0 33 254 1 [8 4 5 8] 0 0.065 누적보상 :  0.065\n","0 33 255 2 [8 5 5 8] 3 0.07 누적보상 :  0.135\n","0 33 256 3 [7 5 5 8] 0 0.075 누적보상 :  0.21000000000000002\n","0 33 257 4 [6 5 5 8] 0 0.08 누적보상 :  0.29000000000000004\n","0 33 258 5 [6 5 5 8] 2 -10 누적보상 :  -9.71\n","0 34 259 1 [8 4 5 8] 0 0.065 누적보상 :  0.065\n","0 34 260 2 [8 3 5 8] 2 -0.04 누적보상 :  0.025\n","0 34 261 3 [7 3 5 8] 0 0.065 누적보상 :  0.09\n","0 34 262 4 [7 4 5 8] 3 0.07 누적보상 :  0.16\n","0 34 263 5 [8 4 5 8] 1 -1.035 누적보상 :  -0.8749999999999999\n","0 34 264 6 [8 3 5 8] 2 -1.04 누적보상 :  -1.915\n","0 34 265 7 [8 2 5 8] 2 -0.045 누적보상 :  -1.96\n","0 34 266 8 [8 2 5 8] 1 -10 누적보상 :  -11.96\n","0 35 267 1 [8 4 0 4] 0 0.06 누적보상 :  0.06\n","0 35 268 2 [7 4 0 4] 0 0.065 누적보상 :  0.125\n","0 35 269 3 [7 4 0 4] 0 -10 누적보상 :  -9.875\n","0 36 270 1 [8 4 5 8] 0 0.065 누적보상 :  0.065\n","0 36 271 2 [8 3 5 8] 2 -0.04 누적보상 :  0.025\n","0 36 272 3 [8 3 5 8] 1 -10 누적보상 :  -9.975\n","0 37 273 1 [8 4 2 8] 0 0.05 누적보상 :  0.05\n","0 37 274 2 [9 4 2 8] 1 -0.055 누적보상 :  -0.0049999999999999975\n","0 37 275 3 [8 4 2 8] 0 -0.95 누적보상 :  -0.955\n","0 37 276 4 [8 5 2 8] 3 0.055 누적보상 :  -0.8999999999999999\n","0 37 277 5 [8 6 2 8] 3 0.06 누적보상 :  -0.8399999999999999\n","0 37 278 6 [7 6 2 8] 0 0.065 누적보상 :  -0.7749999999999999\n","0 37 279 7 [7 7 2 8] 3 0.07 누적보상 :  -0.7049999999999998\n","0 37 280 8 [6 7 2 8] 0 0.075 누적보상 :  -0.6299999999999999\n","0 37 281 9 [7 7 2 8] 1 -1.03 누적보상 :  -1.66\n","0 37 282 10 [6 7 2 8] 0 -0.925 누적보상 :  -2.585\n","0 37 283 11 [7 7 2 8] 1 -1.03 누적보상 :  -3.615\n","0 37 284 12 [6 7 2 8] 0 -0.925 누적보상 :  -4.54\n","0 37 285 13 [5 7 2 8] 0 0.08 누적보상 :  -4.46\n","0 37 286 14 [4 7 2 8] 0 0.085 누적보상 :  -4.375\n","0 37 287 15 [5 7 2 8] 1 -1.02 누적보상 :  -5.395\n","0 37 288 16 [6 7 2 8] 1 -1.025 누적보상 :  -6.42\n","0 37 289 17 [6 7 2 8] 2 -10 누적보상 :  -16.42\n","0 38 290 1 [8 4 4 8] 0 0.06 누적보상 :  0.06\n","0 38 291 2 [8 5 4 8] 3 0.065 누적보상 :  0.125\n","0 38 292 3 [7 5 4 8] 0 0.07 누적보상 :  0.195\n","0 38 293 4 [6 5 4 8] 0 0.075 누적보상 :  0.27\n","0 38 294 5 [7 5 4 8] 1 -1.03 누적보상 :  -0.76\n","0 38 295 6 [6 5 4 8] 0 -0.925 누적보상 :  -1.685\n","0 38 296 7 [6 5 4 8] 2 -10 누적보상 :  -11.685\n","0 39 297 1 [8 4 0 2] 0 0.05 누적보상 :  0.05\n","0 39 298 2 [8 5 0 2] 3 -0.055 누적보상 :  -0.0049999999999999975\n","0 39 299 3 [8 4 0 2] 2 -0.95 누적보상 :  -0.955\n","0 39 300 4 [8 3 0 2] 2 0.055 누적보상 :  -0.8999999999999999\n","0 39 301 5 [8 3 0 2] 1 -10 누적보상 :  -10.9\n","0 40 302 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 40 303 2 [7 4 4 0] 0 0.065 누적보상 :  0.125\n","0 40 304 3 [7 3 4 0] 2 0.07 누적보상 :  0.195\n","0 40 305 4 [7 2 4 0] 2 0.075 누적보상 :  0.27\n","0 40 306 5 [8 2 4 0] 1 -0.03 누적보상 :  0.24000000000000002\n","0 40 307 6 [7 2 4 0] 0 -0.925 누적보상 :  -0.685\n","0 40 308 7 [7 3 4 0] 3 -1.03 누적보상 :  -1.715\n","0 40 309 8 [8 3 4 0] 1 -0.035 누적보상 :  -1.75\n","0 40 310 9 [8 3 4 0] 1 -10 누적보상 :  -11.75\n","0 41 311 1 [8 4 0 3] 0 0.055 누적보상 :  0.055\n","0 41 312 2 [7 4 0 3] 0 0.06 누적보상 :  0.11499999999999999\n","0 41 313 3 [7 5 0 3] 3 -0.045 누적보상 :  0.06999999999999999\n","0 41 314 4 [7 6 0 3] 3 -0.05 누적보상 :  0.01999999999999999\n","0 41 315 5 [7 7 0 3] 3 -0.055 누적보상 :  -0.03500000000000001\n","0 41 316 6 [7 6 0 3] 2 -0.95 누적보상 :  -0.985\n","0 41 317 7 [7 5 0 3] 2 -0.945 누적보상 :  -1.93\n","0 41 318 8 [6 5 0 3] 0 0.06 누적보상 :  -1.8699999999999999\n","0 41 319 9 [6 5 0 3] 2 -10 누적보상 :  -11.87\n","0 42 320 1 [8 4 5 0] 0 0.065 누적보상 :  0.065\n","0 42 321 2 [7 4 5 0] 0 0.07 누적보상 :  0.135\n","0 42 322 3 [7 5 5 0] 3 -0.035 누적보상 :  0.1\n","0 42 323 4 [7 6 5 0] 3 -0.04 누적보상 :  0.060000000000000005\n","0 42 324 5 [7 6 5 0] 0 -10 누적보상 :  -9.94\n","0 43 325 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 43 326 2 [8 3 3 0] 2 0.06 누적보상 :  0.11499999999999999\n","0 43 327 3 [8 3 3 0] 1 -10 누적보상 :  -9.885\n","0 44 328 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 44 329 2 [7 4 3 0] 0 0.06 누적보상 :  0.11499999999999999\n","0 44 330 3 [7 3 3 0] 2 0.065 누적보상 :  0.18\n","0 44 331 4 [7 2 3 0] 2 0.07 누적보상 :  0.25\n","0 44 332 5 [7 1 3 0] 2 0.075 누적보상 :  0.325\n","0 44 333 6 [8 1 3 0] 1 -0.03 누적보상 :  0.29500000000000004\n","0 44 334 7 [8 0 3 0] 2 0.075 누적보상 :  0.37000000000000005\n","0 44 335 8 [7 0 3 0] 0 0.08 누적보상 :  0.45000000000000007\n","0 44 336 9 [6 0 3 0] 0 0.085 누적보상 :  0.535\n","0 44 337 10 [7 0 3 0] 1 -1.02 누적보상 :  -0.485\n","0 44 338 11 [7 0 3 0] 2 -10 누적보상 :  -10.485\n","0 44 339 12 [8 0 3 0] 1 -1.025 누적보상 :  -11.51\n","0 44 340 13 [8 1 3 0] 3 -1.03 누적보상 :  -12.54\n","0 44 341 14 [8 0 3 0] 2 -0.925 누적보상 :  -13.465\n","0 44 342 15 [8 0 3 0] 2 -10 누적보상 :  -23.465\n","0 44 343 16 [8 1 3 0] 3 -1.03 누적보상 :  -24.495\n","0 44 344 17 [8 0 3 0] 2 -0.925 누적보상 :  -25.42\n","0 44 345 18 [7 0 3 0] 0 -0.92 누적보상 :  -26.340000000000003\n","0 44 346 19 [6 0 3 0] 0 -0.915 누적보상 :  -27.255000000000003\n","0 44 347 20 [6 0 3 0] 0 -1.015 누적보상 :  -28.270000000000003\n","0 44 348 21 [7 0 3 0] 1 -1.02 누적보상 :  -29.290000000000003\n","0 44 349 22 [7 1 3 0] 3 -1.025 누적보상 :  -30.315\n","0 44 350 23 [8 1 3 0] 1 -1.03 누적보상 :  -31.345000000000002\n","0 44 351 24 [8 0 3 0] 2 -0.925 누적보상 :  -32.27\n","0 44 352 25 [8 0 3 0] 2 -10 누적보상 :  -42.27\n","0 44 353 26 [8 0 3 0] 2 -10 누적보상 :  -52.27\n","0 44 354 27 [8 1 3 0] 3 -1.03 누적보상 :  -53.300000000000004\n","0 44 355 28 [8 1 3 0] 1 -10 누적보상 :  -63.300000000000004\n","0 45 356 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 45 357 2 [8 3 4 0] 2 0.065 누적보상 :  0.125\n","0 45 358 3 [8 2 4 0] 2 0.07 누적보상 :  0.195\n","0 45 359 4 [7 2 4 0] 0 0.075 누적보상 :  0.27\n","0 45 360 5 [8 2 4 0] 1 -1.03 누적보상 :  -0.76\n","0 45 361 6 [8 1 4 0] 2 0.075 누적보상 :  -0.685\n","0 45 362 7 [7 1 4 0] 0 0.08 누적보상 :  -0.6050000000000001\n","0 45 363 8 [7 0 4 0] 2 0.085 누적보상 :  -0.5200000000000001\n","0 45 364 9 [7 1 4 0] 3 -1.02 누적보상 :  -1.54\n","0 45 365 10 [7 2 4 0] 3 -1.025 누적보상 :  -2.565\n","0 45 366 11 [7 3 4 0] 3 -0.03 누적보상 :  -2.5949999999999998\n","0 45 367 12 [6 3 4 0] 0 0.075 누적보상 :  -2.5199999999999996\n","0 45 368 13 [7 3 4 0] 1 -1.03 누적보상 :  -3.55\n","0 45 369 14 [8 3 4 0] 1 -1.035 누적보상 :  -4.585\n","0 45 370 15 [8 4 4 0] 3 -1.04 누적보상 :  -5.625\n","0 45 371 16 [8 5 4 0] 3 -0.045 누적보상 :  -5.67\n","0 45 372 17 [8 6 4 0] 3 -0.05 누적보상 :  -5.72\n","0 45 373 18 [7 6 4 0] 0 0.055 누적보상 :  -5.665\n","0 45 374 19 [7 6 4 0] 0 -10 누적보상 :  -15.665\n","0 46 375 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 46 376 2 [7 4 3 0] 0 0.06 누적보상 :  0.11499999999999999\n","0 46 377 3 [7 4 3 0] 0 -10 누적보상 :  -9.885\n","0 47 378 1 [8 4 3 8] 0 0.055 누적보상 :  0.055\n","0 47 379 2 [9 4 3 8] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 47 380 3 [8 4 3 8] 0 -0.945 누적보상 :  -0.94\n","0 47 381 4 [7 4 3 8] 0 0.06 누적보상 :  -0.8799999999999999\n","0 47 382 5 [7 4 3 8] 0 -10 누적보상 :  -10.879999999999999\n","0 48 383 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 48 384 2 [8 3 3 0] 2 0.06 누적보상 :  0.11499999999999999\n","0 48 385 3 [8 2 3 0] 2 0.065 누적보상 :  0.18\n","0 48 386 4 [8 2 3 0] 1 -10 누적보상 :  -9.82\n","0 49 387 1 [8 4 0 3] 0 0.055 누적보상 :  0.055\n","0 49 388 2 [8 3 0 3] 2 0.06 누적보상 :  0.11499999999999999\n","0 49 389 3 [8 2 0 3] 2 -0.045 누적보상 :  0.06999999999999999\n","0 49 390 4 [8 2 0 3] 1 -10 누적보상 :  -9.93\n","0 50 391 1 [8 4 5 8] 0 0.065 누적보상 :  0.065\n","0 50 392 2 [8 5 5 8] 3 0.07 누적보상 :  0.135\n","0 50 393 3 [7 5 5 8] 0 0.075 누적보상 :  0.21000000000000002\n","0 50 394 4 [8 5 5 8] 1 -1.03 누적보상 :  -0.8200000000000001\n","0 50 395 5 [8 4 5 8] 2 -1.035 누적보상 :  -1.855\n","0 50 396 6 [7 4 5 8] 0 0.07 누적보상 :  -1.785\n","0 50 397 7 [8 4 5 8] 1 -1.035 누적보상 :  -2.82\n","0 50 398 8 [8 3 5 8] 2 -0.04 누적보상 :  -2.86\n","0 50 399 9 [8 3 5 8] 1 -10 누적보상 :  -12.86\n","0 51 400 1 [8 4 0 6] 0 0.05 누적보상 :  0.05\n","0 51 401 2 [8 5 0 6] 3 0.055 누적보상 :  0.10500000000000001\n","0 51 402 3 [8 4 0 6] 2 -1.05 누적보상 :  -0.9450000000000001\n","0 51 403 4 [7 4 0 6] 0 0.055 누적보상 :  -0.89\n","0 51 404 5 [8 4 0 6] 1 -1.05 누적보상 :  -1.94\n","0 51 405 6 [7 4 0 6] 0 -0.945 누적보상 :  -2.885\n","0 51 406 7 [7 3 0 6] 2 -0.05 누적보상 :  -2.9349999999999996\n","0 51 407 8 [6 3 0 6] 0 0.055 누적보상 :  -2.8799999999999994\n","0 51 408 9 [5 3 0 6] 0 0.06 누적보상 :  -2.8199999999999994\n","0 51 409 10 [5 3 0 6] 2 -10 누적보상 :  -12.82\n","0 52 410 1 [8 4 2 0] 0 0.05 누적보상 :  0.05\n","0 52 411 2 [8 3 2 0] 2 0.055 누적보상 :  0.10500000000000001\n","0 52 412 3 [7 3 2 0] 0 0.06 누적보상 :  0.165\n","0 52 413 4 [6 3 2 0] 0 0.065 누적보상 :  0.23\n","0 52 414 5 [6 3 2 0] 3 -10 누적보상 :  -9.77\n","0 53 415 1 [8 4 2 8] 0 0.05 누적보상 :  0.05\n","0 53 416 2 [9 4 2 8] 1 -0.055 누적보상 :  -0.0049999999999999975\n","0 53 417 3 [8 4 2 8] 0 -0.95 누적보상 :  -0.955\n","0 53 418 4 [8 5 2 8] 3 0.055 누적보상 :  -0.8999999999999999\n","0 53 419 5 [8 4 2 8] 2 -1.05 누적보상 :  -1.95\n","0 53 420 6 [8 3 2 8] 2 -0.055 누적보상 :  -2.005\n","0 53 421 7 [8 4 2 8] 3 -0.95 누적보상 :  -2.955\n","0 53 422 8 [8 5 2 8] 3 -0.945 누적보상 :  -3.9\n","0 53 423 9 [8 5 2 8] 1 -10 누적보상 :  -13.9\n","0 54 424 1 [8 4 4 8] 0 0.06 누적보상 :  0.06\n","0 54 425 2 [9 4 4 8] 1 -0.045 누적보상 :  0.015\n","0 54 426 3 [8 4 4 8] 0 -0.94 누적보상 :  -0.9249999999999999\n","0 54 427 4 [7 4 4 8] 0 0.065 누적보상 :  -0.8599999999999999\n","0 54 428 5 [7 4 4 8] 0 -10 누적보상 :  -10.86\n","0 55 429 1 [8 4 5 0] 0 0.065 누적보상 :  0.065\n","0 55 430 2 [8 5 5 0] 3 -0.04 누적보상 :  0.025\n","0 55 431 3 [7 5 5 0] 0 0.065 누적보상 :  0.09\n","0 55 432 4 [6 5 5 0] 0 0.07 누적보상 :  0.16\n","0 55 433 5 [5 5 5 0] 0 0.075 누적보상 :  0.235\n","0 55 434 6 [5 5 5 0] 2 -10 누적보상 :  -9.765\n","0 56 435 1 [8 4 3 0] 0 0.055 누적보상 :  0.055\n","0 56 436 2 [9 4 3 0] 1 -0.05 누적보상 :  0.0049999999999999975\n","0 56 437 3 [8 4 3 0] 0 -0.945 누적보상 :  -0.94\n","0 56 438 4 [8 5 3 0] 3 -0.05 누적보상 :  -0.99\n","0 56 439 5 [8 5 3 0] 1 -10 누적보상 :  -10.99\n","0 57 440 1 [8 4 2 8] 0 0.05 누적보상 :  0.05\n","0 57 441 2 [8 3 2 8] 2 -0.055 누적보상 :  -0.0049999999999999975\n","0 57 442 3 [8 4 2 8] 3 -0.95 누적보상 :  -0.955\n","0 57 443 4 [8 5 2 8] 3 0.055 누적보상 :  -0.8999999999999999\n","0 57 444 5 [8 5 2 8] 1 -10 누적보상 :  -10.9\n","0 58 445 1 [8 4 4 8] 0 0.06 누적보상 :  0.06\n","0 58 446 2 [7 4 4 8] 0 0.065 누적보상 :  0.125\n","0 58 447 3 [7 3 4 8] 2 -0.04 누적보상 :  0.08499999999999999\n","0 58 448 4 [8 3 4 8] 1 -0.045 누적보상 :  0.039999999999999994\n","0 58 449 5 [8 3 4 8] 1 -10 누적보상 :  -9.96\n","0 59 450 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 59 451 2 [8 3 4 0] 2 0.065 누적보상 :  0.125\n","0 59 452 3 [7 3 4 0] 0 0.07 누적보상 :  0.195\n","0 59 453 4 [7 4 4 0] 3 -0.035 누적보상 :  0.16\n","0 59 454 5 [8 4 4 0] 1 -1.04 누적보상 :  -0.88\n","0 59 455 6 [7 4 4 0] 0 -0.935 누적보상 :  -1.815\n","0 59 456 7 [7 3 4 0] 2 -0.9299999999999999 누적보상 :  -2.745\n","0 59 457 8 [7 4 4 0] 3 -1.035 누적보상 :  -3.7800000000000002\n","0 59 458 9 [8 4 4 0] 1 -1.04 누적보상 :  -4.82\n","0 59 459 10 [8 5 4 0] 3 -0.045 누적보상 :  -4.865\n","0 59 460 11 [8 5 4 0] 1 -10 누적보상 :  -14.865\n","0 60 461 1 [8 4 4 0] 0 0.06 누적보상 :  0.06\n","0 60 462 2 [7 4 4 0] 0 0.065 누적보상 :  0.125\n","0 60 463 3 [7 3 4 0] 2 0.07 누적보상 :  0.195\n","0 60 464 4 [7 4 4 0] 3 -1.035 누적보상 :  -0.8399999999999999\n","0 60 465 5 [7 5 4 0] 3 -0.04 누적보상 :  -0.8799999999999999\n","0 60 466 6 [7 4 4 0] 2 -0.935 누적보상 :  -1.815\n","0 60 467 7 [7 5 4 0] 3 -1.04 누적보상 :  -2.855\n","0 60 468 8 [6 5 4 0] 0 0.065 누적보상 :  -2.79\n","0 60 469 9 [7 5 4 0] 1 -1.04 누적보상 :  -3.83\n","0 60 470 10 [8 5 4 0] 1 -0.045 누적보상 :  -3.875\n","0 60 471 11 [8 5 4 0] 1 -10 누적보상 :  -13.875\n","0 61 472 1 [8 4 3 8] 0 0.055 누적보상 :  0.055\n","0 61 473 2 [8 5 3 8] 3 0.06 누적보상 :  0.11499999999999999\n","0 61 474 3 [8 5 3 8] 1 -10 누적보상 :  -9.885\n","0 62 475 1 [8 4 5 0] 0 0.065 누적보상 :  0.065\n","0 62 476 2 [7 4 5 0] 0 0.07 누적보상 :  0.135\n","0 62 477 3 [7 3 5 0] 2 0.075 누적보상 :  0.21000000000000002\n","0 62 478 4 [7 2 5 0] 2 0.08 누적보상 :  0.29000000000000004\n","0 62 479 5 [7 1 5 0] 2 0.085 누적보상 :  0.37500000000000006\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/Colab_Notebooks/RL/path-finding-rl/data\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 위 경우를 제외하곤 e-greedy로 액션 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;31m#####################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# action = q.sample_action(state1, epsilon)  # 위 조건을 적용 안 할 경우\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab_Notebooks/RL/path-finding-rl/data\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs, epsilon)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Qnet()을 실행해서 모든 action에 대한 Qval을 구한 후 action 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# out = Q_value, obs = state = 그리드맵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mcoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# e-greedy로 action 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcoin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab_Notebooks/RL/path-finding-rl/data\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (N, 4, 10, 9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# (N, 32, 8, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# (N, 64, 6, 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["finish_info = []\n","finish_count = 0\n","#def main(): # DQN... Total Path 학습 모델\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","print('tensor in', device)  # GPU 사용 확인\n","# PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","PATH = __file__ + '/model_conv_colab_no_use.pt' ## PATH 경로 쉽게 수정\n","##############################################\n","tz = pytz.timezone('Asia/Seoul')\n","cur_time = datetime.now(tz)\n","start_time = cur_time.strftime(\"%H:%M:%S\")\n","\n","### 중요 Hyperparameters #####################\n","buffer_limit  = 100000     #1. 50000\n","batch_size    = 200        #2. 32\n","learning_rate = 0.00025      #3. 0.0005\n","sync_freq = 500            #4. q 네트워크 파라미터를 q_target 네트워크에 복사하는 주기\n","train_start = 10000   # (추가)\n","##############################################\n","env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","q = Qnet()\n","q.to(device)\n","optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","\n","train_model_load = False  # 학습한 모델을 불러와서 계속 학습시키고자 하는 경우 True로 바꾼다!!\n","if train_model_load:\n","    ###############################################################\n","    ## 학습한 모델 불러오기\n","    ###############################################################\n","    checkpoint = torch.load(PATH)\n","    q.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    q.train()\n","###############################################################\n","q_target = Qnet()\n","q_target.to(device)\n","q_target.load_state_dict(q.state_dict())\n","memory = ReplayBuffer()\n","\n","### Hyperparameters #####################\n","gamma = 0.99             #5. 0.98\n","####################################################################\n","# epochs = len(env.files)  #6. 훈련용 데이터 갯수\n","epochs = 70000  # 한 군데 갔다 오는 경우에 사용. 반복 횟수는 각자 지정\n","####################################################################\n","losses = []\n","moves = []\n","max_moves = 100          #7.\n","print_interval = 1000    #8.\n","j = 0\n","goal_ob_reward_count = 0 #9. epoch를 반복하면서 finish 카운팅\n","# epsilon = 0.3            #10. annealing 대신 고정\n","loss = 0.0\n","##############################################\n","# __file__ = '/content/drive/MyDrive/aiffelthon/data' \n","### 액션 저장하는 txt파일 만들기 #######################\n","f = open(__file__ + '/ogz_conv_gif' + '.txt', 'w')\n","########################################################\n","\n","for n_epi in range(epochs):\n","    epsilon = max(0.1, 1.00 - 0.1*(n_epi/20000)) # Linear annealing from 8% to 1%\n","    gridmap = env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        # self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        # self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        # self.terminal_location = None                             #4. 최초 목적지\n","        # self.local_target = []                                    #5. 목적지 리스트 초기화\n","        # self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        # self.grid = np.ones((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(1)로 초기화\n","        # self.set_box()                                            #8. 선반 위치(0), 목적지(2) 그리드값 설정.  목적지 리스트 생성\n","        # self.set_obstacle()                                       #9. 장애물 그리드값(0) 설정\n","        # self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        # self.grid[int(self.curloc[0])][int(self.curloc[1])] = -1  #11. 현재 위치(출발점) 그리드값(-1) 세팅\n","        # self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        # return self.grid\n","\n","    # 초기 input은 (시작화면*4장)으로 들어감\n","    #####################################################################################\n","    history1 = np.stack((gridmap,gridmap,gridmap,gridmap), axis = 0) # 초기 state로 히스토리 초기화 (4,10,9)\n","    history1 = np.reshape([history1], (1,4,10,9)) # (배치, 채널, 가로, 세로) 로 reshape\n","    history1 = torch.from_numpy(history1).float() # 텐서로 변환   \n","    #####################################################################################\n","\n","    done = False\n","    mov = 0\n","\n","    while (not done):  # 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과하면 종료\n","        j += 1\n","        mov += 1\n","        # 상태 state1에서 action 결정\n","        #####################################################################################\n","        if env.curloc == [9,4]:  # 출발점에서는 무조건 위로 올라간다 (action=0)\n","            action = 0\n","        elif env.curloc[0] == 0 and env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","            action = 1\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","            action = 2\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","            action = 3\n","        else:  # 위 경우를 제외하곤 e-greedy로 액션 선택\n","            action = q.sample_action(history1, epsilon)\n","        #####################################################################################\n","        # action = q.sample_action(state1, epsilon)  # 위 조건을 적용 안 할 경우\n","        # make Move... action에 따른 이동\n","        gridmap, reward, cum_reward, done = env.step(action)\n","            # self.actions.append((new_x, new_y))...새로 도착한 위치를 경로 리스트에 추가\n","            # 누적 보상 계산\n","        ###########################\n","        print(finish_count, n_epi, j, mov, env.get_current_state(), action, reward,'누적보상 : ', cum_reward)\n","\n","        if env.goal_ob_reward : \n","            finish_count +=1\n","            finish_info.append((n_epi, j, mov, cum_reward))\n","        # print('j =', j, 'action =', action)\n","        # print(gridmap.reshape(10,9))\n","        ###########################\n","\n","        #####################################################################################\n","        # next input 만들어주기\n","        next_gridmap = np.reshape([gridmap],(1,1,10,9))                                             # 다음 그리드를 히스토리 사이즈에 맞게 reshape\n","        next_gridmap = torch.from_numpy(next_gridmap).float()                                       # 텐서로 변환\n","        history2 = torch.cat([next_gridmap,history1[:,:3,:,:]],dim=1) # (1,1+3,10,9)                # 지금 있는 히스토리중 마지막 채널을 지우고 new를 제일 앞에 붙임\n","        done_mask = 0.0 if done else 1.0\n","        memory.put((history1, action, reward, history2, done_mask))  # 경험을 메모리에 저장\n","        history1 = history2\n","        #####################################################################################\n","\n","        # if memory.size() > train_start:  # 메모리에 train_start 크기 이상 쌓이면... 미니배치 훈련\n","        if j > train_start:  # 메모리에 미니배치 크기 이상 쌓이면... 미니배치 훈련\n","            #print('memory size:', memory.size())\n","            loss = train(q, q_target, memory, optimizer)\n","            if j % print_interval == 0:\n","                print('epiode #:', n_epi, 'loss:', loss, 'j:', j)\n","            losses.append(loss)\n","\n","            if j % sync_freq == 0:  # sync_freq마다 q 네트워크 파라미터를 q_target 네트워크로 복사\n","                q_target.load_state_dict(q.state_dict())\n","\n","        ############## 현황 모니터링 #############################################################\n","        if done and j % 100 == 0:  # 100번 마다 mov 횟수 저장:\n","            moves.append(mov)\n","            print('종료: done =', done, '... j =', j, ' move =', mov)\n","        if env.goal_ob_reward:  # 최종 목적지 도달한 경우에만 화면에 표시\n","            goal_ob_reward_count += 1\n","            print(env.items, '종료: env.goal_ob_reward =', env.goal_ob_reward, '... j =', j, ' move =', mov, '@ 에피소드 #', n_epi)\n","            print(f\"{n_epi}번째 에피소드까지 총 {goal_ob_reward_count}번 finish 했습니다.\")\n","        ##########################################################################################\n","\n","        if done or mov > max_moves:\n","            mov = 0\n","            done = True\n","    \n","    # while loop 종료 ----- 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과\n","\n","    if j > train_start:  # 메모리에 train_start 크기 이상 쌓이면... 미니배치 훈련\n","        if (n_epi % 100 == 0 and n_epi != 0) or n_epi >= epochs - 1:\n","            torch.save({'epoch': n_epi, \n","                        'model_state_dict': q.state_dict(), \n","                        'optimizer_state_dict': optimizer.state_dict(), \n","                        'loss': loss, \n","                        }, PATH)\n","            print('▶ 모델 저장됨!!! @ 에피소드', n_epi)\n","\n","        ##### gif 생성을 위한 경로(env.actions) 저장 ####################################    \n","        if n_epi % 1000 == 0 or env.goal_ob_reward:\n","            f.write(str(n_epi)+'/'+str(env.local_target_original)+'\\n')\n","            f.write(str(env.actions))\n","            f.write('\\n')\n","        ######################################### \n","f.close()\n","#########################################\n","## 프로그램 시작 및 종료 시간 출력\n","cur_time = datetime.now(tz)\n","end_time = cur_time.strftime(\"%H:%M:%S\")\n","print ('실행 종료!', 'Start@', start_time, 'End@', end_time)\n","        #cur_time = datetime.now(tz)\n","        #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","        #print('▶ Episode #', iter, 'start time:', simple_cur_time, end='→')\n","\n","print(gridmap.reshape(10,9))  # 최종 그리드맵 확인\n","\n","## loss 그래프\n","plt.figure(figsize=(10,7))\n","plt.plot(losses)\n","plt.xlabel(\"# of Actions\",fontsize=22)\n","plt.ylabel(\"Loss\",fontsize=22)\n","\n","## 경로길이(이동 횟수) 그래프\n","plt.figure(figsize=(10,7))\n","plt.plot(moves)\n","plt.xlabel(\"# of Actions x 100\",fontsize=22)\n","plt.ylabel(\"Moves\",fontsize=22)"]},{"cell_type":"markdown","metadata":{"id":"q2kNcb0-3j-s"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"EUyczG0z3nay"},"source":["## 테스트 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfW9OaPsekYX"},"outputs":[],"source":["## test 코드 #################################################################################\n","def test(epochs=1000, train_mode=False, display=True, model_load=True):  # train_mode = False 테스트 모드\n","##############################################################################################\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","    print('tensor in', device)  # GPU 사용 확인\n","    PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","    test_env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","    test_q = Qnet()\n","    test_q.to(device)\n","\n","    if model_load:\n","        ###############################################################\n","        ## 학습한 모델 불러오기\n","        ###############################################################\n","        checkpoint = torch.load(PATH)\n","        q.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","        q.eval()\n","\n","    ### 액션 저장하는 txt파일 만들기 #######################\n","    f = open('ogz_conv_test_gif' + '.txt', 'w')\n","    ########################################################\n","    #epochs = len(test_env.files)\n","    wins = 0\n","    for n_epi in range(epochs):\n","        gridmap = test_env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        history = np.stack((gridmap,gridmap,gridmap,gridmap), axis = 0) # 초기 state로 히스토리 초기화 (4,10,9)\n","        history = np.reshape([history], (1,4,10,9)) # (배치, 채널, 가로, 세로) 로 reshape\n","        history = torch.from_numpy(history).float() # 텐서로 변환   \n","        done = False\n","        mov = 0\n","        while (not done):  # 최종 목적지 [9,4] 도착\n","            mov += 1\n","            # 상태 state1에서 action 결정\n","            #####################################################################################\n","            if test_env.curloc == [9,4]:  # 출발점에서는 무조건 위로 (action=0)\n","                action = 0\n","            elif test_env.curloc[0] == 0 and test_env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","                action = 1\n","            elif test_env.curloc[0] in [2,3,4,5] and test_env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","                action = 2\n","            elif test_env.curloc[0] in [2,3,4,5] and test_env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","                action = 3\n","            else:  # 위 경우를 제외하곤 test 액션 선택\n","                action = q.test_action(history1)\n","            #####################################################################################\n","            # action = q.test_action(state)  # 위 조건을 적용하지 않을 경우\n","            # make Move... action에 따른 이동\n","            gridmap, reward, cum_reward, done = test_env.step(action)\n","\n","            next_gridmap = np.reshape([gridmap],(1,1,10,9)) # 다음 그리드를 히스토리 사이즈에 맞게 reshape\n","            next_gridmap = torch.from_numpy(next_gridmap).float() # 텐서로 변환\n","            history = torch.cat([next_gridmap,history[:,:3,:,:]],dim=1) # (1,1+3,10,9)\n","            # 지금 있는 히스토리중 마지막 채널을 지우고 new를 제일 앞에 붙임\n","            \n","            state_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","            state = torch.unsqueeze(state_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","            state = torch.unsqueeze(state,0)  # conv2d용 (1, 1, 10, 9)\n","            if display:\n","                print('Move #%s; Taking action: %s' % (mov, action))\n","                print(gridmap.reshape(10,9))\n","\n","            if test_env.goal_ob_reward:\n","                wins += 1\n","                if display:\n","                    print(\"Game won! Reward: %s\" % (cum_reward))\n","                else:\n","                    print(\"Game LOST. Reward: %s\" % (cum_reward))\n","                    \n","            #if (mov > 100):  # 경로 길이가 너무 길어서 제외시키는 조건 추가 필요시 사용\n","                #if display:\n","                    #print(\"Game lost; too many moves.\")\n","                #break\n","        \n","    win_perc = float(wins) / float(epochs)\n","    print(\"Test performed: {0}, # of wins: {1}\".format(epochs,wins))\n","    print(\"Win percentage: {}%\".format(100.0*win_perc))\n","    return win_perc"]},{"cell_type":"code","source":["test(epochs=10, train_mode=False, display=True, model_load=True)"],"metadata":{"id":"vmYkpzs2ryd5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Baseline_reward_DQN_Conv2D_History_v1.0_0602.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","id":"832a7f04","metadata":{"id":"832a7f04"},"source":["# 목표 Multi Q-table\n","order box 타겟(A, B, ..., Q)에 대한 q-table을 q-learning으로 수렴시키게 만든다.   \n","총 18개(A, B, ..., Q, [9,4] = start point)의 수렴된 table을 만들어야 한다.   \n","order box를 얻은 경우 다음 order box를 향한다.    \n","start -> A -> B -> end로 움직이는 알고리즘을 짠다."]},{"cell_type":"markdown","source":["진구님 [깃허브 P-Q러닝 참고](https://github.com/stormofdawn/path-finding-rl/blob/main/P-Q%EB%9F%AC%EB%8B%9D.ipynb)"],"metadata":{"id":"ecMiPbTxbKWu"},"id":"ecMiPbTxbKWu"},{"cell_type":"code","source":["import random\n","from string import ascii_uppercase\n","# from draw_utils import * \n","# from pyglet.gl import *\n","import numpy as np\n","import pandas as pd\n","import os"],"metadata":{"id":"-DOyKcx9bTGm"},"id":"-DOyKcx9bTGm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reward\n","move_reward = -1 \n","obs_reward = -10     # 한 번 장애물에 부딪치면, 10번에 1번정도만 장애물쪽에 이동하도록 함\n","goal_reward = 1000   # Q-value의 빠른 수렴을 위해 넉넉하게 리워드를 부여\n","# print('reward:' , move_reward, obs_reward, goal_reward) \n","inds = list(ascii_uppercase)[:17]  # 클래스 활용이 미숙해 전역변수로 변환\n","inds.append(\"T\")     # T-state (최종 목적지=시작 목적지)를 추가해 줌 (box.csv에도 추가함)\n","Q_table_name_lst = inds\n","print(inds)\n","Q_tables = [] #Q_table_A : np.zero((10, 9, 4)) 형태의 딕셔너리 혹은 Structure Array 형태로 만들어보고 싶었으나 실패\n","for Q_table_name in Q_table_name_lst:\n","    globals()[\"Q_table_{}\".format(Q_table_name)] = np.zeros((10, 9, 4)) \n","    Q_tables.append(globals()[\"Q_table_{}\".format(Q_table_name)]) \n","#일단 완전히 초기화된 Q-Table들의 리스트인 Q-Tables를 만듬, 그냥 np.zeros((18, 10, 9, 4)) 해도 됐을듯 ㅠㅠ\n","\n","local_path = \"/content/drive/Othercomputers/MacBook_Air/path-finding-rl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5BjnrHW1vVO","executionInfo":{"status":"ok","timestamp":1652950946479,"user_tz":-540,"elapsed":377,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"2ea67452-5278-4990-d44c-8a2cef316b37"},"id":"d5BjnrHW1vVO","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'T']\n"]}]},{"cell_type":"code","source":["box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))"],"metadata":{"id":"4Vo1XQuT10Sp"},"id":"4Vo1XQuT10Sp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_coor_index =[]\n","for i in range(len(box_data)):\n","    box_coor_index.append([box_data['row'][i], box_data['col'][i]])\n","print(box_coor_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShiOQg31130P","executionInfo":{"status":"ok","timestamp":1652939960886,"user_tz":-540,"elapsed":351,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"d02773ec-9b12-414c-97f5-971f74bc0c44"},"id":"ShiOQg31130P","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[5, 0], [4, 0], [3, 0], [2, 0], [0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [2, 8], [3, 8], [4, 8], [5, 8]]\n"]}]},{"cell_type":"markdown","source":["### Q-Tables의 인덱스 = box_coor_index의 인덱스"],"metadata":{"id":"kn3wfxRd2737"},"id":"kn3wfxRd2737"},{"cell_type":"code","source":["class Simulator_exploration:  #단순 탐색을 위한 환경\n","    def __init__(self):\n","        '''\n","        height : 그리드 높이\n","        width : 그리드 너비 \n","        inds : A ~ Q alphabet list\n","        '''\n","        # Load train data\n","        self.files = pd.read_csv(os.path.join(local_path, \"./data/Q_Finder.csv\")) #각각 A부터 T까지 하나를 목표로하는 csv파일을 만듬\n","        self.height = 10 #세로 10\n","        self.width = 9 # 가로 9  #A~Q까지 알파벳(target) 선언\n","        self.failcount = 0\n","        self.successcount = 0\n","    def set_box(self):\n","        '''\n","        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n","        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n","        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n","        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n","        '''\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","\n","        # 물건이 들어있을 수 있는 경우\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  #판다스 데이터를 튜플로 iter해준 것 같다. 행/열 위치정보로 각 알파벳이 표시됨\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100   #아이템이 없는 경우 100\n","\n","        # 물건이 실제 들어있는 경우\n","        order_item = list(set(inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100 # 아이템이 있는 경우 -100 <- 값이 지정되면 리턴으로 사용가능?\n","            # local target에 가야 할 위치 좌표 넣기\n","            self.local_target.append(\n","                [getattr(order_box, \"row\"),\n","                 getattr(order_box, \"col\")]\n","                )# 타겟 위치 지정 완료\n","            # print(order_box)\n","            \n","        # print(self.local_target)\n","        # self.local_target.append([9,4])  # 마지막 위치를 추가해주지 않음\n","\n","        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n","\n","    def set_obstacle(self):\n","        '''\n","        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n","        '''\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0 #장애물 행,열 지정\n","\n","    def reset(self, epi):\n","        '''\n","        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n","\n","        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n","        :return: 초기셋팅 된 그리드\n","        :rtype: numpy.ndarray\n","        _____________________________________________________________________________________\n","        items : 이번 에피소드에서 가져와야하는 아이템들\n","        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n","        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n","        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n","        curloc : 현재 위치\n","        '''\n","\n","        # initial episode parameter setting\n","        self.epi = epi\n","        self.items = list(self.files.iloc[self.epi])[0]\n","        self.cumulative_reward = 0\n","        self.terminal_location = None\n","        self.local_target = []\n","        self.actions = [] # 정책에 따라 시간 순서대로 action 리스트에 append해주면 될듯\n","\n","        # initial grid setting\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\") #초기는 전부0\n","\n","        # set information about the gridworld\n","        self.set_box() #빈박스 + 타겟 박스\n","        self.set_obstacle() #장애물\n","\n","        # start point를 grid에 표시\n","        self.curloc = [np.random.randint(10), np.random.randint(9)] #시작 위치 초기화 (랜덤 위치)\n","        # obstacle, target 등의 위치를 빼고 시작한다면 더 빠른 수렴이 가능하겠지만, 어차피 그 위치의 q-value는 사용될리 없으므로 완전 랜덤으로 해줌\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5       \n","        \n","        self.done = False\n","        \n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):\n","        '''\n","        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n","        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n","        \n","        :param x: 에이전트의 현재 x 좌표\n","        :param y: 에이전트의 현재 y 좌표\n","        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n","        :rtype: int, int\n","        '''\n","        new_x = cur_x\n","        new_y = cur_y\n","        # up\n","        if action == 0:\n","            new_x = cur_x - 1\n","        # down\n","        elif action == 1:\n","            new_x = cur_x + 1\n","        # left\n","        elif action == 2:\n","            new_y = cur_y - 1\n","        # right\n","        else:\n","            new_y = cur_y + 1\n","\n","        return int(new_x), int(new_y)\n","\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):\n","        '''\n","        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n","\n","        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n","        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n","        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n","        :return: action에 따른 리워드\n","        :rtype: float\n","        '''\n","\n","        # 바깥으로 나가는 경우\n","        if any(out_of_boundary):\n","            reward = obs_reward\n","                       \n","        else:\n","            # 장애물에 부딪히는 경우 \n","            if self.grid[new_x][new_y] == 0:\n","                reward = obs_reward  \n","\n","            # 현재 목표에 도달한 경우\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                reward = goal_reward\n","\n","            # 그냥 움직이는 경우 \n","            else:\n","                reward = move_reward\n","\n","        return reward\n","\n","    def step(self, action):\n","        ''' \n","        에이전트의 action에 따라 step을 진행한다.\n","        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n","        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n","\n","        :param action: 에이전트 행동\n","        :return:\n","            grid, 그리드\n","            reward, 리워드\n","            cumulative_reward, 누적 리워드\n","            done, 종료 여부\n","            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n","\n","        :rtype: numpy.ndarray, float, float, bool, bool/str\n","\n","        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n","        '''\n","        # print(self.local_target)\n","        self.terminal_location = self.local_target[0]\n","        cur_x,cur_y = self.curloc\n","        self.actions.append((cur_x, cur_y))\n","\n","        goal_ob_reward = False\n","        \n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n","\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n","\n","        # 바깥으로 나가는 경우 종료\n","        if any(out_of_boundary):\n","            self.done = True\n","            goal_ob_reward = True\n","            self.failcount += 1 # 실패 카운터\n","        else:\n","            # 장애물에 부딪히는 경우 종료\n","            if self.grid[new_x][new_y] == 0:\n","                self.done = True\n","                goal_ob_reward = True\n","                self.failcount += 1 # 실패 카운터\n","\n","            # 현재 목표에 도달한 경우, 다음 목표설정\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                self.successcount +=1 # 성공 카운터\n","\n","                # end point 일 때\n","                # if [new_x, new_y] == [9,4]:\n","                self.done = True # 목표에 도착하면 바로 done 수행\n","\n","                # self.local_target.remove(self.local_target[0]) \n","                # self.grid[cur_x][cur_y] = 1\n","                # self.grid[new_x][new_y] = -5\n","                goal_ob_reward = True\n","                self.curloc = [new_x, new_y]\n","            else:\n","                # 그냥 움직이는 경우 \n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                self.curloc = [new_x,new_y]\n","                \n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","\n","        # if self.done == True:\n","        #     if [new_x, new_y] == [9, 4]: #초기위치로 도달했을 때,\n","        #         if self.terminal_location == [9, 4]: # 목표와 같을때만 gif로 만들어줌\n","        #             # 완료되면 GIFS 저장\n","        #             goal_ob_reward = 'finish'\n","        #             height = 10\n","        #             width = 9\n","        #             display = Display(visible=False, size=(width, height))\n","        #             display.start()\n","        #\n","        #             start_point = (9, 4)\n","        #             unit = 50\n","        #             screen_height = height * unit\n","        #             screen_width = width * unit\n","        #             log_path = \"./logs\"\n","        #             data_path = \"./data\"\n","        #             render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n","        #             for idx, new_pos in enumerate(self.actions):\n","        #                 render_cls.update_movement(new_pos, idx+1)\n","        #\n","        #             render_cls.save_gif(self.epi)\n","        #             render_cls.viewer.close()\n","        #             display.stop()\n","        # 코랩환경이라 그림그리기는 주석처리함\n","        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward"],"metadata":{"id":"wXH4mTui29P6"},"id":"wXH4mTui29P6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QAgent_exploration():\n","    def __init__(self):\n","        self.eps = 0.0\n","        self.eps_decay = 0.00\n","    # 랜덤한 위치 -> 굳이 입실론을 통해 탐색하지 않아도 모든 지점에서 시작할 확률이 같으므로 입실론은 무의미\n","    \n","    def select_action(self, curloc, index):\n","        # Q-table은 단순 그리디로 이동, 목표에 따라 Q-Tables에서 인덱스를 알아야하므로 인덱스를 인수로 추가해줌\n","        x, y = curloc\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = Q_tables[index][x,y,:] # 목표의 인덱스 = 목표의 Q-Table\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition, index):\n","        s, a, r , s_prime = transition\n","        x, y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime, index) \n","        Q_tables[index][x,y,a] = Q_tables[index][x,y,a] + 0.1*(r + np.max(Q_tables[index][next_x, next_y, :]) - Q_tables[index][x,y,a])\n","        #Q러닝 업데이트 식을 이용, 마찬가지로 index를 인수로 추가해 table에 접근함\n","\n","    def anneal_eps(self):\n","        self.eps -= self.eps_decay\n","        self.eps = max(self.eps, 0.1) # 만에하나 한번도 못가본 장소가 있을까봐 0.1만 뒀음 (0이라도 상관없을 듯)"],"metadata":{"id":"j44AVv5K3Box"},"id":"j44AVv5K3Box","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    env = Simulator_exploration()\n","    agent = QAgent_exploration()\n","    episode = 10000\n","    files = pd.read_csv(os.path.join(local_path, \"./data/Q_Finder.csv\"))\n","\n","    for n_epi in range(episode): #에피소드 변수만큼 실행\n","\n","        for targetidx in range(len(files)): # 목표를 A~T까지 돌아가면서 수행 + 업데이트\n","            items = list(files.iloc[targetidx])[0]\n","            done = False\n","            if n_epi % 20 == 0:\n","                print(f'에피소드 :{n_epi} ')\n","            env.reset(targetidx)\n","            while not done: # 한 에피소드가 끝날때 까지\n","                s = env.curloc\n","                a = agent.select_action(s, targetidx)\n","                _, r, cum_reward, done, goal_ob_reward = env.step(a)\n","                s_prime = env.curloc\n","                agent.update_table((s, a, r, s_prime), targetidx)\n","                s = s_prime\n","            agent.anneal_eps()"],"metadata":{"id":"uJtgYLzM3EJj"},"id":"uJtgYLzM3EJj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"VdmSi3CW3FoR"},"id":"VdmSi3CW3FoR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","file = open(\"/content/drive/MyDrive/git_path_finding/path-finding-rl/data/save.txt\", \"wb\") \n","pickle.dump(Q_tables, file)\n","file.close"],"metadata":{"id":"lRFCnxSC3dcT"},"id":"lRFCnxSC3dcT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 텍스트로 Q테이블 저장해보려고 했는데 안됨... 알려주세요"],"metadata":{"id":"iiXaZNpf3fau"},"id":"iiXaZNpf3fau"},{"cell_type":"code","source":["with open('Q_tables(20000).pickle', 'wb') as f:\n","    pickle.dump(Q_tables, f)"],"metadata":{"id":"pcal7dkT3eR0"},"id":"pcal7dkT3eR0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 피클로 저장할 수 있다고 해서 저장 -> data 폴더에 넣어둠\n","경로 지정은 어떻게 할지 모르겠습니다.\n","\n","cd 해서 폴더 위치로 오면 될지도?"],"metadata":{"id":"dQWCyv7L3jSh"},"id":"dQWCyv7L3jSh"},{"cell_type":"code","source":["with open('Q_tables(20000).pickle','rb') as f:    #'rb'라는 점을 주의하라\n","    Q_tables = pickle.load(f)"],"metadata":{"id":"TJ9uym3V3i81"},"id":"TJ9uym3V3i81","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["위치 찾아서 위 테이블을 로드하면 어디서나 사용가능"],"metadata":{"id":"jN26Zvwe3mma"},"id":"jN26Zvwe3mma"},{"cell_type":"code","source":["print(Q_tables)"],"metadata":{"id":"P1s2VD0V3l24"},"id":"P1s2VD0V3l24","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Env"],"metadata":{"id":"sTEyspYL35ZZ"},"id":"sTEyspYL35ZZ"},{"cell_type":"code","source":["class Simulator:\n","    def __init__(self):\n","        '''\n","        height : 그리드 높이\n","        width : 그리드 너비 \n","        inds : A ~ Q alphabet list\n","        '''\n","        # Load train data\n","        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_test.csv\"))\n","        self.height = 10 #세로 10\n","        self.width = 9 # 가로 9  #A~Q까지 알파벳(target) 선언\n","        self.failcount = 0\n","        self.successcount = 0\n","    def set_box(self):\n","        '''\n","        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n","        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n","        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n","        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n","        '''\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","\n","        # 물건이 들어있을 수 있는 경우\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  #판다스 데이터를 튜플로 iter해준 것 같다. 행/열 위치정보로 각 알파벳이 표시됨\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100   #아이템이 없는 경우 100\n","\n","        # 물건이 실제 들어있는 경우\n","        order_item = list(set(inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100 # 아이템이 있는 경우 -100 <- 값이 지정되면 리턴으로 사용가능?\n","            # local target에 가야 할 위치 좌표 넣기\n","            self.local_target.append(\n","                [getattr(order_box, \"row\"),\n","                 getattr(order_box, \"col\")]\n","                )# 타겟 위치 지정 완료\n","            # print(order_box)\n","            \n","        # print(self.local_target)\n","        self.local_target.append([9,4])  # 마지막 타겟 = 출발위치\n","\n","        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n","\n","    def set_obstacle(self):\n","        '''\n","        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n","        '''\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0 #장애물 행,열 지정\n","\n","    def reset(self, epi):\n","        '''\n","        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n","\n","        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n","        :return: 초기셋팅 된 그리드\n","        :rtype: numpy.ndarray\n","        _____________________________________________________________________________________\n","        items : 이번 에피소드에서 가져와야하는 아이템들\n","        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n","        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n","        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n","        curloc : 현재 위치\n","        '''\n","\n","        # initial episode parameter setting\n","        self.epi = epi\n","        self.items = list(self.files.iloc[self.epi])[0]\n","        self.cumulative_reward = 0\n","        self.terminal_location = None\n","        self.local_target = []\n","        self.actions = [] # 정책에 따라 시간 순서대로 action 리스트에 append해주면 될듯\n","\n","        # initial grid setting\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\") #초기는 전부0\n","\n","        # set information about the gridworld\n","        self.set_box() #빈박스 + 타겟 박스\n","        self.set_obstacle() #장애물\n","\n","        # start point를 grid에 표시\n","        self.curloc = [9, 4] #시작 위치 초기화 (인덱스 0부터 시작이므로 10번째줄, 5번째 열임)\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5       \n","        \n","        self.done = False\n","        \n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):\n","        '''\n","        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n","        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n","        \n","        :param x: 에이전트의 현재 x 좌표\n","        :param y: 에이전트의 현재 y 좌표\n","        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n","        :rtype: int, int\n","        '''\n","        new_x = cur_x\n","        new_y = cur_y\n","        # up\n","        if action == 0:\n","            new_x = cur_x - 1\n","        # down\n","        elif action == 1:\n","            new_x = cur_x + 1\n","        # left\n","        elif action == 2:\n","            new_y = cur_y - 1\n","        # right\n","        else:\n","            new_y = cur_y + 1\n","\n","        return int(new_x), int(new_y)\n","\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):\n","        '''\n","        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n","\n","        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n","        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n","        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n","        :return: action에 따른 리워드\n","        :rtype: float\n","        '''\n","\n","        # 바깥으로 나가는 경우\n","        if any(out_of_boundary):\n","            reward = obs_reward\n","                       \n","        else:\n","            # 장애물에 부딪히는 경우 \n","            if self.grid[new_x][new_y] == 0:\n","                reward = obs_reward  \n","\n","            # 현재 목표에 도달한 경우\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                reward = goal_reward\n","\n","            # 그냥 움직이는 경우 \n","            else:\n","                reward = move_reward\n","\n","        return reward\n","\n","    def step(self, action):\n","        ''' \n","        에이전트의 action에 따라 step을 진행한다.\n","        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n","        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n","\n","        :param action: 에이전트 행동\n","        :return:\n","            grid, 그리드\n","            reward, 리워드\n","            cumulative_reward, 누적 리워드\n","            done, 종료 여부\n","            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n","\n","        :rtype: numpy.ndarray, float, float, bool, bool/str\n","\n","        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n","        '''\n","        # print(self.local_target)\n","        self.terminal_location = self.local_target[0]\n","        cur_x,cur_y = self.curloc\n","        self.actions.append((cur_x, cur_y))\n","\n","        goal_ob_reward = False\n","        \n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n","\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n","\n","        # 바깥으로 나가는 경우 종료\n","        if any(out_of_boundary):\n","            self.done = True\n","            goal_ob_reward = True\n","            self.failcount += 1\n","        else:\n","            # 장애물에 부딪히는 경우 종료\n","            if self.grid[new_x][new_y] == 0:\n","                self.done = True\n","                goal_ob_reward = True\n","                self.failcount += 1\n","\n","            # 현재 목표에 도달한 경우, 다음 목표설정\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                \n","\n","                # end point 일 때\n","                if [new_x, new_y] == [9,4]:\n","                    self.done = True # 마지막 목표가 설정되면 done을 True로 설정\n","                    self.successcount +=1\n","\n","                self.local_target.remove(self.local_target[0]) # 목표 달성시 목표 맨 앞 리스트를 지워준다.\n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                goal_ob_reward = True\n","                self.curloc = [new_x, new_y]\n","            else:\n","                # 그냥 움직이는 경우 \n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                self.curloc = [new_x,new_y]\n","                \n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","\n","        # if self.done == True:\n","        #     if [new_x, new_y] == [9, 4]: #초기위치로 도달했을 때,\n","        #         if self.terminal_location == [9, 4]: # 목표와 같을때만 gif로 만들어줌\n","        #             # 완료되면 GIFS 저장\n","        #             goal_ob_reward = 'finish'\n","        #             height = 10\n","        #             width = 9\n","        #             display = Display(visible=False, size=(width, height))\n","        #             display.start()\n","        #\n","        #             start_point = (9, 4)\n","        #             unit = 50\n","        #             screen_height = height * unit\n","        #             screen_width = width * unit\n","        #             log_path = \"./logs\"\n","        #             data_path = \"./data\"\n","        #             render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n","        #             for idx, new_pos in enumerate(self.actions):\n","        #                 render_cls.update_movement(new_pos, idx+1)\n","        #\n","        #             render_cls.save_gif(self.epi)\n","        #             render_cls.viewer.close()\n","        #             display.stop()\n","        \n","        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward"],"metadata":{"id":"v02BhxfV33iv"},"id":"v02BhxfV33iv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QAgent_exploitation(): #테스트 클래스, 마찬가지로 입실론은 안써도 된다.\n","    def __init__(self):\n","        self.eps = 0.0  # exploitation만 하면됨\n","        self.eps_decay = 0.00\n","\n","    \n","    def select_action(self, curloc, index):\n","        # index인수로 받아서 접근만 추가함\n","        x, y = curloc\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = Q_tables[index][x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition, index):\n","        s, a, r , s_prime = transition\n","        x, y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime, index) \n","        # Q_tables[index][x,y,a] = Q_tables[index][x,y,a] + 0.1*(r + np.max(Q_tables[index][next_x, next_y, :]) - Q_tables[index][x,y,a])\n","        # table은 더이상 업데이트 안해도됨, Q-Table만 따라 이동하면 된다.\n","\n","\n","    def anneal_eps(self):\n","        self.eps -= self.eps_decay\n","        self.eps = max(self.eps, 0.0)"],"metadata":{"id":"_RZFE5dC38x5"},"id":"_RZFE5dC38x5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    env = Simulator()\n","    agent = QAgent_exploitation()\n","    episode = len(env.files)\n","\n","    for n_epi in range(episode):\n","        alst = []\n","        acount = 0\n","        done = False\n","        env.reset(n_epi)\n","        while not done: # 에피소드가 끝날때 까지 반복\n","            targetidx = box_coor_index.index(env.local_target[0])\n","            s = env.curloc\n","            a = agent.select_action(s, targetidx)\n","            alst.append(a) #액션들을 alst에 모아둠 -> 나중에 시각화하려구\n","            acount += 1 #액션 수\n","            _, r, cum_reward, done, goal_ob_reward = env.step(a)\n","            s_prime = env.curloc\n","            agent.update_table((s, a, r, s_prime), targetidx)\n","            s = s_prime\n","        agent.anneal_eps()\n","        print(f'에피소드 :{n_epi}, 액션 수: {acount}, 리턴: {cum_reward}, 성공율 : {(env.successcount/(env.successcount+env.failcount+0.00001)*100):2f} %')\n","        if n_epi%20 ==0:\n","            print(alst) #20회마다 액션리스트만 print"],"metadata":{"id":"Dt7oIXYn3-0h"},"id":"Dt7oIXYn3-0h","execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"l5UWGMr_4A0P"},"id":"l5UWGMr_4A0P","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"PQL_main.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}
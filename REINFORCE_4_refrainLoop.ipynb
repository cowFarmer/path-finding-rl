{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"REINFORCE_4_refrainLoop","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4Qqf658erLbY7XPOIfr8t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"O_tvrV7qZ8tu","executionInfo":{"status":"error","timestamp":1653404163089,"user_tz":-540,"elapsed":9577,"user":{"displayName":"신휘정","userId":"12805037499998438573"}},"outputId":"d7cbd692-fa59-4721-8d22-576f731f995e"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 124\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/agileSoda/"],"metadata":{"id":"n9v-IWlyaRfV","executionInfo":{"status":"aborted","timestamp":1653404163088,"user_tz":-540,"elapsed":6,"user":{"displayName":"신휘정","userId":"12805037499998438573"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from string import ascii_uppercase\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","#Hyperparameters\n","learning_rate = 0.0002\n","gamma         = 0.98 # 무조건 끝나는 episode라면 gamma == 1도 가능하다. \n","\n","# reward\n","move_reward = 0.1\n","obs_reward = 0.1\n","goal_reward = 10\n","loop_reward = -1\n","print('reward:' , move_reward, obs_reward, goal_reward, loop_reward)\n","\n","# etc\n","max_cum_reward = -9999999999999999\n","\n","__file__ = '/content/drive/MyDrive/agileSoda/'\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))"],"metadata":{"id":"-qJDib4QaNIm","executionInfo":{"status":"aborted","timestamp":1653404144512,"user_tz":-540,"elapsed":9,"user":{"displayName":"신휘정","userId":"12805037499998438573"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Simulator:\n","    def __init__(self):\n","        '''\n","        height : 그리드 높이\n","        width : 그리드 너비 \n","        inds : A ~ Q alphabet list\n","        '''\n","        # Load train data\n","        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_train.csv\"))\n","        self.height = 10\n","        self.width = 9\n","        self.inds = list(ascii_uppercase)[:17]\n","\n","    def set_box(self):\n","        '''\n","        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n","        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n","        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n","        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n","        '''\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","\n","        # 물건이 들어있을 수 있는 경우\n","        for box in box_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100\n","\n","        # 물건이 실제 들어있는 경우\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100\n","            # local target에 가야 할 위치 좌표 넣기\n","            self.local_target.append(\n","                [getattr(order_box, \"row\"),\n","                 getattr(order_box, \"col\")]\n","                )\n","\n","        self.local_target.sort()\n","        self.local_target.append([9,4]) \n","\n","        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n","\n","    def set_obstacle(self):\n","        '''\n","        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n","        '''\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n","\n","    def reset(self, epi):\n","        '''\n","        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n","        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n","        :return: 초기셋팅 된 그리드\n","        :rtype: numpy.ndarray\n","        _____________________________________________________________________________________\n","        items : 이번 에피소드에서 가져와야하는 아이템들\n","        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n","        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n","        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n","        curloc : 현재 위치\n","        '''\n","\n","        # initial episode parameter setting\n","        self.epi = epi\n","        self.items = list(self.files.iloc[self.epi])[0]\n","        self.cumulative_reward = 0\n","        self.terminal_location = None\n","        self.local_target = []\n","        self.actions = []\n","\n","        # initial grid setting\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\")\n","\n","        # set information about the gridworld\n","        self.set_box()\n","        self.set_obstacle()\n","\n","        # start point를 grid에 표시\n","        self.curloc = [9, 4]\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5\n","\n","        self.done = False\n","\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):\n","        '''\n","        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n","        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n","        \n","        :param x: 에이전트의 현재 x 좌표\n","        :param y: 에이전트의 현재 y 좌표\n","        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n","        :rtype: int, int\n","        '''\n","        new_x = cur_x\n","        new_y = cur_y\n","        # up\n","        if action == 0:\n","            new_x = cur_x - 1\n","        # down\n","        elif action == 1:\n","            new_x = cur_x + 1\n","        # left\n","        elif action == 2:\n","            new_y = cur_y - 1\n","        # right\n","        else:\n","            new_y = cur_y + 1\n","\n","        return int(new_x), int(new_y)\n","\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):\n","        '''\n","        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n","        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n","        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n","        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n","        :return: action에 따른 리워드\n","        :rtype: float\n","        '''\n","\n","        # 바깥으로 나가는 경우\n","        if any(out_of_boundary):\n","            reward = obs_reward\n","\n","        else:\n","            # 장애물에 부딪히는 경우 \n","            if self.grid[new_x][new_y] == 0:\n","                reward = obs_reward  \n","\n","            # 현재 목표에 도달한 경우\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                reward = goal_reward\n","\n","            # 그냥 움직이는 경우 \n","            else:\n","                reward = move_reward\n","\n","        return reward\n","\n","    def step(self, action):\n","        ''' \n","        에이전트의 action에 따라 step을 진행한다.\n","        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n","        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n","        :param action: 에이전트 행동\n","        :return:\n","            grid, 그리드\n","            reward, 리워드\n","            cumulative_reward, 누적 리워드\n","            done, 종료 여부\n","            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n","        :rtype: numpy.ndarray, float, float, bool, bool/str\n","        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n","        '''\n","\n","        self.terminal_location = self.local_target[0]\n","        cur_x,cur_y = self.curloc\n","        self.actions.append((cur_x, cur_y))\n","\n","        goal_ob_reward = False\n","\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n","\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n","\n","        # 바깥으로 나가는 경우 종료\n","        if any(out_of_boundary):\n","            self.done = True\n","            goal_ob_reward = True\n","        else:\n","            # 장애물에 부딪히는 경우 종료\n","            if self.grid[new_x][new_y] == 0:\n","                self.done = True\n","                goal_ob_reward = True\n","\n","            # 현재 목표에 도달한 경우, 다음 목표설정\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","\n","                # end point 일 때\n","                if [new_x, new_y] == [9,4]:\n","                    self.done = True\n","\n","                self.local_target.remove(self.local_target[0])\n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                goal_ob_reward = True\n","                self.curloc = [new_x, new_y]\n","            else:\n","                # 그냥 움직이는 경우 \n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                self.curloc = [new_x,new_y]\n","\n","        # 보상 계산하기\n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        \n","        # 10번의 행동동안 state 종류가 n개 이하면  -보상 주도록하기\n","        n = 3\n","        reward = self.refrain_loop(reward, n)\n","\n","        self.cumulative_reward += reward\n","\n","        if self.done == True:\n","            if [new_x, new_y] == [9, 4]:\n","                if self.terminal_location == [9, 4]:\n","                    # 완료되면 GIFS 저장\n","                    goal_ob_reward = 'finish'\n","\n","        return self.get_current_state(), reward, self.cumulative_reward, self.done, goal_ob_reward\n","\n","    def get_current_state(self):\n","        state = (self.curloc[0], self.curloc[1], self.local_target[0][0], self.local_target[0][1])\n","        return np.array(state)\n","        \n","    # 10번의 행동동안 state 종류가 n개 이하면  -보상 주도록하기\n","    def refrain_loop(self, reward, n):\n","        if len(self.actions) <= 10:\n","            return reward\n","        \n","        start_idx = len(self.actions) - 10\n","        temp = self.actions[start_idx:start_idx+10]\n","        temp = set(temp)\n","        if len(temp) <= n:\n","            return reward + loop_reward\n","        else:\n","            return reward"],"metadata":{"id":"IRdKsMYVaTJo","executionInfo":{"status":"aborted","timestamp":1653404144513,"user_tz":-540,"elapsed":9,"user":{"displayName":"신휘정","userId":"12805037499998438573"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Policy(nn.Module):\n","    def __init__(self):\n","        super(Policy, self).__init__()\n","        self.data = []\n","        \n","        # weight를 정의  \n","        self.fc1 = nn.Linear(4, 128) # fully connected first layer\n","        self.fc2 = nn.Linear(128, 4) # fully connected second layer\n","\n","        # optimizer\n","        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","        \n","    # 얜 언제 불러지는걸까요?\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x)) # first layer 통과 후 비선형 함수 통과 \n","        x = F.softmax(self.fc2(x), dim=0) # second layer 통과 후 sofrmax로 합이 1이 되는 확률로 만들어\n","        return x\n","      \n","    # s, a pair 저장\n","    def put_data(self, item): \n","        self.data.append(item)\n","        \n","    # 2-(C) presudo code\n","    def train_net(self):\n","        R = 0\n","        self.optimizer.zero_grad()\n","        for r, prob in self.data[::-1]: # return을 재귀적으로 뒤에서부터 더하면서 계산\n","            R = r + gamma * R # Return Gt\n","            loss = -torch.log(prob) * R # loss function\n","            loss.backward() # gradient 계산\n","        self.optimizer.step() # Adam optimizer update 진해\n","        self.data = []\n"],"metadata":{"id":"9T__FpRGao6r","executionInfo":{"status":"aborted","timestamp":1653404144514,"user_tz":-540,"elapsed":10,"user":{"displayName":"신휘정","userId":"12805037499998438573"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MAIN\n","\n","files = pd.read_csv('./data/factory_order_train.csv')\n","episodes = len(files)\n","\n","env = Simulator()\n","pi = Policy()\n","\n","max_cum_reward = -99999\n","\n","# ########################################################\n","# 액션 저장하는 txt파일 만들기\n","my_file_name='REINFORCE_4_refrainLoop_'+str(move_reward)+'_'+str(obs_reward)+'_'+str(goal_reward)\n","f = open(my_file_name+'.txt', 'w')\n","# ########################################################\n","\n","for epi in range(episodes):\n","    state = env.reset(epi)\n","    state = env.get_current_state()\n","    print('\\n\\n▶▶▶{}번째 에피소드 시작, env 초기화 완료.'.format(epi))\n","\n","    # 아이템 리스트 확인\n","    items = list(files.iloc[epi])[0]\n","    print('가져와야 할 아이템 : ' + list(files.iloc[epi])[0] + '\\n')\n","    items = env.local_target\n","\n","    done = False\n","    isFinished=False\n","\n","    while not done:\n","        prob = pi(torch.from_numpy(state).float())\n","        m = Categorical(prob) # prob 중 높은 확률 값을 리턴\n","        action = m.sample()\n","        \n","        state_prime, reward, cumul ,done, goal_ob_reward = env.step(action.item())\n","        pi.put_data((reward, prob[action]))\n","        print('action', action, 'reward', reward)\n","        \n","        state = state_prime\n","        \n","        if goal_ob_reward == 'finished':\n","            isFinished = True\n","    \n","    pi.train_net()\n","    max_cum_reward = max_cum_reward if max_cum_reward > cumul else cumul\n","    print('▶▶▶ ',epi, 'max cum reward', max_cum_reward, 'isFinished', isFinished, '\\n\\n')\n","  \n","\n","#########################################    \n","    if len(env.actions) > 5:\n","        f.write(str(epi)+'/'+str(items)+'/'+str(cumul)+'/'+str(isFinished)+'\\n')\n","        f.write(str(env.actions))\n","        f.write('\\n')\n","\n","f.close()\n","#########################################\n"],"metadata":{"id":"4rFuZYN5atew","executionInfo":{"status":"aborted","timestamp":1653404145191,"user_tz":-540,"elapsed":687,"user":{"displayName":"신휘정","userId":"12805037499998438573"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/agileSoda/'\n","weight_name = path+my_file_name+'.pt'\n","torch.save(pi.state_dict(), my_file_name)"],"metadata":{"id":"9BHQYlcQcAXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = path+my_file_name+'.pt'\n","torch.save(pi, model_name)"],"metadata":{"id":"Pk1c-pbBKTpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"RnEibUYRKUff"},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","__file__ = '/content/drive/MyDrive/aiffelthon/data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRpfPDrdk9wB","executionInfo":{"status":"ok","timestamp":1653538100741,"user_tz":-540,"elapsed":22342,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}},"outputId":"d9e3a8f3-14b8-48aa-9194-f23282cb324b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from string import ascii_uppercase\n","#from draw_utils import *\n","#from pyglet.gl import *\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","###################################\n","# reward 조정\n","move_reward = -1  # 0.1\n","obs_reward = -10  # 0.1\n","goal_reward = 10  # 10\n","###################################\n","# train or test 모드 지정\n","train_mode = True\n","###################################\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","\n","class Simulator:\n","    def __init__(self):\n","        #height: 그리드 높이; width: 그리드 너비; inds: A ~ Q alphabet list\n","        #######################################################################################\n","        # Load train or test data\n","        if train_mode:  # 훈련 데이타 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","            print('data/factory_order_train.csv used')\n","        else:  # 테스트 데이터 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_test.csv\"))\n","            print('data/factory_order_test.csv used')\n","        #######################################################################################        \n","        self.height = 10\n","        self.width = 9\n","        self.inds = list(ascii_uppercase)[:17]\n","\n","    def set_box(self):  # (수정사항) 목표물 그리드값 -100으로 지정하는 부분 코멘트 처리\n","        '''\n","        아이템들이 있을 위치를 미리 정해 놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n","        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n","        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n","        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야 하므로)가 들어가게 된다.\n","        '''\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        # 물건이 들어있을 수 있는 경우: 선반 위치. 그리드값 = 100\n","        for box in box_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100\n","        # 물건이 실제 들어있는 경우: 정해진 목표. 그리드값 = -100\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n","            ###########################################################################################################################\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100  # >>> main()에서 목표물 하나만 -100으로 바꾼다!!!\n","            ###########################################################################################################################\n","            # local_target에 가야 할 위치 좌표 넣기\n","            self.local_target.append([getattr(order_box, \"row\"), getattr(order_box, \"col\")])\n","        # self.local_target.sort() # 불필요. 인풋 데이터 A, B, C순 정렬되어 있음... 정렬 필요시 코드 바꾸어야 함\n","        ##################################################################\n","        self.local_target.append([9,4]) # 최종 목적지 (출발점) 추가\n","        ##################################################################\n","        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n","\n","    def set_obstacle(self):\n","        '''\n","        장애물이 있어야 하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n","        '''\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n","        \n","    def reset(self, epi):\n","        '''\n","        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n","        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n","        :return: 초기 셋팅된 그리드\n","        :rtype: numpy.ndarray\n","        _____________________________________________________________________________________\n","        items: 이번 에피소드에서 가져와야 하는 아이템들\n","        terminal_location: 현재 에이전트가 찾아가야 하는 목적지\n","        local_target: 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n","        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n","        curloc: 현재 위치\n","        '''\n","        # initial episode parameter setting\n","        self.epi = epi  # 에피소드 번호 받기\n","        self.items = list(self.files.iloc[self.epi])[0]  # 해당 에피소드의 items를 가져 옴. 예, [ 'H', 'L', 'M']\n","        self.cumulative_reward = 0\n","        #self.terminal_location = None\n","        self.local_target = []  # 목적지 리스트\n","        self.actions = []  # 지나 온 경로 리스트\n","        ###### initial grid setting\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\")  # 그리드값 = 1... 10x9 크기의 그리드월드\n","        ###### set information about the gridworld\n","        self.set_box()  # 에이전트가 이번에 방문해야 할 좌표들 저장. 예, [ [0,3], [0,7], [0,8], [9,4] ]  # 그리드값 = 100(선반)\n","        self.set_obstacle()  # 그리드값 = 0\n","        ###### start point를 grid에 표시\n","        self.curloc = [9, 4]  # 출발점\n","        #self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5  # 현재 위치(출발점) 그리드값 = -5\n","        self.done = False  # 종료 여부\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):\n","        '''\n","        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n","        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n","        :param x: 에이전트의 현재 x 좌표\n","        :param y: 에이전트의 현재 y 좌표\n","        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n","        :rtype: int, int\n","        '''\n","        new_x = cur_x\n","        new_y = cur_y\n","        # up\n","        if action == 0:\n","            new_x = cur_x - 1\n","        # down\n","        elif action == 1:\n","            new_x = cur_x + 1\n","        # left\n","        elif action == 2:\n","            new_y = cur_y - 1\n","        # right\n","        else:\n","            new_y = cur_y + 1\n","        return int(new_x), int(new_y)\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):  # 현재 목표에 도달한 경우... 코드 수정했음!!!\n","        '''\n","        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n","        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n","        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n","        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n","        :return: action에 따른 리워드\n","        :rtype: float\n","        '''\n","        # 바깥으로 나가는 경우\n","        if any(out_of_boundary):\n","            reward = obs_reward   # -10점\n","        else:\n","            # 장애물에 부딪히는 경우 \n","            if self.grid[new_x][new_y] == 0:\n","                reward = obs_reward  # -10점\n","            #####################################################################\n","            # >>>>>>>>>> 현재 목표에 도달한 경우... 코드 수정했음!!!\n","            elif new_x == self.end_x and new_y == self.end_y:\n","                reward = goal_reward  # 10점\n","            #####################################################################\n","            # 그냥 움직이는 경우 \n","            else:\n","                reward = move_reward  # -1점\n","        return reward\n","\n","    def step(self, action):  # 현재 목표에 도달한 경우... 수정 했음!!\n","        ''' \n","        에이전트의 action에 따라 step을 진행한다.\n","        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n","        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n","        :param action: 에이전트 행동\n","        :return:\n","            grid, 그리드\n","            reward, 리워드\n","            cumulative_reward, 누적 리워드\n","            done, 종료 여부\n","            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n","        :rtype: numpy.ndarray, float, float, bool, bool/str\n","        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n","        '''\n","        #self.terminal_location = self.local_target[0]  # (중간) 목적지 좌표 지정\n","        cur_x,cur_y = self.curloc  # 현재 위치 지정\n","        self.actions.append((cur_x, cur_y))  # 현재 위치를 (지나 온) 경로 리스트에 추가\n","        goal_ob_reward = False  # 0\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 다음 위치 받기\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # 밖으로 나가면 OB = True 판정\n","        # 바깥으로 나가는 경우 종료\n","        if any(out_of_boundary):\n","            #print('OB')\n","            #원 위치\n","            #new_x = cur_x\n","            #new_y = cur_y\n","            self.done = True  # 종료\n","            self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 = 1 (초기값)로 바꿈\n","            #goal_ob_reward = True  # 1\n","        else:\n","            # 장애물에 부딪히는 경우 종료\n","            if self.grid[new_x][new_y] == 0:\n","                #print('장애물')\n","                #원 위치\n","                #new_x = cur_x\n","                #new_y = cur_y\n","                self.done = True  # 종료\n","                self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 = 1 (초기값)로 바꿈\n","                #goal_ob_reward = True  # 1\n","            #################################################################################\n","            # 현재 목표점에 도착한 경우 종료\n","            #################################################################################\n","            elif new_x == self.end_x and new_y == self.end_y:\n","                #print('목적지 도착')\n","                self.done = True  # 종료\n","                #self.local_target.remove(self.local_target[0])  # 다음 목표 설정을 위해 달성한 목표 제거\n","                #self.grid[cur_x][cur_y] = 1  # 현재의 그리드값 = 1로 바꿈\n","                #self.grid[new_x][new_y] = -5  # 이동한 그리드값 = -5로 바꿈\n","                goal_ob_reward = True  # 1\n","                #self.curloc = [new_x, new_y]  # 현재 위치를 이동한 그리드 위치로 변경\n","            else:\n","                # 그냥 움직이는 경우\n","                #print('이동')\n","                self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 = 1 (초기값)로 바꿈\n","                self.grid[new_x][new_y] = -5  # 이동한 그리드값 = -5 (현 위치)로 바꿈\n","                self.curloc = [new_x,new_y]\n","\n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQRnyv3UlBcu","executionInfo":{"status":"ok","timestamp":1653553204626,"user_tz":-540,"elapsed":513,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}},"outputId":"35b5ea51-2a71-43f3-d8ef-e93f077b87cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -1 -10 10\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwgy-OBbk1b4"},"outputs":[],"source":["import gym\n","import collections\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","#Hyperparameters\n","learning_rate = 1e-3  # 0.0005\n","gamma         = 0.90  # 0.98\n","buffer_limit  = 50000\n","batch_size    = 32\n","\n","class ReplayBuffer():\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","    \n","    def put(self, transition):\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):\n","        mini_batch = random.sample(self.buffer, n)\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","        \n","        for transition in mini_batch:\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","\n","        #return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               #torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               #torch.tensor(done_mask_lst)\n","        ##############################################################################\n","        ## *_list를 np.array(*_lst)로 변경\n","        ##############################################################################\n","        return torch.tensor(np.array(s_lst), dtype=torch.float), torch.tensor(np.array(a_lst)), \\\n","               torch.tensor(np.array(r_lst)), torch.tensor(np.array(s_prime_lst), dtype=torch.float), \\\n","               torch.tensor(np.array(done_mask_lst))\n","    \n","    def size(self):\n","        return len(self.buffer)\n","\n","class Qnet(nn.Module):\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        in_size = 90  # input 크기\n","        L1 = 128  \n","        L2 = 512\n","        out_size  = 4\n","\n","        self.fc1 = nn.Linear(in_size, L1)\n","        self.fc2 = nn.Linear(L1, L2)\n","        self.fc3 = nn.Linear(L2, out_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","      \n","    def sample_action(self, obs, epsilon):\n","        out = self.forward(obs)\n","        coin = random.random()\n","        if coin < epsilon:\n","            return random.randint(0,3)\n","        else : \n","            return out.argmax().item()\n","            \n","def train(q, q_target, memory, optimizer):  # 경로 하나가 완료되면 실행\n","    for i in range(10):  # 미니배치를 10번 뽑아서 그래디언트 업데이트\n","        s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 리플레이 버퍼에서 미니배치를 뽑고... batch_size=32\n","\n","        q_out = q(s)\n","        q_a = q_out.gather(1,a)\n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","        target = r + gamma * max_q_prime * done_mask\n","        loss = F.smooth_l1_loss(q_a, target)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    return loss.data"]},{"cell_type":"code","source":["\n","#def main():\n","tz = pytz.timezone('Asia/Seoul')\n","    ####################################################################################\n","    #for n_epi in range(1000): # range()의 인수로 len(env.files) 사용하면 됨 (=39,999)\n","    ####################################################################################\n","env = Simulator()\n","q = Qnet()\n","q_target = Qnet()\n","q_target.load_state_dict(q.state_dict())\n","memory = ReplayBuffer()\n","optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","    #files = pd.read_csv(\"./data/factory_order_train.csv\") # Simulator 클래스 __init__(self):에서 읽음\n","print('len(env.files):', len(env.files))\n","\n","\n","# >>>>>>>>> 전체 훈련 데이터에 대한 최적 경로 찾기 훈련 시작 <<<<<<<<<<<<<<<<<<<<<\n","for n_epi in range(1000): # range()의 인수로 len(env.files) 사용하면 됨 (=39,999)  # <<<<<<<<<<<<<<<<<<<< 일단 1000개 경로 훈련!!!!\n","\n","    env.grid = env.reset(n_epi)  # env reset: 에피소드 번호에 해당하는 목표물 리스트 env.local_target 생성, 그리드맵 생성\n","    print('▶ Episode #', n_epi,':', list(env.files.iloc[n_epi])[0])\n","    #print('list(env.files.iloc[',0,'])[0]:', list(env.files.iloc[0])[0])\n","    print('env.local_target:', env.local_target)\n","    #print(\"reset(0) 결과로 받은 최초 그리드맵\")\n","    #print(env.grid)\n","\n","    # >>>>>>>>> 하나의 에피소드에 대한 대한 최적 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","    for n_target in range(len(env.local_target)):\n","        sync_freq = 500  # q 네트워크 파라미터 q_target 네트워크 복사 주기 <<<<<<<<<<<<<\n","        path_length = 0  # 경로 길이 초기화\n","        # >>>>>>>>>>>>>> 출발지, 목적지 지정\n","        if n_target == 0:\n","            start_point = [9,4]  # 첫 번째 출발지 지정\n","            print('출발지:', start_point, end=\" → \")\n","        else:\n","            start_point = env.local_target[n_target-1]\n","            print('출발지:', start_point, end=\" → \")\n","        end_point = env.local_target[n_target]  # 목적지 지정\n","        print('목적지:', end_point)\n","        loss = None\n","        cnt = 0  # for loop 조기 종료를 위한 카운터\n","\n","        # >>>>>>>>>>>>>>>>>>>>> 하나의 목적지에 대한 최적 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","        for iter in range(5000): # 최적 반복 횟수 결정 필요. 카트폴: 10000, 논문: 5000  <<<<<<<<<<<<<<\n","            epsilon = max(0.01, 0.08 - 0.01*(iter/200)) #Linear annealing from 8% to 1%\n","            # 출발지, 목적지 좌표 지정\n","            env.x, env.y = start_point                         # 출발지 좌표\n","            env.grid[int(env.x)][int(env.y)] = -5              # 출발지 그리드값 = -5\n","            env.end_x, env.end_y = end_point                   # 목적지 좌표\n","            env.grid[int(env.end_x)][int(env.end_y)] = -200    # 목적지 그리드값 = -200\n","            # 현 위치를 출발지로 reset\n","            env.curloc = start_point\n","            # 목적지 도착 flag (done), 인풋 데이터 (s) 등 초기화\n","            env.done = False\n","            done = False\n","            env.actions = []\n","            #######################################################################\n","            ## 인풋 정보: 그리드맵... 전체 1로 초기화 후, 선반 100, 장애물 0, 목적지 리스트: -100, 이번 목적지 -200, 현 위치 -5\n","            #######################################################################\n","            grid_map = env.grid.reshape(-1)          # 그리드 맵\n","            #print('현 위치(-5), 목적지(-200) 표시 그리드맵: >>> 시작')\n","            #print(grid_map.reshape(10,9))\n","            #######################################################################\n","            s = np.array(grid_map)\n","            while not done:  # 한 칸씩 이동. 목적지 도착하면 종료\n","                #print('>>>>>>>>>>>> while 시작점... current location:', env.curloc)\n","                #####################################################################################\n","                # (option) 출발점에서는 무조건 위로 올라간다 (action=0)\n","                if env.curloc == [9,4]:\n","                    a = 0\n","                else:\n","                    a = q.sample_action(torch.from_numpy(s).float(), epsilon)  # e-greedy로 액션 선택... 네트워크 결과값으로\n","                #####################################################################################\n","                #a = q.sample_action(torch.from_numpy(s).float(), epsilon)  # e-greedy로 액션 선택... 네트워크 결과값으로\n","                grid, r, cum_reward, done, goal_ob_reward = env.step(a)\n","                new_grid_map = grid.reshape(-1)\n","                #print('액션=', a, '한 칸 이동 결과', 'done=', done)\n","                #print(new_grid_map.reshape(10,9))\n","                s_prime = np.array(new_grid_map)\n","                done_mask = 0.0 if done else 1.0\n","                memory.put((s,a,r/100.0,s_prime, done_mask))\n","                s = s_prime\n","                if done:\n","                    break  #  while loop 빠져 나가기 \n","                # while loop 종료\n","            path_length += len(env.actions)\n","            ############################################################################\n","            ## memory.size(): 2000... 이후 훈련 시작\n","            ############################################################################   \n","            if memory.size() > 2000:\n","                loss = train(q, q_target, memory, optimizer)\n","                ## for loop 종료 조건 만들어 넣기 필요 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","                if loss < 1e-3:\n","                    cnt += 1\n","                else:\n","                    cnt = 0\n","                if cnt > 10:  # 10회 연속 loss값이 0.001 보다 작으면\n","                    print('10회 연속 loss값이 0.001 보다 작음 @ iter#', iter)\n","                    break  #  for loop 빠져 나가기\n","            if iter % sync_freq == 0 and iter != 0:\n","                #cur_time = datetime.now(tz)\n","                #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","                #print('▶ Episode #', iter, 'start time:', simple_cur_time, end='→')\n","                q_target.load_state_dict(q.state_dict())  # 얼마나 자주 q 네트워크 파라미터를 q_target 네트워크로 복사할 것인가???\n","                print(\"iteration:{}, 평균 보상: {:.1f}, 버퍼크기: {}, 엡실론: {:.1f}%\".\\\n","                      format(iter, env.cumulative_reward/sync_freq, memory.size(), epsilon*100))\n","                print('loss:', loss, '→ 평균 경로 길이:', path_length/sync_freq)\n","                path_length = 0.0\n","                env.cumulative_reward = 0.0\n","            #torch.save(q, '/content/drive/MyDrive/aiffelthon/data/model.pt')\n","        print('for loop 종료')\n","        # >>>>>>>>>>>>>>>>>>>>> 하나의 목적지에 대한 최적 경로 찾기 종료 <<<<<<<<<<<<<<<<<<<<<\n","    # >>>>>>>>> 하나의 에피소드에 대한 대한 최적 경로 찾기 종료 <<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>> 전체 훈련 데이터에 대한 최적 경로 찾기 훈련 종료 <<<<<<<<<<<<<<<<<<<<<"],"metadata":{"id":"j9qFCqbbjf3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train 모드 실행\n","#main()"],"metadata":{"id":"HmOA2SJgUiR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## test 메서드 추가 #################################################################################\n","def test():\n","    model = torch.load('/content/drive/MyDrive/aiffelthon/data/model.pt')\n","    train_mode = False  # False\n","    tz = pytz.timezone('Asia/Seoul')\n","    env = Simulator()\n","    q = Qnet()\n","    s_before = None\n","    #sync_freq = 1  # \n","    score = 0.0\n","\n","    for n_epi in range(1): # range()의 인수로 len(env.files) 사용하면 됨 (=1225)\n","        env.reset(n_epi)\n","        print('env.local_target:', env.local_target)\n","        ############################################\n","        ## 인풋 정보(obs)에 경로 길이 추가\n","        #path_length = len(env.local_target)\n","        #print('path_length', path_length)\n","        #s_before = env.curloc\n","        #s_before.append(path_length)\n","        ############################################        \n","        s = np.array(s_before)\n","        done = False\n","\n","        while not done:  # OB, 장애물, 최종 목표 도달시 종료\n","            #####################################################################################\n","            # (option) test 경우 추가: 출발점에서는 무조건 위로 올라간다 (action=0)\n","            x, y, _ = s\n","            if [x,y] == [9,4]:\n","                a = 0\n","            else:\n","                a = q.test_action(torch.from_numpy(s).float())  # test_action 수행\n","            #####################################################################################\n","            #a = q.test_action(torch.from_numpy(s).float())  # test_action 수행\n","            #####################################################################################\n","            obs, r, cum_reward, done, goal_ob_reward = env.step(a)\n","            s_prime = np.array(env.curloc)\n","            s = s_prime\n","            score += r\n","\n","        # sync_freq (=1) 마다 현황 디스플레이하고 q_target 업데이트 및 score 초기화\n","        if n_epi % sync_freq == 0 and n_epi != 0:\n","            cur_time = datetime.now(tz)\n","            simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","            print('▶ Episode #', n_epi, end=' → ')\n","            print('Score =', score, end='...')\n","            print('경로 길이:', len(env.actions), end=', ')\n","            if goal_ob_reward == 'finish':\n","                print('성공 여부: ', goal_ob_reward)\n","            else:\n","                print('성공 여부 : 실패', goal_ob_reward)\n","        \n","        score = 0.0\n","        print('→ pred_path:', env.actions)\n","\n","    # 모든 에피소드 종료 후 결과 디스플레이\n","    # 코드 추가할 것!!!\n","'''"],"metadata":{"id":"drl-vDwuQWNC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"name":"as_DQN_0526.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
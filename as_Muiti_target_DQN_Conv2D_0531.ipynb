{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27263,"status":"ok","timestamp":1654006129808,"user":{"displayName":"노현호","userId":"15410213599666758892"},"user_tz":-540},"id":"lRpfPDrdk9wB","outputId":"0f19de21-76fe-44c7-a212-daf2c0fbca92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","__file__ = '/content/drive/Othercomputers/MacBook_Air/path-finding-rl/data'"]},{"cell_type":"markdown","metadata":{"id":"UHFqTLgjMNbJ"},"source":["## Env\n","- 경로 이동 제한 사항 반영: del apply_action() 수정\n","- def grid_box(self):  # 그리드 박스 초기화 용도로 정의\n","- 그리드값 변경\n","  - 목표물: 10, 9, 8, ...\n","  - 현 위치: 1\n","  - 기본: 0\n","  - 선반: -1\n","  - 장애물: -10\n","- 종료 조건\n","  - 그리드월드 밖으로 나가는 경우 종료하지 않음\n","  - 장애물에 부딪히는 경우 종료"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1973,"status":"ok","timestamp":1654006502222,"user":{"displayName":"노현호","userId":"15410213599666758892"},"user_tz":-540},"id":"tQRnyv3UlBcu","outputId":"f55d6e6d-1392-4cff-cb16-f0e04af1050c"},"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -0.1 -10 100\n"]}],"source":["from string import ascii_uppercase\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","###################################\n","# reward 조정\n","move_reward = -0.1  # 0.1\n","obs_reward = -10  # 0.1\n","goal_reward = 100  # 10\n","###################################\n","# train / test 모드 지정\n","train_mode = True\n","###################################\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","class Simulator:\n","    def __init__(self):\n","        # Load train or test data\n","        if train_mode:  # 훈련 데이타 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","            print('data/factory_order_train.csv used')\n","        else:           # 테스트 데이터 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_test.csv\"))\n","            print('data/factory_order_test.csv used')\n","       \n","        self.height = 10  # 그리드 높이\n","        self.width = 9    #  그리드 너비\n","        self.inds = list(ascii_uppercase)[:17]  # A ~ Q alphabet list\n","\n","    def set_box(self):  # 선반 위치, 목적지 그리드값 설정.  목적지 리스트 생성\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = -1\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = -1\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = 10, 9, 8, ...\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","            self.local_target.append([getattr(order_box, \"row\"), getattr(order_box, \"col\")])  # 목적지 리스트 생성: local_target\n","        # self.local_target.sort() # 불필요. 인풋 데이터 A, B, C순 정렬되어 있음... 정렬 필요시 코드 바꾸어야 함\n","        self.local_target.append([9,4]) # 목적지 리스트에 최종 목적지(9,4) 추가\n","        self.local_target_original = self.local_target  # gif 생성을 위해 추가.  에피소드의 경로 저장\n","        self.target_length = len(self.local_target_original)  # 목적지 그리드값 10, 9, 8, ... 지정 위해 추가\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","\n","    def set_obstacle(self):  # 장애물 위치 그리드값 설정 = -10\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = -10\n","\n","    def grid_box(self):  # 그리드 박스 초기화 용도로 정의: 선반, 목적지, 장애물 그리드값 초기화\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = -1\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = -1\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = 10, 9, 8, ...\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","        self.set_obstacle()  # 장애물 위치 그리드값 설정 = 0\n","        \n","    def reset(self, epi):  # 에피소드 시작시 12개 값 초기화\n","        self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        ####################\n","        coin = random.randint(0,16)\n","        # print('coin', coin, chr(coin+65))\n","        self.items = chr(coin+65)\n","        ####################\n","        self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        self.terminal_location = None                             #4. 최초 목적지\n","        self.local_target = []                                    #5. 목적지 리스트 초기화\n","        self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        self.grid = np.zeros((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(0) 초기화\n","        self.set_box()                                            #8. 선반 위치(-1), 목적지(10, 9, 8, ...) 그리드값 설정.  목적지 리스트 생성\n","        self.set_obstacle()                                       #9. 장애물 그리드값(-10) 설정\n","        ######################\n","        # print('최초 그리드맵:')  # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = 1  #11. 현재 위치(출발점) 그리드값(1) 세팅\n","        self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        self.goal_ob_reward = False                      # (추가) #13. 최종 목적지 도착 여부 = False\n","        ######################\n","        # print('현 위치 추가:')   # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):  # action에 따른 새 좌표값 반환\n","        new_x = cur_x\n","        new_y = cur_y\n","        if action == 0:          # up\n","            #new_x = cur_x - 1\n","            new_x = self.move_up(cur_x, cur_y, new_x, new_y)\n","        elif action == 1:        # down\n","            #new_x = cur_x + 1\n","            new_x = self.move_down(cur_x, cur_y, new_x, new_y)\n","        elif action == 2:        # left\n","            #new_y = cur_y - 1\n","            new_y = self.move_left(cur_x, cur_y, new_x, new_y)\n","        else:                    # right\n","            #new_y = cur_y + 1\n","            new_y = self.move_right(cur_x, cur_y, new_x, new_y)\n","        return int(new_x), int(new_y)\n","    \n","    # >>> 현재 위치에서 이동이 불가한 위치 추가 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def move_up(self, cur_x, cur_y, new_x, new_y):  # action == 0:\n","        if cur_x in [6,5,4,3,2] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x - 1\n","        return new_x\n","    def move_down(self, cur_x, cur_y, new_x, new_y): # action == 1:\n","        if cur_x in [1,2,3,4,5] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x + 1\n","        return new_x\n","    def move_left(self, cur_x, cur_y, new_x, new_y): # left elif action == 2:\n","        if cur_y in [1,2,3,4,5,6,7,8] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y - 1\n","        return new_y\n","    def move_right(self, cur_x, cur_y, new_x, new_y): # right else: action == 3:\n","        if cur_y in [0,1,2,3,4,5,6,7] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y + 1\n","        return new_y\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):  # action에 의해 이동시 얻는 보상\n","        if any(out_of_boundary):                          # 바깥으로 나가는 경우\n","            reward = obs_reward                               # -10점\n","        else:\n","            if self.grid[new_x][new_y] in [-1, -10]:              # 장애물 혹은 빈 선반에 부딪히는 경우\n","                reward = obs_reward                           # -10점\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  # 현재 목적지에 도달한 경우\n","                reward = goal_reward                          # 10점\n","            else:                                         # 그냥 움직이는 경우 \n","                reward = move_reward                          # -1점\n","        return reward\n","\n","    def step(self, action):  # action에 따른 이동 실행\n","        self.terminal_location = self.local_target[0]           # 목적지 리스트의 첫 번째 요소를 목적지로 설정\n","        cur_x,cur_y = self.curloc                               # 현재 위치 기억\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 다음 위치 받기\n","        self.actions.append((new_x, new_y))                     # 새로 도착한 위치를 경로 리스트에 추가\n","        #self.goal_ob_reward = False  # 사용하지 않음. 사용할 경우, 최종 목적지에 도착한 경우에 True. self.reset()에서 초기화\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # 그리드월드 밖으로 나갔는가? OB = True\n","        if any(out_of_boundary):             #1. 바깥으로 나가는 경우 종료하지 않음  \n","            del self.actions[-1]\n","            new_x, new_y = cur_x, cur_y\n","            self.actions.append((new_x, new_y))\n","        else:\n","            if self.grid[new_x][new_y] in [-1, -10]:  #2. 장애물에 부딪히는 경우 종료\n","                # print('장애물')\n","                ###################################################종료 처리할 경우 실행\n","                self.done = True  # while loop 종료\n","                self.grid[cur_x][cur_y] = 0  # 현 위치의 그리드값을 기본값(0)으로 초기화\n","                self.grid_box()              # 선반(-1), 목적지(10, 9, 8, ...), 장애물(-10) 그리드값 초기화\n","                self.grid[new_x][new_y] = 1  # 현 위치(장애물)에서 종료되었음을 표시(1). 원래는 장애물(-10) 이었음\n","                ###################################################\n","                #pass\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  #3. 현재 목적지에 도착한 경우 다음 목적지 설정\n","                #print('목적지 도착')\n","                if [new_x, new_y] == [9, 4]:  #3-1. 최종 목적지에 도착한 경우 종료\n","                    self.done = True  # while loop 종료\n","                    self.goal_ob_reward = True  # True = 1 (OB, 장애물 종료 처리 할 경우 self.done=True가 많아지므로 이때 사용)\n","\n","                self.local_target.remove(self.local_target[0])  # 다음 목적지 설정. 최종 목적지에 도착한 경우에는 마지막 요소인 [9,4]를  제거\n","                self.grid[cur_x][cur_y] = 0   # 현 위치의 그리드값을 기본값(0)으로 초기화\n","                # self.grid_box()               # 선반(-1), 목적지(10, 9, 8, ...), 장애물(-10) 그리드값 초기화\n","                self.grid[new_x][new_y] = 1   # 현 위치(목적지)에서 종료되었음을 표시(1). 원래는 목적지 이었음\n","                self.curloc = [new_x, new_y]  # 새로 도착한 위치를 현재 위치로 변경\n","            else:                             #4. 그냥 움직이는 경우\n","                #print('이동')\n","                self.grid[cur_x][cur_y] = 0   # 현 위치의 그리드값을 기본값(0)으로 초기화\n","                self.grid_box()               # 선반(-1), 목적지(10, 9, 8, ...), 장애물(-10) 그리드값 초기화\n","                self.grid[new_x][new_y] = 1   # 현 위치에서 종료되었음을 표시(1)\n","                self.curloc = [new_x,new_y]\n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done\n"]},{"cell_type":"markdown","metadata":{"id":"JfJoLw7BM2xw"},"source":["## Agent\n","- def test_action(self, obs): 추가"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4131,"status":"ok","timestamp":1654006507192,"user":{"displayName":"노현호","userId":"15410213599666758892"},"user_tz":-540},"id":"uwgy-OBbk1b4"},"outputs":[],"source":["import collections\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ReplayBuffer():  # 리플레이 버퍼: 경험 저장소\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","    \n","    def put(self, transition):  # 리플레이 버퍼(메모리)를 경험(transition)으로 채우기\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):  # memory.sample(batch_size)로 사용\n","        mini_batch = random.sample(self.buffer, n)  # 미니배치 샘플링\n","        '''\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","        for transition in mini_batch:  # 경험의 각 요소들을 각각의 미니배치로 구성\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","        '''\n","        state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in mini_batch])\n","        action_batch = torch.Tensor([a for (s1,a,r,s2,d) in mini_batch])\n","        reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in mini_batch])\n","        state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in mini_batch])\n","        done_batch = torch.Tensor([d for (s1,a,r,s2,d) in mini_batch])\n","\n","        return state1_batch, action_batch, reward_batch, state2_batch, done_batch\n","    \n","    def size(self):  # 메모리 크기\n","        return len(self.buffer)\n","\n","class Qnet(nn.Module):\n","    '''##### Linear 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        in_size = 90  # input 크기\n","        L1 = 250\n","        L2 = 150\n","        out_size  = 4\n","\n","        self.fc1 = nn.Linear(in_size, L1)\n","        self.fc2 = nn.Linear(L1, L2)\n","        self.fc3 = nn.Linear(L2, out_size)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","    '''\n","    ##### Convolution 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.fc1 = nn.Linear(1920, 128)\n","        self.fc2 = nn.Linear(128, 4)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","    def sample_action(self, obs, epsilon):  # Qnet()을 실행해서 모든 action에 대한 Qval을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        coin = random.random()  # e-greedy로 action 선택\n","        if coin < epsilon:\n","            return random.randint(0,3)\n","        else : \n","            return out.argmax().item()\n","    \n","    def test_action(self, obs):  # Qnet()을 실행해서 모든 action에 대한 Qvalue을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        return out.argmax().item()\n","\n","def train(q, q_target, memory, optimizer):  # 메모리에 경험이 미니배치 크기 이상 쌓이면 미니배치 훈련 시작\n","    '''\n","    for i in range(10):\n","        s, a, r, cr, s_prime, done_mask, gr = memory.sample(batch_size)\n","        q_out = q(s)\n","        q_a = q_out.gather(1,a.to(device))\n","        \n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","        target = r.to(device) + gamma * max_q_prime * done_mask.to(device)\n","        \n","        loss = F.smooth_l1_loss(q_a, target)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    '''\n","    s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 메모리에서 미니배치를 뽑기: batch_size=1600\n","    Q1 = q(s)  # Qnet의 Qvalue\n","    X = Q1.gather(dim=1,index=a.long().unsqueeze(dim=1).to(device)).squeeze() # Qnet의 Qvalue 변환\n","    with torch.no_grad():\n","        Q2 = q_target(s_prime)\n","    Y = r.to(device) + gamma * done_mask.to(device) * torch.max(Q2,dim=1)[0]\n","    loss = F.smooth_l1_loss(X, Y.detach())\n","    #loss = nn.MSELoss(X.to(torch.float32), Y.detach().to(torch.float32))  # 손실함수 변경. target.detach()로 변경\n","    optimizer.zero_grad()  # gradient값 초기화\n","    loss.backward()  # 자동미분\n","    optimizer.step()  # gradient 업데이트\n","    return loss.item()"]},{"cell_type":"code","source":["#def main(): # DQN... Total Path 학습 모델\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","print('tensor in', device)  # GPU 사용 확인\n","# PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","PATH = __file__ + '/model_conv_colab.pt'\n","tz = pytz.timezone('Asia/Seoul')\n","cur_time = datetime.now(tz)\n","start_time = cur_time.strftime(\"%H:%M:%S\")\n","\n","### 중요 Hyperparameters #####################\n","buffer_limit  = 16000      #1. 50000\n","batch_size    = 1600       #2. 32\n","learning_rate = 0.0005     #3. 0.0005\n","sync_freq = 500           #4. q 네트워크 파라미터를 q_target 네트워크에 복사하는 주기\n","##############################################\n","env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","q = Qnet()\n","q.to(device)\n","optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","###############################################################\n","## 학습한 모델 불러오기\n","###############################################################\n","# checkpoint = torch.load(PATH)\n","# q.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# epoch = checkpoint['epoch']\n","# loss = checkpoint['loss']\n","# q.train()\n","###############################################################\n","q_target = Qnet()\n","q_target.to(device)\n","q_target.load_state_dict(q.state_dict())\n","memory = ReplayBuffer()\n","\n","### Hyperparameters #####################\n","gamma = 0.999             #5. 0.98\n","# epochs = len(env.files)  #6. 훈련용 데이터 갯수\n","epochs = 1\n","losses = []\n","moves = []\n","max_moves = 350          #7.\n","print_interval = 10000   #8.\n","j = 0\n","epsilon = 0.3          #9. annealing 대신 고정\n","##############################################\n","\n","### 액션 저장하는 txt파일 만들기 #######################\n","f = open('ogz_conv_gif' + '.txt', 'w')\n","########################################################\n","\n","for n_epi in range(epochs):\n","    # epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n","    gridmap = env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        # self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        # self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        # self.terminal_location = None                             #4. 최초 목적지\n","        # self.local_target = []                                    #5. 목적지 리스트 초기화\n","        # self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        # self.grid = np.ones((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(1)로 초기화\n","        # self.set_box()                                            #8. 선반 위치(0), 목적지(2) 그리드값 설정.  목적지 리스트 생성\n","        # self.set_obstacle()                                       #9. 장애물 그리드값(0) 설정\n","        # self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        # self.grid[int(self.curloc[0])][int(self.curloc[1])] = -1  #11. 현재 위치(출발점) 그리드값(-1) 세팅\n","        # self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        # return self.grid\n","    state1_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","    state1 = torch.unsqueeze(state1_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","    state1 = torch.unsqueeze(state1,0)  # conv2d용 (1, 1, 10, 9)\n","    #print(state1.shape)\n","    done = False\n","    mov = 0\n","    env.actions.append((9,4))  # 경로 리스트에 [9,4] 추가\n","\n","    while (not done):  # 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과하면 종료\n","        j += 1\n","        mov += 1\n","        # 상태 state1에서 action 결정\n","        #####################################################################################\n","        if env.curloc == [9,4]:  # 출발점에서는 무조건 위로 올라간다 (action=0)\n","            action = 0\n","        elif env.curloc[0] == 0 and env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","            action = 1\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","            action = 2\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","            action = 3\n","        else:  # 위 경우를 제외하곤 e-greedy로 액션 선택\n","            action = q.sample_action(state1, epsilon)\n","        #####################################################################################\n","        # action = q.sample_action(state1, epsilon)  # 위 조건을 적용 안 할 경우\n","        # make Move... action에 따른 이동\n","        #####################################################################################\n","        # gridmap, reward, cum_reward, done = env.step(action)\n","        test_actions = [0,0,0,0]\n","        gridmap, reward, cum_reward, done = env.step(test_actions[j])\n","\n","            # self.actions.append((new_x, new_y))...새로 도착한 위치를 경로 리스트에 추가\n","            # 누적 보상 계산\n","        state2_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","        state2 = torch.unsqueeze(state2_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","        state2 = torch.unsqueeze(state2,0)  # conv2d용 (1, 1, 10, 9)\n","        done_mask = 0.0 if done else 1.0\n","        memory.put((state1, action, reward, state2, done_mask))  # 경험을 메모리에 저장\n","        state1 = state2\n","\n","        if memory.size() > batch_size:  # 메모리에 미니배치 크기 이상 쌓이면... 미니배치 훈련\n","            #print('memory size:', memory.size())\n","            loss = train(q, q_target, memory, optimizer)\n","            if j % print_interval == 0:\n","                print('epiode #:', n_epi, 'loss:', loss)\n","            losses.append(loss)\n","\n","            if j % sync_freq == 0:  # sync_freq마다 q 네트워크 파라미터를 q_target 네트워크로 복사\n","                q_target.load_state_dict(q.state_dict())\n","\n","\n","        ############## 현황 모니터링 #############################################################\n","        print(gridmap)\n","        if done:\n","            print(f\"target은 {env.items} ▶▶▶ {env.local_target}입니다.\")\n","            print(f\"{n_epi}번째 에피소드의 보상은 {env.cumulative_reward:.1f}입니다.\")\n","            print(f\"action은 총 {len(env.actions)}번 움직였습니다.\\n{env.actions}\")\n","        if done and j % 100 == 0:  # 100번 마다 mov 횟수 저장:\n","            moves.append(mov)\n","            print('종료: done =', done, '... j =', j, ' move =', mov)\n","        if env.goal_ob_reward:  # 최종 목적지 도달한 경우에만 화면에 표시\n","            print(\"#####################\")\n","            print('종료: env.goal_ob_reward =', env.goal_ob_reward, '... j =', j, ' move =', mov)\n","            print(f\"action은 총 {len(env.actions)}번 움직였습니다.\\n{env.actions}\")\n","        ##########################################################################################\n","\n","        if done or mov > max_moves:\n","            mov = 0\n","            done = True\n","\n","    # while loop 종료 ----- 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과\n","\n","    if (n_epi % 1000 == 0 and n_epi != 0) or n_epi >= 39990:\n","        torch.save({'epoch': n_epi, \n","                    'model_state_dict': q.state_dict(), \n","                    'optimizer_state_dict': optimizer.state_dict(), \n","                    'loss': loss, \n","                    }, PATH)\n","        print('▶ 모델 저장됨!!! @ 에피소드', n_epi)\n","\n","    ##### gif 생성을 위한 경로(env.actions) 저장 ####################################    \n","    if n_epi % 1000 == 0 or env.goal_ob_reward:\n","        f.write(str(n_epi)+'/'+str(env.local_target_original)+'\\n')\n","        f.write(str(env.actions))\n","        f.write('\\n')\n","    ######################################### \n","f.close()\n","#########################################\n","## 프로그램 시작 및 종료 시간 출력\n","cur_time = datetime.now(tz)\n","end_time = cur_time.strftime(\"%H:%M:%S\")\n","print ('실행 종료!', 'Start@', start_time, 'End@', end_time)\n","        #cur_time = datetime.now(tz)\n","        #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","        #print('▶ Episode #', iter, 'start time:', simple_cur_time, end='→')"],"metadata":{"id":"YnBfhH5ViD3i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wK35ngIkNM6I"},"source":["## Main: DQN...\n","  - GPU를 사용할 경우 구현\n","  - 학습한 모델 불러올 수 있는 코드 추가\n","    - '#' 제거하고 실행시키면 됨"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"j9qFCqbbjf3F","executionInfo":{"status":"ok","timestamp":1654007504942,"user_tz":-540,"elapsed":1147,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"25bcd90d-dcc6-437d-9a36-b3ebc231a006"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor in cuda\n","data/factory_order_train.csv used\n","[[ 10.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [ -1.   0.   0.   0.   0.   0.   0.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [  0.   0. -10.   0. -10.   0. -10.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   1.   0.   0.   0.   0.]\n"," [-10. -10. -10. -10.   9. -10. -10. -10. -10.]]\n","[[ 10.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [ -1.   0.   0.   0.   0.   0.   0.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [  0.   0. -10.   0. -10.   0. -10.   0.   0.]\n"," [  0.   0.   0.   0.   1.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [-10. -10. -10. -10.   9. -10. -10. -10. -10.]]\n","[[ 10.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [ -1.   0.   0.   0.   0.   0.   0.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [  0.   0. -10.   0.   1.   0. -10.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [-10. -10. -10. -10.   9. -10. -10. -10. -10.]]\n","target은 E ▶▶▶ [[0, 0], [9, 4]]입니다.\n","0번째 에피소드의 보상은 -0.3입니다.\n","action은 총 4번 움직였습니다.\n","[(9, 4), (8, 4), (7, 4), (6, 4)]\n","실행 종료! Start@ 23:31:43 End@ 23:31:43\n","[[ 10.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [ -1.   0.   0.   0.   0.   0.   0.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [ -1.   0. -10.   0. -10.   0. -10.   0.  -1.]\n"," [  0.   0. -10.   0.   1.   0. -10.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"," [-10. -10. -10. -10.   9. -10. -10. -10. -10.]]\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Moves')"]},"metadata":{},"execution_count":7},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["#def main(): # DQN... Total Path 학습 모델\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","print('tensor in', device)  # GPU 사용 확인\n","# PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","PATH = __file__ + '/model_conv_colab.pt'\n","tz = pytz.timezone('Asia/Seoul')\n","cur_time = datetime.now(tz)\n","start_time = cur_time.strftime(\"%H:%M:%S\")\n","\n","### 중요 Hyperparameters #####################\n","buffer_limit  = 16000      #1. 50000\n","batch_size    = 1600       #2. 32\n","learning_rate = 0.0005     #3. 0.0005\n","sync_freq = 500           #4. q 네트워크 파라미터를 q_target 네트워크에 복사하는 주기\n","##############################################\n","env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","q = Qnet()\n","q.to(device)\n","optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","###############################################################\n","## 학습한 모델 불러오기\n","###############################################################\n","# checkpoint = torch.load(PATH)\n","# q.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# epoch = checkpoint['epoch']\n","# loss = checkpoint['loss']\n","# q.train()\n","###############################################################\n","q_target = Qnet()\n","q_target.to(device)\n","q_target.load_state_dict(q.state_dict())\n","memory = ReplayBuffer()\n","\n","### Hyperparameters #####################\n","gamma = 0.999             #5. 0.98\n","epochs = len(env.files)  #6. 훈련용 데이터 갯수\n","losses = []\n","moves = []\n","max_moves = 350          #7.\n","print_interval = 10000   #8.\n","j = 0\n","epsilon = 0.3          #9. annealing 대신 고정\n","##############################################\n","\n","### 액션 저장하는 txt파일 만들기 #######################\n","f = open('ogz_conv_gif' + '.txt', 'w')\n","########################################################\n","\n","for n_epi in range(epochs):\n","    # epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n","    gridmap = env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        # self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        # self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        # self.terminal_location = None                             #4. 최초 목적지\n","        # self.local_target = []                                    #5. 목적지 리스트 초기화\n","        # self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        # self.grid = np.ones((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(1)로 초기화\n","        # self.set_box()                                            #8. 선반 위치(0), 목적지(2) 그리드값 설정.  목적지 리스트 생성\n","        # self.set_obstacle()                                       #9. 장애물 그리드값(0) 설정\n","        # self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        # self.grid[int(self.curloc[0])][int(self.curloc[1])] = -1  #11. 현재 위치(출발점) 그리드값(-1) 세팅\n","        # self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        # return self.grid\n","    state1_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","    state1 = torch.unsqueeze(state1_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","    state1 = torch.unsqueeze(state1,0)  # conv2d용 (1, 1, 10, 9)\n","    #print(state1.shape)\n","    done = False\n","    mov = 0\n","    env.actions.append((9,4))  # 경로 리스트에 [9,4] 추가\n","\n","    while (not done):  # 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과하면 종료\n","        j += 1\n","        mov += 1\n","        # 상태 state1에서 action 결정\n","        #####################################################################################\n","        if env.curloc == [9,4]:  # 출발점에서는 무조건 위로 올라간다 (action=0)\n","            action = 0\n","        elif env.curloc[0] == 0 and env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","            action = 1\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","            action = 2\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","            action = 3\n","        else:  # 위 경우를 제외하곤 e-greedy로 액션 선택\n","            action = q.sample_action(state1, epsilon)\n","        #####################################################################################\n","        # action = q.sample_action(state1, epsilon)  # 위 조건을 적용 안 할 경우\n","        # make Move... action에 따른 이동\n","        gridmap, reward, cum_reward, done = env.step(action)\n","            # self.actions.append((new_x, new_y))...새로 도착한 위치를 경로 리스트에 추가\n","            # 누적 보상 계산\n","        state2_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","        state2 = torch.unsqueeze(state2_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","        state2 = torch.unsqueeze(state2,0)  # conv2d용 (1, 1, 10, 9)\n","        done_mask = 0.0 if done else 1.0\n","        memory.put((state1, action, reward, state2, done_mask))  # 경험을 메모리에 저장\n","        state1 = state2\n","\n","        if memory.size() > batch_size:  # 메모리에 미니배치 크기 이상 쌓이면... 미니배치 훈련\n","            #print('memory size:', memory.size())\n","            loss = train(q, q_target, memory, optimizer)\n","            if j % print_interval == 0:\n","                print('epiode #:', n_epi, 'loss:', loss)\n","            losses.append(loss)\n","\n","            if j % sync_freq == 0:  # sync_freq마다 q 네트워크 파라미터를 q_target 네트워크로 복사\n","                q_target.load_state_dict(q.state_dict())\n","\n","\n","        ############## 현황 모니터링 #############################################################\n","        if done:\n","            print(f\"target은 {env.items} ▶▶▶ {env.local_target}입니다.\")\n","            print(f\"{n_epi}번째 에피소드의 보상은 {env.cumulative_reward:.1f}입니다.\")\n","            print(f\"action은 총 {len(env.actions)}번 움직였습니다.\\n{env.actions}\")\n","        if done and j % 100 == 0:  # 100번 마다 mov 횟수 저장:\n","            moves.append(mov)\n","            print('종료: done =', done, '... j =', j, ' move =', mov)\n","        if env.goal_ob_reward:  # 최종 목적지 도달한 경우에만 화면에 표시\n","            print(\"#####################\")\n","            print('종료: env.goal_ob_reward =', env.goal_ob_reward, '... j =', j, ' move =', mov)\n","            print(f\"action은 총 {len(env.actions)}번 움직였습니다.\\n{env.actions}\")\n","        ##########################################################################################\n","\n","        if done or mov > max_moves:\n","            mov = 0\n","            done = True\n","\n","    # while loop 종료 ----- 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과\n","\n","    if (n_epi % 1000 == 0 and n_epi != 0) or n_epi >= 39990:\n","        torch.save({'epoch': n_epi, \n","                    'model_state_dict': q.state_dict(), \n","                    'optimizer_state_dict': optimizer.state_dict(), \n","                    'loss': loss, \n","                    }, PATH)\n","        print('▶ 모델 저장됨!!! @ 에피소드', n_epi)\n","\n","    ##### gif 생성을 위한 경로(env.actions) 저장 ####################################    \n","    if n_epi % 1000 == 0 or env.goal_ob_reward:\n","        f.write(str(n_epi)+'/'+str(env.local_target_original)+'\\n')\n","        f.write(str(env.actions))\n","        f.write('\\n')\n","    ######################################### \n","f.close()\n","#########################################\n","## 프로그램 시작 및 종료 시간 출력\n","cur_time = datetime.now(tz)\n","end_time = cur_time.strftime(\"%H:%M:%S\")\n","print ('실행 종료!', 'Start@', start_time, 'End@', end_time)\n","        #cur_time = datetime.now(tz)\n","        #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","        #print('▶ Episode #', iter, 'start time:', simple_cur_time, end='→')\n","\n","print(gridmap.reshape(10,9))  # 최종 그리드맵 확인\n","\n","##  loss 그래프\n","plt.figure(figsize=(10,7))\n","plt.plot(losses)\n","plt.xlabel(\"Epochs\",fontsize=22)\n","plt.ylabel(\"Loss\",fontsize=22)\n","\n","## 경로길이(이동 횟수) 그래프\n","plt.figure(figsize=(10,7))\n","plt.plot(moves)\n","plt.xlabel(\"Epochs\",fontsize=22)\n","plt.ylabel(\"Moves\",fontsize=22)"]},{"cell_type":"markdown","metadata":{"id":"q2kNcb0-3j-s"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"EUyczG0z3nay"},"source":["## 테스트 코드"]},{"cell_type":"code","source":["## test 코드 #################################################################################\n","def test(train_mode = False, display=True):  # train_mode = False 테스트 모드\n","##############################################################################################\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","    print('tensor in', device)  # GPU 사용 확인\n","    PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","    test_env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","    test_q = Qnet()\n","    test_q.to(device)\n","    ###############################################################\n","    ## 학습한 모델 불러오기\n","    ###############################################################\n","    checkpoint = torch.load(PATH)\n","    q.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    q.eval()\n","    ###############################################################\n","    ### 액션 저장하는 txt파일 만들기 #######################\n","    f = open('ogz_conv_test_gif' + '.txt', 'w')\n","    ########################################################\n","    epochs = len(env.files)\n","    wins = 0\n","    for n_epi in range(epochs):\n","        gridmap = test_env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        state_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","        state = torch.unsqueeze(state_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","        state = torch.unsqueeze(state,0)  # conv2d용 (1, 1, 10, 9)\n","        done = False\n","        mov = 0\n","        while (not done):  # 최종 목적지 [9,4] 도착\n","            mov += 1\n","            # 상태 state1에서 action 결정\n","            #####################################################################################\n","            if env.curloc == [9,4]:  # 출발점에서는 무조건 위로 (action=0)\n","                action = 0\n","            elif env.curloc[0] == 0 and env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","                action = 1\n","            elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","                action = 2\n","            elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","                action = 3\n","            else:  # 위 경우를 제외하곤 test 액션 선택\n","                action = q.test_action(state)\n","            #####################################################################################\n","            # action = q.test_action(state)  # 위 조건을 적용하지 않을 경우\n","            # make Move... action에 따른 이동\n","            gridmap, reward, cum_reward, done = env.step(action)\n","            state_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","            state = torch.unsqueeze(state_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","            state = torch.unsqueeze(state,0)  # conv2d용 (1, 1, 10, 9)\n","            if display:\n","                print('Move #%s; Taking action: %s' % (mov, action))\n","                print(gridmap.reshape(10,9))\n","\n","            if test_env.goal_ob_reward:\n","                wins += 1\n","                if display:\n","                    print(\"Game won! Reward: %s\" % (cum_reward))\n","                else:\n","                    print(\"Game LOST. Reward: %s\" % (cum_reward))\n","                    \n","            #if (mov > 100):  # 조건 추가 필요시 사용\n","                #if display:\n","                    #print(\"Game lost; too many moves.\")\n","                #break\n","        \n","    win_perc = float(wins) / float(epochs)\n","    print(\"Test performed: {0}, # of wins: {1}\".format(epochs,wins))\n","    print(\"Win percentage: {}%\".format(100.0*win_perc))\n","    return win_perc"],"metadata":{"id":"I3nZCJvNYqbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfW9OaPsekYX"},"outputs":[],"source":["'''\n","## test 코드 #################################################################################\n","# def test():\n","##############################################################################################\n","train_mode = False  # False: 테스트 모드\n","tz = pytz.timezone('Asia/Seoul')\n","env = Simulator()\n","q = Qnet()\n","PATH = '/content/drive/MyDrive/aiffelthon/data/model_total.pt'\n","q.load_state_dict(torch.load(PATH))\n","q.eval()\n","print('len(env.files):', len(env.files))\n","success_cnt = 0\n","pred_final_path = []\n","\n","# >>>>>>>>> 전체 테스트 데이터에 대한 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","for n_epi in range(1225): # range()의 인수로 len(env.files) 사용하면 됨 (=1225)\n","\n","    env.grid = env.reset(n_epi)  # 에피소드 번호에 해당하는 목표물 리스트 env.local_target 생성, 그리드맵 생성\n","    print('▶ Episode #', n_epi,':', list(env.files.iloc[n_epi])[0])\n","    print('   목적지 좌표: ', env.local_target)\n","    #print(\"reset(0) 결과로 받은 최초 그리드맵\")\n","    #print(env.grid)  # ===== 1\n","    pred_local_path = []\n","\n","    # >>>>>>>>> 하나의 에피소드에 대한 대한 최적 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","    for n_target in range(len(env.local_target)):\n","    \n","        path_length = 0  # 경로 길이 초기화\n","        env.grid_box()  # 그리드값 초기화\n","        # >>>>>>>>>>>>>> 출발지, 목적지 지정\n","        if n_target == 0:\n","            start_point = [9,4]  # 첫 번째 출발지 지정\n","            print('   출발지', start_point, end=\" → \")\n","        else:\n","            start_point = env.local_target[n_target-1]\n","            print('   출발지', start_point, end=\" → \")\n","        end_point = env.local_target[n_target]  # 목적지 지정\n","        print('목적지', end_point, end=\" >>>>> \")\n","\n","        # >>>>>>>>>>>>>>>>>>>>> 하나의 목적지에 대한 최적 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","\n","        # 출발지, 목적지 좌표 지정\n","        env.x, env.y = start_point                         # 출발지 좌표\n","        env.grid[int(env.x)][int(env.y)] = -5              # 출발지 그리드값 = -5\n","        env.end_x, env.end_y = end_point                   # 목적지 좌표\n","        # 현 위치를 출발지로 reset\n","        env.curloc = start_point\n","        # 목적지 도착 flag (done), 인풋 데이터 (s) 등 초기화\n","        env.done = False\n","        done = False\n","        env.actions = []\n","        env.actions.append(tuple(start_point))\n","        grid_map = env.grid.reshape(-1)          # 그리드 맵\n","        #print(grid_map.reshape(10,9))  # ===== 2\n","        s = np.array(grid_map)\n","\n","        while not done:  # 한 칸씩 이동. 목적지 도착하면 종료\n","            #print('>>>>>>>>>>>> while 시작점... current location:', env.curloc)\n","            #####################################################################################\n","            # (option) 출발점에서는 무조건 위로 올라간다 (action=0)\n","            #if env.curloc == [9,4]:\n","                #a = 0\n","            #elif env.curloc == [0,0] or env.curloc == [0,8]:\n","                #a = 1\n","            #else:\n","                #a = q.sample_action(torch.from_numpy(s).float(), epsilon)  # e-greedy로 액션 선택... 네트워크 결과값으로\n","            #####################################################################################\n","            a = q.test_action(torch.from_numpy(s).float())  # 네트워크 결과값으로 액션 선택\n","            grid, r, cum_reward, done, goal_ob_reward = env.step(a)\n","\n","            # >>>>> 테스트에서 성공 여부 체크에 사용: env.goal_count == len(env.local_target) 이면 성공!\n","            if goal_ob_reward:\n","                env.goal_count += 1  # env.reset()에서 초기화 됨\n","            # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","\n","            new_grid_map = grid.reshape(-1)\n","            #print('액션=', a, '한 칸 이동 결과', 'done=', done)\n","            #print(new_grid_map.reshape(10,9))  # ===== 3\n","            s_prime = np.array(new_grid_map)\n","            s = s_prime\n","            if done:\n","                break  #  while loop 빠져 나가기 \n","            # while loop 종료\n","        \n","        pred_local_path.append(env.actions)  # 최종 경로 구하기 위해 구간 경로를 추가\n","        path_length += len(env.actions)\n","        print('예측 경로:', env.actions)\n","        path_length = 0.0\n","        env.cumulative_reward = 0.0\n","        #print('for loop 종료')\n","        # >>>>>>>>>>>>>>>>>>>>> 하나의 목적지에 대한 최적 경로 찾기 종료 <<<<<<<<<<<<<<<<<<<<<\n","    \n","    # >>>>>>>>> 하나의 에피소드에 대한 대한 최적 경로 찾기 종료 <<<<<<<<<<<<<<<<<<<<<\n","\n","    pred_final_path[n_epi] = pred_local_path  # gif 생성을 위해 최종 경로 저장\n","    ###################################################################################\n","    print('▶ Episode #', n_epi,':', list(env.files.iloc[n_epi])[0])\n","    print('target Grid:', env.local_target)\n","    if env.goal_count >= len(env.local_target):\n","        success_cnt += 1\n","        success_rate = success_cnt /(n_epi+1)\n","        print('성공!!!', env.goal_count, '곳 방문..', '누적 성공률:', success_rate)\n","    else:\n","        print('fail...', len(env.local_target), '중', env.goal_count, '곳 방문..')\n","    ###################################################################################\n","\n","# >>>>>>>>> 전체 훈련 데이터에 대한 최적 경로 찾기 훈련 종료 <<<<<<<<<<<<<<<<<<<<<\n","'''"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"as_Muiti_target_DQN_Conv2D_0531.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
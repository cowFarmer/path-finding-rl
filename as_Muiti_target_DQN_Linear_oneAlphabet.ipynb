{"cells":[{"cell_type":"code","source":["\bfrom google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRpfPDrdk9wB","executionInfo":{"status":"ok","timestamp":1653978285220,"user_tz":-540,"elapsed":2533,"user":{"displayName":"신휘정","userId":"12805037499998438573"}},"outputId":"7dbb93b5-93fa-4b9c-99a0-acc4809bacbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/path-finding-rl/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Q8bS-FgxuHT","executionInfo":{"status":"ok","timestamp":1653978285220,"user_tz":-540,"elapsed":13,"user":{"displayName":"신휘정","userId":"12805037499998438573"}},"outputId":"a07e68e0-c1b9-495d-b7a4-b1943d25ae81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/path-finding-rl\n"]}]},{"cell_type":"code","source":["__file__ = '/content/drive/MyDrive/path-finding-rl/'\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))"],"metadata":{"id":"2QJztpRhyBbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Env\n","- 경로 이동 제한 사항 반영\n","  - del apply_action() 수정\n","- def grid_box(self):  # 그리드 박스 초기화 용도로 정의"],"metadata":{"id":"UHFqTLgjMNbJ"}},{"cell_type":"code","source":["from string import ascii_uppercase\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","###################################\n","# reward 조정\n","move_reward = -1  # 0.1\n","obs_reward = -10  # 0.1\n","goal_reward = 10  # 10\n","###################################\n","# train / test 모드 지정\n","train_mode = True\n","###################################\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","class Simulator:\n","    def __init__(self):\n","        # Load train or test data\n","        if train_mode:  # 훈련 데이타 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","            print('data/factory_order_train.csv used')\n","        else:           # 테스트 데이터 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_test.csv\"))\n","            print('data/factory_order_test.csv used')\n","       \n","        self.height = 10  # 그리드 높이\n","        self.width = 9    #  그리드 너비\n","        self.inds = list(ascii_uppercase)[:17]  # A ~ Q alphabet list\n","\n","    def set_box(self):  # 선반 위치, 목적지 그리드값 설정.  목적지 리스트 생성\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = 0\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 0\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):  # 목적지: 그리드값 = 2\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 2\n","            self.local_target.append([getattr(order_box, \"row\"), getattr(order_box, \"col\")])  # 목적지 리스트 생성: local_target\n","        # self.local_target.sort() # 불필요. 인풋 데이터 A, B, C순 정렬되어 있음... 정렬 필요시 코드 바꾸어야 함\n","        self.local_target.append([9,4]) # 목적지 리스트에 최종 목적지(9,4) 추가\n","\n","    def set_obstacle(self):  # 장애물 위치 그리드값 설정 = 0\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n","\n","    def grid_box(self):  # 그리드 박스 초기화 용도로 정의: 선반, 목적지, 장애물 그리드값 초기화\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = 0\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 0\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):  # 목적지: 그리드값 = 2\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 2\n","        self.set_obstacle()  # 장애물 위치 그리드값 설정 = 0\n","        \n","    def reset(self, epi):  # 에피소드 시작시 12개 값 초기화\n","        ################################################################################################################################################################\n","        # 현재 모드는 한 개의 아이템을 가지고오게 하기 위한 모드이므로, factory_order_train 파일값을 읽어오지 않습니다.\n","        # 원래 모드로 사용하시려면 여기 위치를 전부 주석 처리하시고, 아래 self.items가 있는 2번 코드를 주석해제하십시오.\n","\n","        coin = random.randint(0,16)\n","        print('coin', coin, chr(coin+65))\n","        self.items = chr(coin+65)\n","        ################################################################################################################################################################\n","        \n","        self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        self.terminal_location = None                             #4. 최초 목적지\n","        self.local_target = []                                    #5. 목적지 리스트 초기화\n","        self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(1)로 초기화\n","        self.set_box()                                            #8. 선반 위치(0), 목적지(2) 그리드값 설정.  목적지 리스트 생성\n","        self.set_obstacle()                                       #9. 장애물 그리드값(0) 설정\n","        self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -1  #11. 현재 위치(출발점) 그리드값(-1) 세팅\n","        self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):  # action에 따른 새 좌표값 반환\n","        new_x = cur_x\n","        new_y = cur_y\n","        if action == 0:          # up\n","            #new_x = cur_x - 1\n","            new_x = self.move_up(cur_x, cur_y, new_x, new_y)\n","        elif action == 1:        # down\n","            #new_x = cur_x + 1\n","            new_x = self.move_down(cur_x, cur_y, new_x, new_y)\n","        elif action == 2:        # left\n","            #new_y = cur_y - 1\n","            new_y = self.move_left(cur_x, cur_y, new_x, new_y)\n","        else:                    # right\n","            #new_y = cur_y + 1\n","            new_y = self.move_right(cur_x, cur_y, new_x, new_y)\n","        return int(new_x), int(new_y)\n","    \n","    # >>> 현재 위치에서 이동이 불가한 위치 추가 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def move_up(self, cur_x, cur_y, new_x, new_y):  # action == 0:\n","        if cur_x in [6,5,4,3,2] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x - 1\n","        return new_x\n","    def move_down(self, cur_x, cur_y, new_x, new_y): # action == 1:\n","        if cur_x in [1,2,3,4,5] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x + 1\n","        return new_x\n","    def move_left(self, cur_x, cur_y, new_x, new_y): # left elif action == 2:\n","        if cur_y in [1,2,3,4,5,6,7,8] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y - 1\n","        return new_y\n","    def move_right(self, cur_x, cur_y, new_x, new_y): # right else: action == 3:\n","        if cur_y in [0,1,2,3,4,5,6,7] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y + 1\n","        return new_y\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):  # action에 의해 이동시 얻는 보상\n","        if any(out_of_boundary):                          # 바깥으로 나가는 경우\n","            reward = obs_reward                               # -10점\n","        else:\n","            if self.grid[new_x][new_y] == 0:              # 선반이나 장애물에 부딪히는 경우\n","                reward = obs_reward                           # -10점\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  # 현재 목적지에 도달한 경우\n","                reward = goal_reward                          # 10점\n","            else:                                         # 그냥 움직이는 경우 \n","                reward = move_reward                          # -1점\n","        return reward\n","\n","    def step(self, action):  # action에 따른 이동 실행\n","        self.terminal_location = self.local_target[0]           # 목적지 리스트의 첫 번째 요소를 목적지로 설정\n","        cur_x,cur_y = self.curloc                               # 현재 위치 기억\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 다음 위치 받기\n","        self.actions.append((new_x, new_y))                     # 새로 도착한 위치를 경로 리스트에 추가\n","        #goal_ob_reward = False  # 사용하지 않음. 사용할 경우, 최종 목적지에 도착한 경우에 True\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # 그리드월드 밖으로 나갔는가? OB = True\n","        if any(out_of_boundary):             #1. 바깥으로 나가는 경우 종료하지 않음...\n","            #print('OB')\n","            ######################################################종료 처리할 경우 실행\n","            #self.done = True  # while loop 종료\n","            #self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 초기화(1)\n","            #self.grid_box()  # 선반(0), 목적지(2), 장애물(0) 그리드값 초기화\n","            ######################################################\n","            pass\n","        else:\n","            if self.grid[new_x][new_y] == 0:  #2. 장애물에 부딪히는 경우 종료하지 않음\n","                #print('장애물')\n","                ###################################################종료 처리할 경우 실행\n","                #self.done = True  # while loop 종료\n","                #self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 초기화(1)\n","                #self.grid_box()  # 선반(0), 목적지(2), 장애물(0) 그리드값 초기화\n","                #self.grid[new_x][new_y] = -1  # 현 위치 그리드값(-1)\n","                ###################################################\n","                pass\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  #3. 현재 목적지에 도착한 경우 다음 목적지 설정\n","                #print('목적지 도착')\n","                if [new_x, new_y] == [9, 4]:  #3-1. 최종 목적지에 도착한 경우 종료\n","                    self.done = True  # while loop 종료\n","                    #goal_ob_reward = True  # True = 1 (OB, 장애물 종료 처리 할 경우 self.done=True가 많아지므로 이때 사용)\n","\n","                self.local_target.remove(self.local_target[0])  # 다음 목적지 설정. 최종 목적지에 도착한 경우에는 마지막 요소인 [9,4]를  제거\n","                self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 초기화(1)\n","                self.grid_box()               # 선반(0), 목적지(2), 장애물(0) 그리드값 초기화\n","                self.grid[new_x][new_y] = -1  # 현 위치 그리드값(-1)\n","                self.curloc = [new_x, new_y]  # 새로 도착한 위치를 현재 위치로 변경\n","            else:                             #4. 그냥 움직이는 경우\n","                #print('이동')\n","                self.grid[cur_x][cur_y] = 1   # 현재의 그리드값 초기화(1)\n","                self.grid_box()               # 선반(0), 목적지(2), 장애물(0) 그리드값 초기화\n","                self.grid[new_x][new_y] = -1  # 현 위치 그리드값(-1)\n","                self.curloc = [new_x,new_y]\n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQRnyv3UlBcu","executionInfo":{"status":"ok","timestamp":1653978595595,"user_tz":-540,"elapsed":434,"user":{"displayName":"신휘정","userId":"12805037499998438573"}},"outputId":"f14470da-1b8e-42ca-a6b9-00c0aa1654c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -1 -10 10\n"]}]},{"cell_type":"markdown","source":["## Agent\n","- def test_action(self, obs): 추가"],"metadata":{"id":"JfJoLw7BM2xw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwgy-OBbk1b4"},"outputs":[],"source":["import collections\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ReplayBuffer():  # 리플레이 버퍼: 경험 저장소\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","    \n","    def put(self, transition):  # 리플레이 버퍼(메모리)를 경험(transition)으로 채우기\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):  # memory.sample(batch_size)로 사용\n","        mini_batch = random.sample(self.buffer, n)  # 미니배치 샘플링\n","        '''\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","        for transition in mini_batch:  # 경험의 각 요소들을 각각의 미니배치로 구성\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","        '''\n","        state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in mini_batch])\n","        action_batch = torch.Tensor([a for (s1,a,r,s2,d) in mini_batch])\n","        reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in mini_batch])\n","        state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in mini_batch])\n","        done_batch = torch.Tensor([d for (s1,a,r,s2,d) in mini_batch])\n","\n","        return state1_batch, action_batch, reward_batch, state2_batch, done_batch\n","    \n","    def size(self):  # 메모리 크기\n","        return len(self.buffer)\n","\n","class Qnet(nn.Module):\n","    ##### Linear 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        in_size = 90  # input 크기\n","        L1 = 250\n","        L2 = 150\n","        out_size  = 4\n","\n","        self.fc1 = nn.Linear(in_size, L1)\n","        self.fc2 = nn.Linear(L1, L2)\n","        self.fc3 = nn.Linear(L2, out_size)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def sample_action(self, obs, epsilon):  # Qnet()을 실행해서 모든 action에 대한 Qval을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        coin = random.random()  # e-greedy로 action 선택\n","        if coin < epsilon:\n","            return random.randint(0,3)\n","        else : \n","            return out.argmax().item()\n","    \n","    def test_action(self, obs):  # Qnet()을 실행해서 모든 action에 대한 Qvalue을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        return out.argmax().item()\n","\n","def train(q, q_target, memory, optimizer):  # 메모리에 경험이 미니배치 크기 이상 쌓이면 미니배치 훈련 시작\n","    '''\n","    for i in range(10):\n","        s, a, r, cr, s_prime, done_mask, gr = memory.sample(batch_size)\n","        q_out = q(s)\n","        q_a = q_out.gather(1,a.to(device))\n","        \n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","        target = r.to(device) + gamma * max_q_prime * done_mask.to(device)\n","        \n","        loss = F.smooth_l1_loss(q_a, target)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    '''\n","    s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 메모리에서 미니배치를 뽑기: batch_size=1600\n","    Q1 = q(s)  # Qnet의 Qvalue\n","    X = Q1.gather(dim=1,index=a.long().unsqueeze(dim=1).to(device)).squeeze() # Qnet의 Qvalue 변환\n","    with torch.no_grad():\n","        Q2 = q_target(s_prime)\n","    Y = r.to(device) + gamma * done_mask.to(device) * torch.max(Q2,dim=1)[0]\n","    loss = F.smooth_l1_loss(X, Y.detach())\n","    #loss = nn.MSELoss(X.to(torch.float32), Y.detach().to(torch.float32))  # 손실함수 변경. target.detach()로 변경\n","    optimizer.zero_grad()  # gradient값 초기화\n","    loss.backward()  # 자동미분\n","    optimizer.step()  # gradient 업데이트\n","    return loss.item()"]},{"cell_type":"markdown","source":["## Main: DQN...\n","  - GPU를 사용할 경우???\n","  - 학습한 모델 불러올 수 있는 코드 추가\n","    - '#' 제거하고 실행시키면 됨"],"metadata":{"id":"wK35ngIkNM6I"}},{"cell_type":"code","source":["#def main(): # DQN... Total Path 학습 모델\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","print('tensor in', device)  # GPU 사용 확인\n","PATH = '/content/drive/MyDrive/aiffelthon/data/model_total.pt'\n","tz = pytz.timezone('Asia/Seoul')\n","cur_time = datetime.now(tz)\n","start_time = cur_time.strftime(\"%H:%M:%S\")\n","\n","### 중요 Hyperparameters #####################\n","buffer_limit  = 16000      #1. 50000\n","batch_size    = 1600       #2. 32\n","learning_rate = 0.0005     #3. 0.0005\n","sync_freq = 4000           #4. q 네트워크 파라미터를 q_target 네트워크에 복사하는 주기\n","##############################################\n","env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","q = Qnet()\n","q.to(device)\n","optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","###############################################################\n","## 학습한 모델 불러오기\n","###############################################################\n","#checkpoint = torch.load(PATH)\n","#q.load_state_dict(checkpoint['model_state_dict'])\n","#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","#epoch = checkpoint['epoch']\n","#loss = checkpoint['loss']\n","#q.train()\n","###############################################################\n","q_target = Qnet()\n","q_target.to(device)\n","q_target.load_state_dict(q.state_dict())\n","memory = ReplayBuffer()\n","\n","### Hyperparameters #####################\n","gamma         = 0.90     #5. 0.98\n","epochs = len(env.files)  #6. 훈련용 데이터 갯수\n","losses = []\n","max_moves = 350          #7.\n","print_interval = 10000    #8.\n","j = 0\n","# epsilon = 0.3            #9. annealing 대신 고정\n","##############################################\n","\n","for n_epi in range(epochs):\n","    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n","    gridmap = env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        # self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        # self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        # self.terminal_location = None                             #4. 최초 목적지\n","        # self.local_target = []                                    #5. 목적지 리스트 초기화\n","        # self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        # self.grid = np.ones((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(1)로 초기화\n","        # self.set_box()                                            #8. 선반 위치(0), 목적지(2) 그리드값 설정.  목적지 리스트 생성\n","        # self.set_obstacle()                                       #9. 장애물 그리드값(0) 설정\n","        # self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        # self.grid[int(self.curloc[0])][int(self.curloc[1])] = -1  #11. 현재 위치(출발점) 그리드값(-1) 세팅\n","        # self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        # return self.grid\n","    state1_ = gridmap.reshape(1,90) + np.random.rand(1,90)/100.0  # noise 추가: ReLU activation에서 'dead neurons' 발생 방지\n","    state1 = torch.from_numpy(state1_).float()  # 넘파이 array를 Torch 텐서로 변환\n","    done = False\n","    mov = 0\n","    env.actions.append((9,4))  # 경로 리스트에 [9,4] 추가\n","\n","    while (not done):  # 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과하면 종료\n","        j += 1\n","        mov += 1\n","        # 상태 state1에서 action 결정\n","        #####################################################################################\n","        if env.curloc == [9,4]:  # 출발점에서는 무조건 위로 올라간다 (action=0)\n","            action = 0\n","        elif env.curloc[0] == 0 and env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","            action = 1\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 8:  # (2,0)~(5,0)에서는 무조건 왼쪽으로\n","            action = 2\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","            action = 3\n","        else:  # 위 경우를 제외하곤 e-greedy로 액션 선택\n","            action = q.sample_action(state1, epsilon)  \n","        #####################################################################################\n","        # action = q.sample_action(state1, epsilon)  # 위 조건을 적용 안 할 경우\n","        # make Move... action에 따른 이동\n","        gridmap, reward, cum_reward, done = env.step(action)\n","            # self.actions.append((new_x, new_y))...새로 도착한 위치를 경로 리스트에 추가\n","            # 누적 보상 계산\n","        state2_ = gridmap.reshape(1,90) + np.random.rand(1,90)/100.0  # noise 추가: ReLU activation에서 'dead neurons' 발생 방지\n","        state2 = torch.from_numpy(state2_).float()  # 넘파이 array를 Torch 텐서로 변환\n","        done_mask = 0.0 if done else 1.0\n","        memory.put((state1, action, reward, state2, done_mask))  # 경험을 메모리에 저장\n","        state1 = state2\n","\n","        if memory.size() > batch_size:  # 메모리에 미니배치 크기 이상 쌓이면... 미니배치 훈련\n","            #print('memory size:', memory.size())\n","            loss = train(q, q_target, memory, optimizer)\n","            if j % print_interval == 0:\n","                print('epiode #:', n_epi, 'loss:', loss)\n","            losses.append(loss)\n","\n","            if j % sync_freq == 0:  # sync_freq마다 q 네트워크 파라미터를 q_target 네트워크로 복사\n","                q_target.load_state_dict(q.state_dict())\n","        if done:\n","            print('종료: done =', done, '... j =', j, ' move =', mov)\n","        if done or mov > max_moves:\n","            mov = 0\n","            done = True\n","\n","    # while loop 종료 ----- 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과\n","\n","    if n_epi % 1000 == 0 and n_epi != 0:\n","        torch.save({'epoch': n_epi, \n","                    'model_state_dict': q.state_dict(), \n","                    'optimizer_state_dict': optimizer.state_dict(), \n","                    'loss': loss, \n","                    }, PATH)\n","        print('▶ 모델 저장됨!!!')\n","'''\n","cur_time = datetime.now(tz)\n","end_time = cur_time.strftime(\"%H:%M:%S\")\n","print ('실행 종료!', 'Start@', start_time, 'End@', end_time)\n","        #cur_time = datetime.now(tz)\n","        #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","        #print('▶ Episode #', iter, 'start time:', simple_cur_time, end='→')\n","'''\n","##print(new_grid_map.reshape(10,9))"],"metadata":{"id":"j9qFCqbbjf3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,7))\n","plt.plot(losses)\n","plt.xlabel(\"Epochs\",fontsize=22)\n","plt.ylabel(\"Loss\",fontsize=22)"],"metadata":{"id":"Kq1LVWZdCXIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"q2kNcb0-3j-s"}},{"cell_type":"markdown","source":["## 테스트 코드"],"metadata":{"id":"EUyczG0z3nay"}},{"cell_type":"code","source":["## test 코드 #################################################################################\n","# def test():\n","##############################################################################################\n","train_mode = False  # False: 테스트 모드\n","tz = pytz.timezone('Asia/Seoul')\n","env = Simulator()\n","q = Qnet()\n","PATH = '/content/drive/MyDrive/aiffelthon/data/model_total.pt'\n","q.load_state_dict(torch.load(PATH))\n","q.eval()\n","print('len(env.files):', len(env.files))\n","success_cnt = 0\n","pred_final_path = []\n","\n","# >>>>>>>>> 전체 테스트 데이터에 대한 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","for n_epi in range(1225): # range()의 인수로 len(env.files) 사용하면 됨 (=1225)\n","\n","    env.grid = env.reset(n_epi)  # 에피소드 번호에 해당하는 목표물 리스트 env.local_target 생성, 그리드맵 생성\n","    print('▶ Episode #', n_epi,':', list(env.files.iloc[n_epi])[0])\n","    print('   목적지 좌표: ', env.local_target)\n","    #print(\"reset(0) 결과로 받은 최초 그리드맵\")\n","    #print(env.grid)  # ===== 1\n","    pred_local_path = []\n","\n","    # >>>>>>>>> 하나의 에피소드에 대한 대한 최적 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","    for n_target in range(len(env.local_target)):\n","    \n","        path_length = 0  # 경로 길이 초기화\n","        env.grid_box()  # 그리드값 초기화\n","        # >>>>>>>>>>>>>> 출발지, 목적지 지정\n","        if n_target == 0:\n","            start_point = [9,4]  # 첫 번째 출발지 지정\n","            print('   출발지', start_point, end=\" → \")\n","        else:\n","            start_point = env.local_target[n_target-1]\n","            print('   출발지', start_point, end=\" → \")\n","        end_point = env.local_target[n_target]  # 목적지 지정\n","        print('목적지', end_point, end=\" >>>>> \")\n","\n","        # >>>>>>>>>>>>>>>>>>>>> 하나의 목적지에 대한 최적 경로 찾기 시작 <<<<<<<<<<<<<<<<<<<<<\n","\n","        # 출발지, 목적지 좌표 지정\n","        env.x, env.y = start_point                         # 출발지 좌표\n","        env.grid[int(env.x)][int(env.y)] = -5              # 출발지 그리드값 = -5\n","        env.end_x, env.end_y = end_point                   # 목적지 좌표\n","        # 현 위치를 출발지로 reset\n","        env.curloc = start_point\n","        # 목적지 도착 flag (done), 인풋 데이터 (s) 등 초기화\n","        env.done = False\n","        done = False\n","        env.actions = []\n","        env.actions.append(tuple(start_point))\n","        grid_map = env.grid.reshape(-1)          # 그리드 맵\n","        #print(grid_map.reshape(10,9))  # ===== 2\n","        s = np.array(grid_map)\n","\n","        while not done:  # 한 칸씩 이동. 목적지 도착하면 종료\n","            #print('>>>>>>>>>>>> while 시작점... current location:', env.curloc)\n","            #####################################################################################\n","            # (option) 출발점에서는 무조건 위로 올라간다 (action=0)\n","            #if env.curloc == [9,4]:\n","                #a = 0\n","            #elif env.curloc == [0,0] or env.curloc == [0,8]:\n","                #a = 1\n","            #else:\n","                #a = q.sample_action(torch.from_numpy(s).float(), epsilon)  # e-greedy로 액션 선택... 네트워크 결과값으로\n","            #####################################################################################\n","            a = q.test_action(torch.from_numpy(s).float())  # 네트워크 결과값으로 액션 선택\n","            grid, r, cum_reward, done, goal_ob_reward = env.step(a)\n","\n","            # >>>>> 테스트에서 성공 여부 체크에 사용: env.goal_count == len(env.local_target) 이면 성공!\n","            if goal_ob_reward:\n","                env.goal_count += 1  # env.reset()에서 초기화 됨\n","            # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","\n","            new_grid_map = grid.reshape(-1)\n","            #print('액션=', a, '한 칸 이동 결과', 'done=', done)\n","            #print(new_grid_map.reshape(10,9))  # ===== 3\n","            s_prime = np.array(new_grid_map)\n","            s = s_prime\n","            if done:\n","                break  #  while loop 빠져 나가기 \n","            # while loop 종료\n","        \n","        pred_local_path.append(env.actions)  # 최종 경로 구하기 위해 구간 경로를 추가\n","        path_length += len(env.actions)\n","        print('예측 경로:', env.actions)\n","        path_length = 0.0\n","        env.cumulative_reward = 0.0\n","        #print('for loop 종료')\n","        # >>>>>>>>>>>>>>>>>>>>> 하나의 목적지에 대한 최적 경로 찾기 종료 <<<<<<<<<<<<<<<<<<<<<\n","    \n","    # >>>>>>>>> 하나의 에피소드에 대한 대한 최적 경로 찾기 종료 <<<<<<<<<<<<<<<<<<<<<\n","\n","    pred_final_path[n_epi] = pred_local_path  # gif 생성을 위해 최종 경로 저장\n","    ###################################################################################\n","    print('▶ Episode #', n_epi,':', list(env.files.iloc[n_epi])[0])\n","    print('target Grid:', env.local_target)\n","    if env.goal_count >= len(env.local_target):\n","        success_cnt += 1\n","        success_rate = success_cnt /(n_epi+1)\n","        print('성공!!!', env.goal_count, '곳 방문..', '누적 성공률:', success_rate)\n","    else:\n","        print('fail...', len(env.local_target), '중', env.goal_count, '곳 방문..')\n","    ###################################################################################\n","\n","# >>>>>>>>> 전체 훈련 데이터에 대한 최적 경로 찾기 훈련 종료 <<<<<<<<<<<<<<<<<<<<<"],"metadata":{"id":"bfW9OaPsekYX"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"name":"as_Muiti_target_DQN_Linear_oneAlphabet.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21945,"status":"ok","timestamp":1654134490875,"user":{"displayName":"노현호","userId":"15410213599666758892"},"user_tz":-540},"id":"lRpfPDrdk9wB","outputId":"e5ae7470-ae84-4b19-ba92-4cde8c518f82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","__file__ = '/content/drive/Othercomputers/MacBook_Air/path-finding-rl/data'"]},{"cell_type":"markdown","metadata":{"id":"UHFqTLgjMNbJ"},"source":["## Env\n","- 경로 이동 제한 사항 반영: del apply_action() 수정\n","- def grid_box(self):  # 그리드 박스 초기화 용도로 정의\n","- 그리드값 변경\n","  - target_gridval (목표물)  = 10, 9, 8, ...\n","  - curloc_gridval (현 위치) = 1\n","  - default_gridval (기본)   = 0\n","  - rack_gridval (선반)      = -1\n","  - obs_gridval (장애물)     = -10\n","- 종료 조건\n","  - **그리드월드 밖**으로 나가는 경우 **종료하지 않음**\n","  - **장애물**에 부딪히는 경우 **종료**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":771,"status":"ok","timestamp":1654114055527,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"},"user_tz":-540},"id":"tQRnyv3UlBcu","outputId":"e3c4200b-da91-4ee3-8280-e3cf1272da0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -1 -10 10\n"]}],"source":["from string import ascii_uppercase\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","###################################\n","# 보상 Reward\n","move_reward = -1  # 0.1\n","obs_reward = -10  # 0.1\n","goal_reward = 10  # 10\n","###################################\n","# 그리드값\n","# target_gridval = 10, 9, 8... # 목표물: 10, 9, 8, ...\n","curloc_gridval  = 1            # 현 위치: 1\n","default_gridval = 0            # 기본: 0\n","rack_gridval    = -1           # 선반: -1\n","obs_gridval     = -10          # 장애물: -10\n","###################################\n","# train / test 모드 지정\n","train_mode = True\n","###################################\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","class Simulator:\n","    def __init__(self):\n","        # Load train or test data\n","        #if train_mode:  # 훈련 데이타 읽기\n","        #    self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","        #    print('data/factory_order_train.csv used')\n","        #else:           # 테스트 데이터 읽기\n","        #    self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_test.csv\"))\n","        #    print('data/factory_order_test.csv used')\n","        ##########################################################################################\n","        self.height = 10  # 그리드 높이\n","        self.width = 9    #  그리드 너비\n","        self.inds = list(ascii_uppercase)[:17]  # A ~ Q alphabet list\n","\n","    def set_box(self):  # 선반 위치, 목적지 그리드값 설정. 목적지 리스트 생성\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = rack_gridval\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = rack_gridval\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = target_gridval\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","            self.local_target.append([getattr(order_box, \"row\"), getattr(order_box, \"col\")])  # 목적지 리스트 생성: local_target\n","        # self.local_target.sort() # 불필요. 인풋 데이터 A, B, C순 정렬되어 있음... 정렬 필요시 코드 바꾸어야 함\n","        self.local_target.append([9,4]) # 목적지 리스트에 최종 목적지(9,4) 추가\n","        self.local_target_original = self.local_target.copy()  # gif 생성을 위해 추가.  에피소드의 경로 저장\n","        self.target_length = len(self.local_target_original)  # 목적지 그리드값 10, 9, 8, ... 지정 위해 추가\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","\n","    def set_obstacle(self):  # 장애물 위치 그리드값 설정 = obs_gridval\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = obs_gridval\n","\n","    def grid_box(self):  # 그리드 박스 초기화 용도로 정의: 선반, 목적지, 장애물 그리드값 초기화\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = rack_gridval\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = rack_gridval\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = target_gridval\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","        self.set_obstacle()  # 장애물 위치 그리드값 설정 = obs_gridval\n","        \n","    def reset(self, epi):  # 에피소드 시작시 12개 값 초기화\n","        self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        ####################################################################################\n","        coin = random.randint(0,16)\n","        self.items = [chr(coin+65)]  # A~Q 까지 1개 랜덤으로 받아서 items에 1개 넣어주기, 반복 횟수는 main의 epochs 조절하기\n","        ####################################################################################\n","        self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        self.terminal_location = None                             #4. 최초 목적지\n","        self.local_target = []                                    #5. 목적지 리스트 초기화\n","        self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        self.grid = np.zeros((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(default_gridval) 초기화\n","        self.set_box()                                            #8. 선반 위치, 목적지 그리드값 설정. 목적지 리스트 생성\n","        self.set_obstacle()                                       #9. 장애물 그리드값 설정\n","        ######################\n","        # print('최초 그리드맵:')  # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = curloc_gridval  #11. 현재 위치(출발점) 그리드값 세팅\n","        self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        self.goal_ob_reward = False                      # (추가) #13. 최종 목적지 도착 여부 = False\n","        ######################\n","        # print('현 위치 추가:')   # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):  # action에 따른 새 좌표값 반환\n","        new_x = cur_x\n","        new_y = cur_y\n","        if action == 0:          # up\n","            #new_x = cur_x - 1\n","            new_x = self.move_up(cur_x, cur_y, new_x, new_y)\n","        elif action == 1:        # down\n","            #new_x = cur_x + 1\n","            new_x = self.move_down(cur_x, cur_y, new_x, new_y)\n","        elif action == 2:        # left\n","            #new_y = cur_y - 1\n","            new_y = self.move_left(cur_x, cur_y, new_x, new_y)\n","        else:                    # right\n","            #new_y = cur_y + 1\n","            new_y = self.move_right(cur_x, cur_y, new_x, new_y)\n","        return int(new_x), int(new_y)\n","    \n","    # >>> 현재 위치에서 이동이 불가한 위치 추가 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def move_up(self, cur_x, cur_y, new_x, new_y):  # action == 0:\n","        if cur_x in [6,5,4,3,2] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x - 1\n","        return new_x\n","    def move_down(self, cur_x, cur_y, new_x, new_y): # action == 1:\n","        if cur_x in [1,2,3,4,5] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x + 1\n","        return new_x\n","    def move_left(self, cur_x, cur_y, new_x, new_y): # left elif action == 2:\n","        if cur_y in [1,2,3,4,5,6,7,8] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y - 1\n","        return new_y\n","    def move_right(self, cur_x, cur_y, new_x, new_y): # right else: action == 3:\n","        if cur_y in [0,1,2,3,4,5,6,7] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y + 1\n","        return new_y\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):  # action에 의해 이동시 얻는 보상\n","        if any(out_of_boundary):                                        # 바깥으로 나가는 경우\n","            reward = obs_reward\n","        else:\n","            if self.grid[new_x][new_y] in [rack_gridval, obs_gridval]:  # 선반이나 장애물에 부딪히는 경우\n","                reward = obs_reward\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  # 현재 목적지에 도달한 경우\n","                reward = goal_reward\n","            else:                                                       # 그냥 움직이는 경우 \n","                reward = move_reward\n","        return reward\n","\n","    def step(self, action):  # action에 따른 이동 실행\n","        self.terminal_location = self.local_target[0]           # 목적지 리스트의 첫 번째 요소를 목적지로 설정\n","        cur_x, cur_y = self.curloc                              # 현재 위치 기억\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 다음 위치 받기\n","        self.actions.append((cur_x, cur_y))                     # 현재 위치(지나온 위치)를 경로 리스트에 추가\n","        self.goal_ob_reward = False                             # 최종 목적지에 도착한 경우에 True. self.reset()에서 초기화\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # 그리드월드 밖으로 나갔는가? OB = True\n","\n","        if any(out_of_boundary) or self.grid[new_x][new_y] == rack_gridval:  #1. 바깥으로 나가는 경우, 빈 선반에 부딪치는 경우 종료하지 않음\n","            #print('OB')\n","            reward = self.get_reward(new_x, new_y, out_of_boundary)\n","            new_x = cur_x  # 종료 처리할 경우 코멘트 처리\n","            new_y = cur_y  # 종료 처리할 경우 코멘트 처리\n","            ######################################################종료 처리할 경우 실행\n","            #self.done = True                             # while loop 종료\n","            #self.grid_box()                              # 선반, 목적지, 장애물 그리드값 초기화\n","            #self.actions.append((new_x,new_y))           # 경로 리스트에 최종 위치 추가. (주의) 좌표값이 그리드월드를 벗어나는 값 발생!\n","            ######################################################\n","        else:\n","            if self.grid[new_x][new_y] == obs_gridval:  #2. 장애물에 부딪히는 경우 종료\n","                # print('장애물')\n","                reward = self.get_reward(new_x, new_y, out_of_boundary)\n","                # new_x = cur_x  # 종료 처리할 경우 코멘트 처리\n","                # new_y = cur_y  # 종료 처리할 경우 코멘트 처리\n","                ###################################################종료 처리할 경우 실행\n","                self.done = True                           # while loop 종료\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치(장애물)에서 종료되었음을 표시. 원래는 장애물이었음\n","                self.actions.append((new_x,new_y))         # 경로 리스트에 최종 위치 추가\n","                ###################################################\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  #3. 현재 목적지에 도착한 경우 다음 목적지 설정\n","                #print('목적지 도착')\n","                reward = self.get_reward(new_x, new_y, out_of_boundary)\n","                if [new_x, new_y] == [9, 4]:            #3-1. 최종 목적지에 도착한 경우 종료\n","                    self.done = True                       # while loop 종료\n","                    self.goal_ob_reward = True             # True = 1 (OB, 장애물 종료 처리 할 경우 self.done=True가 많아지므로 이때 사용)\n","                    self.actions.append((new_x,new_y))     # 경로 리스트에 최종 위치 추가\n","\n","                self.local_target.remove(self.local_target[0])  # 다음 목적지 설정. 최종 목적지에 도착한 경우에는 마지막 요소인 [9,4]를  제거\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치(목적지)에서 종료되었음을 표시. 원래는 목적지이었음\n","                self.curloc = [new_x, new_y]               # 새로 도착한 위치를 현재 위치로 변경\n","            else:                                       #4. 그냥 움직이는 경우\n","                #print('이동')\n","                reward = self.get_reward(new_x, new_y, out_of_boundary)\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치에서 종료되었음을 표시\n","                self.curloc = [new_x,new_y]\n","\n","        #reward = self.get_reward(new_x, new_y, out_of_boundary)  # 각 경우에 받는 것으로 처리!!\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done\n"]},{"cell_type":"markdown","metadata":{"id":"JfJoLw7BM2xw"},"source":["## Agent\n","- def test_action(self, obs): 추가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwgy-OBbk1b4"},"outputs":[],"source":["import collections\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ReplayBuffer():  # 리플레이 버퍼: 경험 저장소\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","    \n","    def put(self, transition):  # 리플레이 버퍼(메모리)를 경험(transition)으로 채우기\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):  # memory.sample(batch_size)로 사용\n","        mini_batch = random.sample(self.buffer, n)  # 미니배치 샘플링\n","        '''\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","        for transition in mini_batch:  # 경험의 각 요소들을 각각의 미니배치로 구성\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","        '''\n","        state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in mini_batch])\n","        action_batch = torch.Tensor([a for (s1,a,r,s2,d) in mini_batch])\n","        reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in mini_batch])\n","        state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in mini_batch])\n","        done_batch = torch.Tensor([d for (s1,a,r,s2,d) in mini_batch])\n","\n","        return state1_batch, action_batch, reward_batch, state2_batch, done_batch\n","    \n","    def size(self):  # 메모리 크기\n","        return len(self.buffer)\n","\n","class Qnet(nn.Module):\n","    '''##### Linear 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        in_size = 90  # input 크기\n","        L1 = 250\n","        L2 = 150\n","        out_size  = 4\n","\n","        self.fc1 = nn.Linear(in_size, L1)\n","        self.fc2 = nn.Linear(L1, L2)\n","        self.fc3 = nn.Linear(L2, out_size)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","    '''\n","    ##### Convolution 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(4, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n","        self.fc1 = nn.Linear(1920, 128)\n","        self.fc2 = nn.Linear(128, 4)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x) )\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","    def sample_action(self, obs, epsilon):  # Qnet()을 실행해서 모든 action에 대한 Qval을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        coin = random.random()  # e-greedy로 action 선택\n","        if coin < epsilon:\n","            return random.randint(0,3)\n","        else : \n","            return out.argmax().item()\n","    \n","    def test_action(self, obs):  # Qnet()을 실행해서 모든 action에 대한 Qvalue을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        return out.argmax().item()\n","\n","def train(q, q_target, memory, optimizer):  # 메모리에 경험이 미니배치 크기 이상 쌓이면 미니배치 훈련 시작\n","    '''\n","    for i in range(10):\n","        s, a, r, cr, s_prime, done_mask, gr = memory.sample(batch_size)\n","        q_out = q(s)\n","        q_a = q_out.gather(1,a.to(device))\n","        \n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","        target = r.to(device) + gamma * max_q_prime * done_mask.to(device)\n","        \n","        loss = F.smooth_l1_loss(q_a, target)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    '''\n","    s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 메모리에서 미니배치를 뽑기: batch_size=1600\n","    Q1 = q(s)  # Qnet의 Qvalue\n","    X = Q1.gather(dim=1,index=a.long().unsqueeze(dim=1).to(device)).squeeze() # Qnet의 Qvalue 변환\n","    with torch.no_grad():\n","        Q2 = q_target(s_prime)\n","    Y = r.to(device) + gamma * done_mask.to(device) * torch.max(Q2,dim=1)[0]\n","    loss = F.smooth_l1_loss(X, Y.detach())\n","    #loss = nn.MSELoss(X.to(torch.float32), Y.detach().to(torch.float32))  # 손실함수 변경. target.detach()로 변경\n","    optimizer.zero_grad()  # gradient값 초기화\n","    loss.backward()  # 자동미분\n","    optimizer.step()  # gradient 업데이트\n","    return loss.item()"]},{"cell_type":"markdown","metadata":{"id":"wK35ngIkNM6I"},"source":["## Main: DQN...\n","  - GPU를 사용할 경우 구현\n","  - 학습한 모델 불러올 수 있는 코드 추가\n","    - '#' 제거하고 실행시키면 됨"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9qFCqbbjf3F","outputId":"55b73c91-ac86-4fc7-bf7f-3020d05f5eef"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor in cuda\n","종료: done = True ... j = 6600  move = 7\n","종료: done = True ... j = 7700  move = 12\n","종료: done = True ... j = 10400  move = 3\n","▶ 모델 저장됨!!! @ 에피소드 400\n","종료: done = True ... j = 11600  move = 46\n","종료: done = True ... j = 12800  move = 6\n","▶ 모델 저장됨!!! @ 에피소드 500\n","epiode #: 549 loss: 0.03257600590586662 j: 15000\n","종료: done = True ... j = 16100  move = 17\n","▶ 모델 저장됨!!! @ 에피소드 600\n","종료: done = True ... j = 19600  move = 25\n","epiode #: 654 loss: 0.006056811194866896 j: 20000\n","▶ 모델 저장됨!!! @ 에피소드 700\n","epiode #: 731 loss: 0.0012423344887793064 j: 25000\n","▶ 모델 저장됨!!! @ 에피소드 800\n","epiode #: 807 loss: 0.00038427094114013016 j: 30000\n","종료: done = True ... j = 32000  move = 33\n","종료: done = True ... j = 32400  move = 17\n","epiode #: 885 loss: 0.00021350693714339286 j: 35000\n","▶ 모델 저장됨!!! @ 에피소드 900\n","종료: done = True ... j = 36500  move = 17\n","epiode #: 960 loss: 1.3686680176761001e-05 j: 40000\n","▶ 모델 저장됨!!! @ 에피소드 1000\n","epiode #: 1038 loss: 0.0009038006537593901 j: 45000\n","▶ 모델 저장됨!!! @ 에피소드 1100\n","epiode #: 1119 loss: 7.775659469189122e-05 j: 50000\n","종료: done = True ... j = 51400  move = 25\n","종료: done = True ... j = 51800  move = 23\n","epiode #: 1197 loss: 1.8905115211964585e-05 j: 55000\n","▶ 모델 저장됨!!! @ 에피소드 1200\n","종료: done = True ... j = 58500  move = 79\n","종료: done = True ... j = 59200  move = 37\n","epiode #: 1279 loss: 8.94088443601504e-05 j: 60000\n","▶ 모델 저장됨!!! @ 에피소드 1300\n","종료: done = True ... j = 61300  move = 45\n","epiode #: 1357 loss: 0.00014853976608719677 j: 65000\n","▶ 모델 저장됨!!! @ 에피소드 1400\n","epiode #: 1445 loss: 2.5073684810195118e-05 j: 70000\n","종료: done = True ... j = 72600  move = 19\n","▶ 모델 저장됨!!! @ 에피소드 1500\n","epiode #: 1521 loss: 3.992064011981711e-05 j: 75000\n","epiode #: 1596 loss: 4.391804395709187e-06 j: 80000\n","▶ 모델 저장됨!!! @ 에피소드 1600\n","epiode #: 1667 loss: 2.0892346583423205e-05 j: 85000\n","▶ 모델 저장됨!!! @ 에피소드 1700\n","epiode #: 1749 loss: 0.00018029795319307595 j: 90000\n","종료: done = True ... j = 91000  move = 19\n","▶ 모델 저장됨!!! @ 에피소드 1800\n","epiode #: 1828 loss: 3.732205368578434e-05 j: 95000\n","종료: done = True ... j = 97000  move = 45\n","▶ 모델 저장됨!!! @ 에피소드 1900\n","epiode #: 1910 loss: 2.9273758627823554e-05 j: 100000\n","epiode #: 1986 loss: 5.804047759738751e-05 j: 105000\n","종료: done = True ... j = 105100  move = 35\n","▶ 모델 저장됨!!! @ 에피소드 2000\n","epiode #: 2075 loss: 2.696369483601302e-05 j: 110000\n","▶ 모델 저장됨!!! @ 에피소드 2100\n","epiode #: 2167 loss: 1.6789590517873876e-05 j: 115000\n","▶ 모델 저장됨!!! @ 에피소드 2200\n","epiode #: 2246 loss: 1.5385316146421246e-05 j: 120000\n","▶ 모델 저장됨!!! @ 에피소드 2300\n","epiode #: 2325 loss: 6.101120106905e-06 j: 125000\n","▶ 모델 저장됨!!! @ 에피소드 2400\n","epiode #: 2402 loss: 1.8919084823210142e-06 j: 130000\n","종료: done = True ... j = 130400  move = 73\n","종료: done = True ... j = 133000  move = 25\n","epiode #: 2478 loss: 8.879750339474413e-07 j: 135000\n","▶ 모델 저장됨!!! @ 에피소드 2500\n","종료: done = True ... j = 137000  move = 69\n","epiode #: 2552 loss: 7.792796168359928e-06 j: 140000\n","▶ 모델 저장됨!!! @ 에피소드 2600\n","epiode #: 2625 loss: 0.0003735657373908907 j: 145000\n","▶ 모델 저장됨!!! @ 에피소드 2700\n","epiode #: 2703 loss: 1.1629243090283126e-05 j: 150000\n","epiode #: 2775 loss: 5.2426075853873044e-05 j: 155000\n","▶ 모델 저장됨!!! @ 에피소드 2800\n","종료: done = True ... j = 159500  move = 9\n","epiode #: 2854 loss: 1.3950445918453624e-06 j: 160000\n","종료: done = True ... j = 161500  move = 10\n","▶ 모델 저장됨!!! @ 에피소드 2900\n","epiode #: 2935 loss: 9.430030331714079e-05 j: 165000\n","종료: done = True ... j = 166200  move = 58\n","▶ 모델 저장됨!!! @ 에피소드 3000\n","epiode #: 3017 loss: 6.083413950364047e-07 j: 170000\n","epiode #: 3097 loss: 1.0116345947608352e-05 j: 175000\n","▶ 모델 저장됨!!! @ 에피소드 3100\n","epiode #: 3181 loss: 3.980585915996926e-06 j: 180000\n","▶ 모델 저장됨!!! @ 에피소드 3200\n","epiode #: 3258 loss: 3.1185566058411496e-06 j: 185000\n","종료: done = True ... j = 186400  move = 15\n","▶ 모델 저장됨!!! @ 에피소드 3300\n","epiode #: 3337 loss: 0.00011507685121614486 j: 190000\n","▶ 모델 저장됨!!! @ 에피소드 3400\n","epiode #: 3407 loss: 1.3317879165697377e-05 j: 195000\n","epiode #: 3477 loss: 3.6215224099578336e-05 j: 200000\n","▶ 모델 저장됨!!! @ 에피소드 3500\n","종료: done = True ... j = 202400  move = 21\n","epiode #: 3553 loss: 3.8970323657849804e-05 j: 205000\n","종료: done = True ... j = 206800  move = 41\n","▶ 모델 저장됨!!! @ 에피소드 3600\n","epiode #: 3638 loss: 0.00040709771565161645 j: 210000\n","▶ 모델 저장됨!!! @ 에피소드 3700\n","epiode #: 3715 loss: 6.273255712585524e-05 j: 215000\n","epiode #: 3796 loss: 0.0002733884612098336 j: 220000\n","▶ 모델 저장됨!!! @ 에피소드 3800\n","종료: done = True ... j = 223800  move = 47\n","종료: done = True ... j = 224900  move = 61\n","epiode #: 3879 loss: 0.00011886948777828366 j: 225000\n","▶ 모델 저장됨!!! @ 에피소드 3900\n","epiode #: 3955 loss: 0.00023465306730940938 j: 230000\n","종료: done = True ... j = 231700  move = 59\n","▶ 모델 저장됨!!! @ 에피소드 4000\n","epiode #: 4030 loss: 5.230836904956959e-05 j: 235000\n","종료: done = True ... j = 235900  move = 99\n","▶ 모델 저장됨!!! @ 에피소드 4100\n","epiode #: 4107 loss: 0.0015111044049263 j: 240000\n","종료: done = True ... j = 241500  move = 17\n","epiode #: 4194 loss: 0.00012635854363907129 j: 245000\n","▶ 모델 저장됨!!! @ 에피소드 4200\n","epiode #: 4273 loss: 8.991028153104708e-05 j: 250000\n","▶ 모델 저장됨!!! @ 에피소드 4300\n","epiode #: 4344 loss: 0.0005500746192410588 j: 255000\n","종료: done = True ... j = 257400  move = 33\n","▶ 모델 저장됨!!! @ 에피소드 4400\n","epiode #: 4422 loss: 7.371007086476311e-05 j: 260000\n","종료: done = True ... j = 263800  move = 95\n","epiode #: 4498 loss: 2.8455699066398665e-05 j: 265000\n","▶ 모델 저장됨!!! @ 에피소드 4500\n","종료: done = True ... j = 269500  move = 29\n","epiode #: 4583 loss: 0.0001591815089341253 j: 270000\n","▶ 모델 저장됨!!! @ 에피소드 4600\n","종료: done = True ... j = 272400  move = 7\n","epiode #: 4666 loss: 0.0007305556209757924 j: 275000\n","▶ 모델 저장됨!!! @ 에피소드 4700\n","epiode #: 4748 loss: 4.208603058941662e-06 j: 280000\n","종료: done = True ... j = 280500  move = 75\n","▶ 모델 저장됨!!! @ 에피소드 4800\n","종료: done = True ... j = 283800  move = 51\n","epiode #: 4824 loss: 3.7351379432948306e-05 j: 285000\n","epiode #: 4896 loss: 2.1171751996007515e-06 j: 290000\n","▶ 모델 저장됨!!! @ 에피소드 4900\n","종료: done = True ... j = 294900  move = 59\n","epiode #: 4977 loss: 2.7735495677916333e-07 j: 295000\n","▶ 모델 저장됨!!! @ 에피소드 5000\n","epiode #: 5065 loss: 1.2358163985481951e-05 j: 300000\n","종료: done = True ... j = 300500  move = 63\n","▶ 모델 저장됨!!! @ 에피소드 5100\n","epiode #: 5143 loss: 6.413114078895887e-06 j: 305000\n","▶ 모델 저장됨!!! @ 에피소드 5200\n","epiode #: 5217 loss: 2.605388772281003e-06 j: 310000\n","epiode #: 5292 loss: 1.5291283489204943e-05 j: 315000\n","▶ 모델 저장됨!!! @ 에피소드 5300\n","epiode #: 5370 loss: 1.828590399099994e-07 j: 320000\n","▶ 모델 저장됨!!! @ 에피소드 5400\n","종료: done = True ... j = 323900  move = 71\n","epiode #: 5446 loss: 2.0978604879928753e-05 j: 325000\n","종료: done = True ... j = 325400  move = 13\n","종료: done = True ... j = 328100  move = 87\n","▶ 모델 저장됨!!! @ 에피소드 5500\n","epiode #: 5524 loss: 1.1107715636171633e-06 j: 330000\n","종료: done = True ... j = 330800  move = 39\n","▶ 모델 저장됨!!! @ 에피소드 5600\n","epiode #: 5603 loss: 2.0529887478915043e-06 j: 335000\n","epiode #: 5672 loss: 1.541593633191951e-06 j: 340000\n","▶ 모델 저장됨!!! @ 에피소드 5700\n","epiode #: 5753 loss: 3.1502136152994353e-06 j: 345000\n","▶ 모델 저장됨!!! @ 에피소드 5800\n","epiode #: 5840 loss: 6.573837163159624e-05 j: 350000\n","▶ 모델 저장됨!!! @ 에피소드 5900\n","epiode #: 5917 loss: 1.9491704733809456e-05 j: 355000\n","epiode #: 5985 loss: 3.6604669730877504e-05 j: 360000\n","▶ 모델 저장됨!!! @ 에피소드 6000\n","종료: done = True ... j = 364700  move = 35\n","epiode #: 6081 loss: 0.00012969024828635156 j: 365000\n","종료: done = True ... j = 365500  move = 97\n","종료: done = True ... j = 366300  move = 15\n","▶ 모델 저장됨!!! @ 에피소드 6100\n","종료: done = True ... j = 368200  move = 49\n","epiode #: 6159 loss: 9.507840331934858e-06 j: 370000\n","▶ 모델 저장됨!!! @ 에피소드 6200\n","종료: done = True ... j = 373500  move = 41\n","epiode #: 6234 loss: 0.00016922537179198116 j: 375000\n","▶ 모델 저장됨!!! @ 에피소드 6300\n","epiode #: 6318 loss: 3.564633516361937e-06 j: 380000\n","epiode #: 6392 loss: 4.27022172289071e-07 j: 385000\n","▶ 모델 저장됨!!! @ 에피소드 6400\n","종료: done = True ... j = 387500  move = 15\n","epiode #: 6482 loss: 7.786990749991674e-07 j: 390000\n","▶ 모델 저장됨!!! @ 에피소드 6500\n","epiode #: 6565 loss: 0.0006982784834690392 j: 395000\n","▶ 모델 저장됨!!! @ 에피소드 6600\n","epiode #: 6648 loss: 1.4746757187822368e-05 j: 400000\n","▶ 모델 저장됨!!! @ 에피소드 6700\n","종료: done = True ... j = 404700  move = 13\n","epiode #: 6725 loss: 5.844080078531988e-05 j: 405000\n","epiode #: 6800 loss: 2.4509137801942416e-05 j: 410000\n","▶ 모델 저장됨!!! @ 에피소드 6800\n","epiode #: 6879 loss: 0.00011630915832938626 j: 415000\n","▶ 모델 저장됨!!! @ 에피소드 6900\n","epiode #: 6952 loss: 0.00023378025798592716 j: 420000\n","▶ 모델 저장됨!!! @ 에피소드 7000\n","epiode #: 7033 loss: 2.3870490622357465e-05 j: 425000\n","종료: done = True ... j = 428300  move = 15\n","▶ 모델 저장됨!!! @ 에피소드 7100\n","종료: done = True ... j = 429700  move = 47\n","epiode #: 7113 loss: 6.454831236624159e-06 j: 430000\n","종료: done = True ... j = 430000  move = 3\n","epiode #: 7194 loss: 4.480378947846475e-07 j: 435000\n","▶ 모델 저장됨!!! @ 에피소드 7200\n","epiode #: 7274 loss: 5.341446467355127e-07 j: 440000\n","▶ 모델 저장됨!!! @ 에피소드 7300\n","epiode #: 7347 loss: 6.009636308590416e-06 j: 445000\n","▶ 모델 저장됨!!! @ 에피소드 7400\n","epiode #: 7422 loss: 1.093322339329461e-06 j: 450000\n","▶ 모델 저장됨!!! @ 에피소드 7500\n","epiode #: 7506 loss: 3.5001883134100353e-06 j: 455000\n","epiode #: 7584 loss: 6.97387531545246e-06 j: 460000\n","▶ 모델 저장됨!!! @ 에피소드 7600\n","종료: done = True ... j = 460900  move = 23\n","epiode #: 7663 loss: 0.000153789616888389 j: 465000\n","▶ 모델 저장됨!!! @ 에피소드 7700\n","epiode #: 7743 loss: 0.00016290252096951008 j: 470000\n","▶ 모델 저장됨!!! @ 에피소드 7800\n","epiode #: 7827 loss: 2.149605097656604e-05 j: 475000\n","▶ 모델 저장됨!!! @ 에피소드 7900\n","epiode #: 7908 loss: 6.86049897922203e-05 j: 480000\n","종료: done = True ... j = 482000  move = 89\n","epiode #: 7985 loss: 2.5299577828263864e-05 j: 485000\n","▶ 모델 저장됨!!! @ 에피소드 8000\n","epiode #: 8073 loss: 8.430601155851036e-05 j: 490000\n","▶ 모델 저장됨!!! @ 에피소드 8100\n","epiode #: 8151 loss: 0.00015047259512357414 j: 495000\n","▶ 모델 저장됨!!! @ 에피소드 8200\n","epiode #: 8223 loss: 2.5092704163398594e-06 j: 500000\n","종료: done = True ... j = 503500  move = 3\n","▶ 모델 저장됨!!! @ 에피소드 8300\n","epiode #: 8307 loss: 3.5064469557255507e-05 j: 505000\n","epiode #: 8391 loss: 7.550219015683979e-05 j: 510000\n","▶ 모델 저장됨!!! @ 에피소드 8400\n","종료: done = True ... j = 514700  move = 21\n","epiode #: 8463 loss: 1.0540748007770162e-05 j: 515000\n","▶ 모델 저장됨!!! @ 에피소드 8500\n","epiode #: 8537 loss: 1.282781568079372e-06 j: 520000\n","종료: done = True ... j = 523400  move = 53\n","▶ 모델 저장됨!!! @ 에피소드 8600\n","epiode #: 8610 loss: 2.7501954491526703e-07 j: 525000\n","epiode #: 8688 loss: 7.745869879727252e-06 j: 530000\n","▶ 모델 저장됨!!! @ 에피소드 8700\n","epiode #: 8762 loss: 1.5601142422383418e-06 j: 535000\n","▶ 모델 저장됨!!! @ 에피소드 8800\n","epiode #: 8849 loss: 7.760351650176744e-07 j: 540000\n","▶ 모델 저장됨!!! @ 에피소드 8900\n","epiode #: 8926 loss: 2.272147980875161e-07 j: 545000\n","종료: done = True ... j = 545900  move = 31\n","epiode #: 8997 loss: 3.1229928936227225e-06 j: 550000\n","▶ 모델 저장됨!!! @ 에피소드 9000\n","epiode #: 9078 loss: 6.158618361951085e-06 j: 555000\n","▶ 모델 저장됨!!! @ 에피소드 9100\n","종료: done = True ... j = 558100  move = 7\n","epiode #: 9160 loss: 1.7425945770810358e-05 j: 560000\n","종료: done = True ... j = 560900  move = 67\n","▶ 모델 저장됨!!! @ 에피소드 9200\n","epiode #: 9248 loss: 6.44706005914486e-07 j: 565000\n","▶ 모델 저장됨!!! @ 에피소드 9300\n","epiode #: 9322 loss: 2.0616332676581806e-06 j: 570000\n","▶ 모델 저장됨!!! @ 에피소드 9400\n","epiode #: 9412 loss: 1.2227934576003463e-07 j: 575000\n","epiode #: 9491 loss: 8.470133394666846e-08 j: 580000\n","▶ 모델 저장됨!!! @ 에피소드 9500\n","종료: done = True ... j = 582000  move = 15\n","epiode #: 9578 loss: 8.056720162130659e-07 j: 585000\n","▶ 모델 저장됨!!! @ 에피소드 9600\n","종료: done = True ... j = 588900  move = 33\n","epiode #: 9662 loss: 2.443335688440129e-05 j: 590000\n","종료: done = True ... j = 591400  move = 27\n","▶ 모델 저장됨!!! @ 에피소드 9700\n","종료: done = True ... j = 594100  move = 27\n","epiode #: 9739 loss: 4.538876510196133e-06 j: 595000\n","▶ 모델 저장됨!!! @ 에피소드 9800\n","epiode #: 9809 loss: 7.177262659752159e-07 j: 600000\n","종료: done = True ... j = 604000  move = 9\n","epiode #: 9882 loss: 2.348195948798093e-07 j: 605000\n","▶ 모델 저장됨!!! @ 에피소드 9900\n","종료: done = True ... j = 607700  move = 9\n","epiode #: 9963 loss: 4.279292653563971e-08 j: 610000\n","▶ 모델 저장됨!!! @ 에피소드 10000\n","epiode #: 10044 loss: 1.5250402611854952e-06 j: 615000\n","종료: done = True ... j = 617400  move = 97\n","▶ 모델 저장됨!!! @ 에피소드 10100\n","epiode #: 10123 loss: 9.147462947112217e-07 j: 620000\n","종료: done = True ... j = 624800  move = 87\n","▶ 모델 저장됨!!! @ 에피소드 10200\n","epiode #: 10201 loss: 8.159254321071785e-07 j: 625000\n","종료: done = True ... j = 628200  move = 68\n","epiode #: 10279 loss: 9.816360488912323e-07 j: 630000\n","▶ 모델 저장됨!!! @ 에피소드 10300\n","epiode #: 10361 loss: 2.6242713602186996e-07 j: 635000\n","▶ 모델 저장됨!!! @ 에피소드 10400\n","epiode #: 10439 loss: 1.1879928933922201e-06 j: 640000\n","▶ 모델 저장됨!!! @ 에피소드 10500\n","종료: done = True ... j = 644400  move = 71\n","epiode #: 10518 loss: 1.9965378328379302e-07 j: 645000\n","종료: done = True ... j = 645100  move = 7\n","epiode #: 10595 loss: 1.9965312958447612e-07 j: 650000\n","▶ 모델 저장됨!!! @ 에피소드 10600\n","epiode #: 10669 loss: 1.0746057341748383e-05 j: 655000\n","▶ 모델 저장됨!!! @ 에피소드 10700\n","epiode #: 10747 loss: 5.3435651352629066e-06 j: 660000\n","▶ 모델 저장됨!!! @ 에피소드 10800\n","epiode #: 10822 loss: 5.116548891237471e-07 j: 665000\n","epiode #: 10900 loss: 8.914023055694997e-06 j: 670000\n","▶ 모델 저장됨!!! @ 에피소드 10900\n","epiode #: 10985 loss: 4.008029009128222e-06 j: 675000\n","▶ 모델 저장됨!!! @ 에피소드 11000\n","종료: done = True ... j = 677200  move = 33\n","epiode #: 11066 loss: 2.431180746498285e-06 j: 680000\n","▶ 모델 저장됨!!! @ 에피소드 11100\n","종료: done = True ... j = 684900  move = 41\n","epiode #: 11144 loss: 3.654738748082309e-06 j: 685000\n","▶ 모델 저장됨!!! @ 에피소드 11200\n","epiode #: 11216 loss: 2.290679049110622e-06 j: 690000\n","epiode #: 11286 loss: 1.4406499815322604e-07 j: 695000\n","▶ 모델 저장됨!!! @ 에피소드 11300\n","epiode #: 11369 loss: 2.4142909751390107e-05 j: 700000\n","▶ 모델 저장됨!!! @ 에피소드 11400\n","epiode #: 11446 loss: 9.059932040145213e-07 j: 705000\n","종료: done = True ... j = 706000  move = 9\n","▶ 모델 저장됨!!! @ 에피소드 11500\n","epiode #: 11526 loss: 5.37641863047611e-05 j: 710000\n","종료: done = True ... j = 711900  move = 80\n","epiode #: 11600 loss: 0.00021701294463127851 j: 715000\n","▶ 모델 저장됨!!! @ 에피소드 11600\n","epiode #: 11675 loss: 0.00019117507326882333 j: 720000\n","▶ 모델 저장됨!!! @ 에피소드 11700\n","epiode #: 11749 loss: 0.00016692161443643272 j: 725000\n","▶ 모델 저장됨!!! @ 에피소드 11800\n","epiode #: 11825 loss: 0.00021303021640051156 j: 730000\n","epiode #: 11897 loss: 6.078444130253047e-05 j: 735000\n","▶ 모델 저장됨!!! @ 에피소드 11900\n","epiode #: 11978 loss: 0.00022566827828995883 j: 740000\n","▶ 모델 저장됨!!! @ 에피소드 12000\n","epiode #: 12052 loss: 5.7869932788889855e-05 j: 745000\n","▶ 모델 저장됨!!! @ 에피소드 12100\n","epiode #: 12125 loss: 3.591709537431598e-05 j: 750000\n","▶ 모델 저장됨!!! @ 에피소드 12200\n","epiode #: 12202 loss: 2.1114015908096917e-05 j: 755000\n","epiode #: 12287 loss: 2.9231816824903945e-06 j: 760000\n","▶ 모델 저장됨!!! @ 에피소드 12300\n","epiode #: 12362 loss: 9.238760867447127e-06 j: 765000\n","▶ 모델 저장됨!!! @ 에피소드 12400\n","epiode #: 12440 loss: 1.628551399335265e-05 j: 770000\n","▶ 모델 저장됨!!! @ 에피소드 12500\n","epiode #: 12519 loss: 7.419916414619365e-07 j: 775000\n","epiode #: 12600 loss: 2.6843565592571395e-07 j: 780000\n","▶ 모델 저장됨!!! @ 에피소드 12600\n","epiode #: 12677 loss: 6.82960717313108e-06 j: 785000\n","▶ 모델 저장됨!!! @ 에피소드 12700\n","epiode #: 12754 loss: 6.5952322074736e-06 j: 790000\n","▶ 모델 저장됨!!! @ 에피소드 12800\n","종료: done = True ... j = 793600  move = 72\n","epiode #: 12844 loss: 0.003604274243116379 j: 795000\n","▶ 모델 저장됨!!! @ 에피소드 12900\n","epiode #: 12929 loss: 3.0036944735911675e-05 j: 800000\n","▶ 모델 저장됨!!! @ 에피소드 13000\n","epiode #: 13030 loss: 0.00033374130725860596 j: 805000\n","▶ 모델 저장됨!!! @ 에피소드 13100\n","epiode #: 13101 loss: 3.6393266782397404e-05 j: 810000\n","epiode #: 13170 loss: 1.1780184649978764e-05 j: 815000\n","▶ 모델 저장됨!!! @ 에피소드 13200\n","epiode #: 13246 loss: 3.483199861875619e-06 j: 820000\n","▶ 모델 저장됨!!! @ 에피소드 13300\n","epiode #: 13328 loss: 8.043619345698971e-06 j: 825000\n","▶ 모델 저장됨!!! @ 에피소드 13400\n","epiode #: 13410 loss: 1.407312538503902e-05 j: 830000\n","종료: done = True ... j = 832100  move = 47\n","종료: done = True ... j = 834300  move = 27\n","epiode #: 13482 loss: 1.1035129318770487e-05 j: 835000\n","▶ 모델 저장됨!!! @ 에피소드 13500\n","epiode #: 13554 loss: 1.2422304280335084e-05 j: 840000\n","종료: done = True ... j = 841000  move = 53\n","▶ 모델 저장됨!!! @ 에피소드 13600\n","epiode #: 13630 loss: 9.78594835032709e-06 j: 845000\n","▶ 모델 저장됨!!! @ 에피소드 13700\n","epiode #: 13713 loss: 7.713338163739536e-06 j: 850000\n","epiode #: 13790 loss: 2.6488411094760522e-05 j: 855000\n","▶ 모델 저장됨!!! @ 에피소드 13800\n","종료: done = True ... j = 858100  move = 43\n","종료: done = True ... j = 858200  move = 43\n","epiode #: 13871 loss: 0.0005657756701111794 j: 860000\n","▶ 모델 저장됨!!! @ 에피소드 13900\n","epiode #: 13954 loss: 4.885155431111343e-05 j: 865000\n","종료: done = True ... j = 866700  move = 29\n","▶ 모델 저장됨!!! @ 에피소드 14000\n","epiode #: 14033 loss: 3.7320020055631176e-05 j: 870000\n","▶ 모델 저장됨!!! @ 에피소드 14100\n","epiode #: 14110 loss: 2.051501905953046e-05 j: 875000\n","epiode #: 14188 loss: 1.8892495063482784e-05 j: 880000\n","▶ 모델 저장됨!!! @ 에피소드 14200\n","종료: done = True ... j = 882900  move = 11\n","epiode #: 14267 loss: 1.2594007330335444e-06 j: 885000\n","▶ 모델 저장됨!!! @ 에피소드 14300\n","epiode #: 14343 loss: 7.356185051321518e-06 j: 890000\n","▶ 모델 저장됨!!! @ 에피소드 14400\n","epiode #: 14422 loss: 7.341096534219105e-06 j: 895000\n","▶ 모델 저장됨!!! @ 에피소드 14500\n","epiode #: 14505 loss: 2.2464225821750006e-06 j: 900000\n","epiode #: 14581 loss: 1.176742443931289e-06 j: 905000\n","▶ 모델 저장됨!!! @ 에피소드 14600\n","epiode #: 14655 loss: 5.9692479226214346e-06 j: 910000\n","▶ 모델 저장됨!!! @ 에피소드 14700\n","종료: done = True ... j = 913000  move = 69\n","epiode #: 14737 loss: 1.015843054119614e-06 j: 915000\n","▶ 모델 저장됨!!! @ 에피소드 14800\n","epiode #: 14818 loss: 0.00015689841529820114 j: 920000\n","종료: done = True ... j = 921500  move = 97\n","epiode #: 14898 loss: 1.4730652253547305e-07 j: 925000\n","▶ 모델 저장됨!!! @ 에피소드 14900\n"]}],"source":["#def main(): # DQN... Total Path 학습 모델\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","print('tensor in', device)  # GPU 사용 확인\n","# PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","PATH = __file__ + '/model_conv_colab_no_use.pt' ## PATH 경로 쉽게 수정\n","##############################################\n","tz = pytz.timezone('Asia/Seoul')\n","cur_time = datetime.now(tz)\n","start_time = cur_time.strftime(\"%H:%M:%S\")\n","\n","### 중요 Hyperparameters #####################\n","buffer_limit  = 10000     #1. 50000\n","batch_size    = 200        #2. 32\n","learning_rate = 0.001      #3. 0.0005\n","sync_freq = 500            #4. q 네트워크 파라미터를 q_target 네트워크에 복사하는 주기\n","train_start = 10000   # (추가)\n","##############################################\n","env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","q = Qnet()\n","q.to(device)\n","optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n","\n","train_model_load = False  # 학습한 모델을 불러와서 계속 학습시키고자 하는 경우 True로 바꾼다!!\n","if train_model_load:\n","    ###############################################################\n","    ## 학습한 모델 불러오기\n","    ###############################################################\n","    checkpoint = torch.load(PATH)\n","    q.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    q.train()\n","###############################################################\n","q_target = Qnet()\n","q_target.to(device)\n","q_target.load_state_dict(q.state_dict())\n","memory = ReplayBuffer()\n","\n","### Hyperparameters #####################\n","gamma = 0.90             #5. 0.98\n","####################################################################\n","# epochs = len(env.files)  #6. 훈련용 데이터 갯수\n","epochs = 50000  # 한 군데 갔다 오는 경우에 사용. 반복 횟수는 각자 지정\n","####################################################################\n","losses = []\n","moves = []\n","max_moves = 100          #7.\n","print_interval = 5000    #8.\n","j = 0\n","goal_ob_reward_count = 0 #9. epoch를 반복하면서 finish 카운팅\n","epsilon = 0.3            #10. annealing 대신 고정\n","loss = 0.0\n","##############################################\n","# __file__ = '/content/drive/MyDrive/aiffelthon/data' \n","### 액션 저장하는 txt파일 만들기 #######################\n","f = open(__file__ + 'ogz_conv_gif' + '.txt', 'w')\n","########################################################\n","\n","for n_epi in range(epochs):\n","    # epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n","    gridmap = env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        # self.epi = epi                                            #1. 에피소드 번호 받기\n","        # self.items = list(self.files.iloc[self.epi])[0]           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        # self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        # self.terminal_location = None                             #4. 최초 목적지\n","        # self.local_target = []                                    #5. 목적지 리스트 초기화\n","        # self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        # self.grid = np.ones((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(1)로 초기화\n","        # self.set_box()                                            #8. 선반 위치(0), 목적지(2) 그리드값 설정.  목적지 리스트 생성\n","        # self.set_obstacle()                                       #9. 장애물 그리드값(0) 설정\n","        # self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        # self.grid[int(self.curloc[0])][int(self.curloc[1])] = -1  #11. 현재 위치(출발점) 그리드값(-1) 세팅\n","        # self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        # return self.grid\n","    state1_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","    state1 = torch.unsqueeze(state1_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","    state1 = torch.unsqueeze(state1,0)  # conv2d용 (1, 1, 10, 9)\n","    #print(state1.shape)\n","    done = False\n","    mov = 0\n","\n","    while (not done):  # 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과하면 종료\n","        j += 1\n","        mov += 1\n","        # 상태 state1에서 action 결정\n","        #####################################################################################\n","        if env.curloc == [9,4]:  # 출발점에서는 무조건 위로 올라간다 (action=0)\n","            action = 0\n","        elif env.curloc[0] == 0 and env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","            action = 1\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","            action = 2\n","        elif env.curloc[0] in [2,3,4,5] and env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","            action = 3\n","        else:  # 위 경우를 제외하곤 e-greedy로 액션 선택\n","            action = q.sample_action(state1, epsilon)\n","        #####################################################################################\n","        # action = q.sample_action(state1, epsilon)  # 위 조건을 적용 안 할 경우\n","        # make Move... action에 따른 이동\n","        gridmap, reward, cum_reward, done = env.step(action)\n","            # self.actions.append((new_x, new_y))...새로 도착한 위치를 경로 리스트에 추가\n","            # 누적 보상 계산\n","        ###########################\n","        # print('j =', j, 'action =', action)\n","        # print(gridmap.reshape(10,9))\n","        ###########################\n","        state2_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","        state2 = torch.unsqueeze(state2_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","        state2 = torch.unsqueeze(state2,0)  # conv2d용 (1, 1, 10, 9)\n","        done_mask = 0.0 if done else 1.0\n","        memory.put((state1, action, reward, state2, done_mask))  # 경험을 메모리에 저장\n","        state1 = state2\n","\n","        # if memory.size() > train_start:  # 메모리에 train_start 크기 이상 쌓이면... 미니배치 훈련\n","        if j > train_start:  # 메모리에 미니배치 크기 이상 쌓이면... 미니배치 훈련\n","            #print('memory size:', memory.size())\n","            loss = train(q, q_target, memory, optimizer)\n","            if j % print_interval == 0:\n","                print('epiode #:', n_epi, 'loss:', loss, 'j:', j)\n","            losses.append(loss)\n","\n","            if j % sync_freq == 0:  # sync_freq마다 q 네트워크 파라미터를 q_target 네트워크로 복사\n","                q_target.load_state_dict(q.state_dict())\n","\n","        ############## 현황 모니터링 #############################################################\n","        if done and j % 100 == 0:  # 100번 마다 mov 횟수 저장:\n","            moves.append(mov)\n","            print('종료: done =', done, '... j =', j, ' move =', mov)\n","        if env.goal_ob_reward:  # 최종 목적지 도달한 경우에만 화면에 표시\n","            goal_ob_reward_count += 1\n","            print(env.items, '종료: env.goal_ob_reward =', env.goal_ob_reward, '... j =', j, ' move =', mov, '@ 에피소드 #', n_epi)\n","            print(f\"{n_epi}번째 에피소드까지 총 {goal_ob_reward_count}번 finish 했습니다.\")\n","        ##########################################################################################\n","\n","        if done or mov > max_moves:\n","            mov = 0\n","            done = True\n","\n","    # while loop 종료 ----- 최종 목적지 [9,4] 도착 또는 max_moves (=350) 초과\n","    \n","    if j > train_start:  # 메모리에 train_start 크기 이상 쌓이면... 미니배치 훈련\n","        if (n_epi % 100 == 0 and n_epi != 0) or n_epi >= epochs - 1:\n","            torch.save({'epoch': n_epi, \n","                        'model_state_dict': q.state_dict(), \n","                        'optimizer_state_dict': optimizer.state_dict(), \n","                        'loss': loss, \n","                        }, PATH)\n","            print('▶ 모델 저장됨!!! @ 에피소드', n_epi)\n","\n","        ##### gif 생성을 위한 경로(env.actions) 저장 ####################################    \n","        if n_epi % 1000 == 0 or env.goal_ob_reward:\n","            f.write(str(n_epi)+'/'+str(env.local_target_original)+'\\n')\n","            f.write(str(env.actions))\n","            f.write('\\n')\n","        ######################################### \n","f.close()\n","#########################################\n","## 프로그램 시작 및 종료 시간 출력\n","cur_time = datetime.now(tz)\n","end_time = cur_time.strftime(\"%H:%M:%S\")\n","print ('실행 종료!', 'Start@', start_time, 'End@', end_time)\n","        #cur_time = datetime.now(tz)\n","        #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","        #print('▶ Episode #', iter, 'start time:', simple_cur_time, end='→')\n","\n","print(gridmap.reshape(10,9))  # 최종 그리드맵 확인\n","\n","## loss 그래프\n","plt.figure(figsize=(10,7))\n","plt.plot(losses)\n","plt.xlabel(\"# of Actions\",fontsize=22)\n","plt.ylabel(\"Loss\",fontsize=22)\n","\n","## 경로길이(이동 횟수) 그래프\n","plt.figure(figsize=(10,7))\n","plt.plot(moves)\n","plt.xlabel(\"# of Actions x 100\",fontsize=22)\n","plt.ylabel(\"Moves\",fontsize=22)"]},{"cell_type":"markdown","metadata":{"id":"q2kNcb0-3j-s"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"EUyczG0z3nay"},"source":["## 테스트 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfW9OaPsekYX"},"outputs":[],"source":["## test 코드 #################################################################################\n","def test(epochs=1000, train_mode=False, display=True, model_load=True):  # train_mode = False 테스트 모드\n","##############################################################################################\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","    print('tensor in', device)  # GPU 사용 확인\n","    PATH = '/content/drive/MyDrive/aiffelthon/data/model_conv_colab.pt'\n","    test_env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","    test_q = Qnet()\n","    test_q.to(device)\n","\n","    if model_load:\n","        ###############################################################\n","        ## 학습한 모델 불러오기\n","        ###############################################################\n","        checkpoint = torch.load(PATH)\n","        q.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","        q.eval()\n","\n","    ### 액션 저장하는 txt파일 만들기 #######################\n","    f = open('ogz_conv_test_gif' + '.txt', 'w')\n","    ########################################################\n","    #epochs = len(test_env.files)\n","    wins = 0\n","    for n_epi in range(epochs):\n","        gridmap = test_env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        state_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","        state = torch.unsqueeze(state_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","        state = torch.unsqueeze(state,0)  # conv2d용 (1, 1, 10, 9)\n","        done = False\n","        mov = 0\n","        while (not done):  # 최종 목적지 [9,4] 도착\n","            mov += 1\n","            # 상태 state1에서 action 결정\n","            #####################################################################################\n","            if test_env.curloc == [9,4]:  # 출발점에서는 무조건 위로 (action=0)\n","                action = 0\n","            elif test_env.curloc[0] == 0 and test_env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","                action = 1\n","            elif test_env.curloc[0] in [2,3,4,5] and test_env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","                action = 2\n","            elif test_env.curloc[0] in [2,3,4,5] and test_env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","                action = 3\n","            else:  # 위 경우를 제외하곤 test 액션 선택\n","                action = q.test_action(state)\n","            #####################################################################################\n","            # action = q.test_action(state)  # 위 조건을 적용하지 않을 경우\n","            # make Move... action에 따른 이동\n","            gridmap, reward, cum_reward, done = test_env.step(action)\n","            state_ = torch.from_numpy(gridmap).float()  # 넘파이 array를 Torch 텐서로 변환  # (10, 9)\n","            state = torch.unsqueeze(state_,0)  # conv2d용 차원 증가 (1, 10, 9)\n","            state = torch.unsqueeze(state,0)  # conv2d용 (1, 1, 10, 9)\n","            if display:\n","                print('Move #%s; Taking action: %s' % (mov, action))\n","                print(gridmap.reshape(10,9))\n","\n","            if test_env.goal_ob_reward:\n","                wins += 1\n","                if display:\n","                    print(\"Game won! Reward: %s\" % (cum_reward))\n","                else:\n","                    print(\"Game LOST. Reward: %s\" % (cum_reward))\n","                    \n","            #if (mov > 100):  # 경로 길이가 너무 길어서 제외시키는 조건 추가 필요시 사용\n","                #if display:\n","                    #print(\"Game lost; too many moves.\")\n","                #break\n","        \n","    win_perc = float(wins) / float(epochs)\n","    print(\"Test performed: {0}, # of wins: {1}\".format(epochs,wins))\n","    print(\"Win percentage: {}%\".format(100.0*win_perc))\n","    return win_perc"]},{"cell_type":"code","source":["test(epochs=10, train_mode=False, display=True, model_load=True)"],"metadata":{"id":"vmYkpzs2ryd5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ho_Baseline_DQN_Conv2D_v1.0_0602.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOVJnJT2VwKm","executionInfo":{"status":"ok","timestamp":1654442549196,"user_tz":-540,"elapsed":2801,"user":{"displayName":"오강자","userId":"02252026995617554470"}},"outputId":"e5aa0a7e-5255-4f7a-aabc-46755f0a4d1a"},"id":"zOVJnJT2VwKm","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"id":"b8842c1a","metadata":{"id":"b8842c1a","executionInfo":{"status":"ok","timestamp":1654442549197,"user_tz":-540,"elapsed":8,"user":{"displayName":"오강자","userId":"02252026995617554470"}}},"outputs":[],"source":["__file__ = '/content/drive/MyDrive/data'"]},{"cell_type":"code","execution_count":3,"id":"6719ef2f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6719ef2f","executionInfo":{"status":"ok","timestamp":1654442550113,"user_tz":-540,"elapsed":922,"user":{"displayName":"오강자","userId":"02252026995617554470"}},"outputId":"569f0032-0de7-467e-9c32-50018d95dc92"},"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -1 -10 10\n"]}],"source":["from string import ascii_uppercase\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","###################################\n","# 보상 Reward\n","move_reward = -1  # 0.1\n","obs_reward = -10  # 0.1\n","goal_reward = 10  # 10\n","###################################\n","# 그리드값\n","# target_gridval = 10, 9, 8... # 목표물: 10, 9, 8, ...\n","curloc_gridval  = 1            # 현 위치: 1\n","default_gridval = 0            # 기본: 0\n","rack_gridval    = -1           # 선반: -1\n","obs_gridval     = -10          # 장애물: -10\n","###################################\n","# train / test 모드 지정\n","train_mode = False\n","###################################\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","PATH = __file__ + '/model_ho_v2_baseline_ddqn.pt'\n","\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","class Simulator:\n","    def __init__(self):\n","        # Load train or test data\n","        if train_mode:  # 훈련 데이타 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","            print('data/factory_order_train.csv used')\n","        else:           # 테스트 데이터 읽기\n","            self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_test.csv\"))\n","            print('data/factory_order_test.csv used')\n","        ##########################################################################################\n","        self.height = 10  # 그리드 높이\n","        self.width = 9    #  그리드 너비\n","        self.inds = list(ascii_uppercase)[:17]  # A ~ Q alphabet list\n","\n","    def set_box(self):  # 선반 위치, 목적지 그리드값 설정. 목적지 리스트 생성\n","        box_data = pd.read_csv(os.path.join(local_path, \"data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = rack_gridval\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = rack_gridval\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = target_gridval\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","            self.local_target.append([getattr(order_box, \"row\"), getattr(order_box, \"col\")])  # 목적지 리스트 생성: local_target\n","        # self.local_target.sort() # 불필요. 인풋 데이터 A, B, C순 정렬되어 있음... 정렬 필요시 코드 바꾸어야 함\n","        self.local_target.append([9,4]) # 목적지 리스트에 최종 목적지(9,4) 추가\n","        self.local_target_original = self.local_target.copy()  # gif 생성을 위해 추가.  에피소드의 경로 저장\n","        self.target_length = len(self.local_target_original)  # 목적지 그리드값 10, 9, 8, ... 지정 위해 추가\n","        self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","\n","    def set_obstacle(self):  # 장애물 위치 그리드값 설정 = obs_gridval\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = obs_gridval\n","\n","    ########################################################################################################\n","    '''\n","    writer 신휘정\n","    step에서 아이템을 먹으면 self.items의 첫번째 요소를 pop해줍니다.\n","    그리고 새로운 item에 대해서 grid_box를 그려줍니다.\n","    '''\n","    ########################################################################################################\n","    def grid_box(self):  # 그리드 박스 초기화 용도로 정의: 선반, 목적지, 장애물 그리드값 초기화\n","        box_data = pd.read_csv(os.path.join(local_path, \"data/box.csv\"))\n","        for box in box_data.itertuples(index = True, name ='Pandas'):  # 선반 위치: 그리드값 = rack_gridval\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = rack_gridval\n","        # order_item = list(set(self.inds) & set(self.items))\n","        if len(self.items) > 0:\n","            order_item = list(self.items)\n","            order_csv = box_data[box_data['item'].isin(order_item)]\n","            for i, order_box in enumerate(order_csv.itertuples(index = True, name ='Pandas')):  # 목적지: 그리드값 = target_gridval\n","                self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 10 - i\n","            self.grid[9,4] = 10 + 1 - self.target_length  # 최종 목적지 [9,4] 그리드값 지정 위해 추가\n","        else : \n","            self.grid[9,4] = 10\n","        self.set_obstacle()  # 장애물 위치 그리드값 설정 = obs_gridval\n","        \n","    def reset(self, epi):  # 에피소드 시작시 12개 값 초기화\n","        self.epi = epi                                            #1. 에피소드 번호 받기\n","        self.items = eval(list(self.files.iloc[self.epi])[0])           #2. 에피소드의 목적지 받기: [ 'H', 'L', 'M']\n","        #####################################################################################\n","        # coin = random.randint(0,16)\n","        # self.items = [chr(coin+65)]  # A~Q 까지 1개 랜덤으로 받아서 items에 1개 넣어주기, 반복 횟수는 main의 epochs 조절하기\n","#         self.items = [chr(epi+65)]\n","        ####################################################################################\n","        self.cumulative_reward = 0                                #3. 누적 보상값 = 0\n","        self.terminal_location = None                             #4. 최초 목적지\n","        self.local_target = []                                    #5. 목적지 리스트 초기화\n","        self.actions = []                                         #6. 지나온 경로 리스트 초기화\n","        self.grid = np.zeros((self.height, self.width), dtype=\"float16\")  #7. 그리드월드 전체 그리드값(default_gridval) 초기화\n","        self.set_box()                                            #8. 선반 위치, 목적지 그리드값 설정. 목적지 리스트 생성\n","        self.set_obstacle()                                       #9. 장애물 그리드값 설정\n","        ######################\n","        # print('최초 그리드맵:')  # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        self.curloc = [9, 4]                                      #10. 현재 위치를 출발점으로 세팅\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = curloc_gridval  #11. 현재 위치(출발점) 그리드값 세팅\n","        self.done = False                                         #12. 경로 찾기 종료 여부 = False\n","        self.goal_ob_reward = False                      # (추가) #13. 최종 목적지 도착 여부 = False\n","        ######################\n","        # print('현 위치 추가:')   # 그리드맵 확인\n","        # print(self.grid.reshape(10,9))\n","        ######################\n","        self.move_track = [[9,4]]                                  #14. 이동하는 경로를 저장하는 변수, 에피소드 시작시 에이전트가 [9,4]에 있으므로 [[9,4]]로 초기화\n","        self.isReset =  False                                      #15. 처음으로 terminal_location == 9,4가 되면, moveTrack을 초기화해주기 위해 필요한 변수 isFinal -> isReset 로 변경(처음으로 불러졌냐 안불러졌냐)\n","        self.prior_distance = 9999                                 #16. 이전 time step의 거리와 현재 거리를 비교하기 위해서 필요한 변수\n","        ######################\n","        self.wasGoal = False                                       # 17. 이전 step이 Goal이었는지 확인하는 변수\n","                                                                   # 이전 step이 Goal이었으면 먹고 나가야 하는 곳이 move_track이 곂치기 때문에 -보상받음. 이를 방지해기위한 FLAG\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):  # action에 따른 새 좌표값 반환\n","        new_x = cur_x\n","        new_y = cur_y\n","        if action == 0:          # up\n","            #new_x = cur_x - 1\n","            new_x = self.move_up(cur_x, cur_y, new_x, new_y)\n","        elif action == 1:        # down\n","            #new_x = cur_x + 1\n","            new_x = self.move_down(cur_x, cur_y, new_x, new_y)\n","        elif action == 2:        # left\n","            #new_y = cur_y - 1\n","            new_y = self.move_left(cur_x, cur_y, new_x, new_y)\n","        else:                    # right\n","            #new_y = cur_y + 1\n","            new_y = self.move_right(cur_x, cur_y, new_x, new_y)\n","        return int(new_x), int(new_y)\n","    \n","    # >>> 현재 위치에서 이동이 불가한 위치 추가 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def move_up(self, cur_x, cur_y, new_x, new_y):  # action == 0:\n","        if cur_x in [6,5,4,3,2] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x - 1\n","        return new_x\n","    def move_down(self, cur_x, cur_y, new_x, new_y): # action == 1:\n","        if cur_x in [1,2,3,4,5] and cur_y in [0,8]:\n","            pass\n","        else:\n","            new_x = cur_x + 1\n","        return new_x\n","    def move_left(self, cur_x, cur_y, new_x, new_y): # left elif action == 2:\n","        if cur_y in [1,2,3,4,5,6,7,8] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y - 1\n","        return new_y\n","    def move_right(self, cur_x, cur_y, new_x, new_y): # right else: action == 3:\n","        if cur_y in [0,1,2,3,4,5,6,7] and cur_x == 0:\n","            pass\n","        else:\n","            new_y = cur_y + 1\n","        return new_y\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):  # action에 의해 이동시 얻는 보상\n","        if any(out_of_boundary):                                        # 바깥으로 나가는 경우\n","            reward = obs_reward\n","        else:\n","            if self.grid[new_x][new_y] == obs_gridval :     # 장애물에 부딪히는 경우\n","                reward = obs_reward\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치(장애물)에서 종료되었음을 표시. 원래는 장애물이었음                \n","            elif self.grid[new_x][new_y] == rack_gridval:   # 선반에 부딪히는 경우\n","                reward = obs_reward\n","                new_x = self.curloc[0]  # 선반인 경우 못들어가고, 현재 위치로 유지\n","                new_y = self.curloc[1]  # 선반인 경우 못들어가고, 현재 위치로 유지\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  # 현재 목적지에 도달한 경우\n","                reward = goal_reward\n","                self.prior_distance = 9999                  # 목적지에 갈 때 prior_distance = 1이라, 나갈 때 무조건 prior_distance가 작아서 negative_reward를 받기때문에 여기서 못받도록 수정해줌.\n","            # 그냥 움직이는 경우 \n","            else: \n","                cur_distance = self.get_distance()\n","                if self.prior_distance > cur_distance :                         # 이전의 거리 > 현재의 거리 = 현재 아이템에 더 가깝다 => + 보상얻기\n","                    reward = self.get_positive_distance_reward(cur_distance)\n","                else:                                                           # 이전의 거리 < 현재의 거리 = 현재 아이템에서 멀어졌다 => - 보상얻기\n","                    reward = self.get_negative_distance_reward(cur_distance)\n","                self.prior_distance = cur_distance      \n","\n","                if [new_x, new_y] not in self.move_track:                           # 이동한 곳이 처음온 곳이면 move_track에 추가\n","                    self.move_track.append([new_x, new_y])\n","                else:                                                           # 이동한 곳이 이미 왔던 곳이면 기존 보상에서 -1\n","                    reward = reward -1\n","        return reward\n","\n","    def step(self, action):  # action에 따른 이동 실행\n","        self.terminal_location = self.local_target[0]           # 목적지 리스트의 첫 번째 요소를 목적지로 설정\n","        cur_x, cur_y = self.curloc                              # 현재 위치 기억\n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 다음 위치 받기\n","        self.actions.append((cur_x, cur_y))                     # 현재 위치(지나온 위치)를 경로 리스트에 추가\n","        self.goal_ob_reward = False                             # 최종 목적지에 도착한 경우에 True. self.reset()에서 초기화\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # 그리드월드 밖으로 나갔는가? OB = True\n","\n","        if any(out_of_boundary):  #1. 바깥으로 나가는 경우, 빈 선반에 부딪치는 경우 종료하지 않음\n","            #print('OB')\n","            new_x = cur_x  # 종료 처리할 경우 코멘트 처리\n","            new_y = cur_y  # 종료 처리할 경우 코멘트 처리\n","            ######################################################종료 처리할 경우 실행\n","            #self.done = True                             # while loop 종료\n","            #self.grid_box()                              # 선반, 목적지, 장애물 그리드값 초기화\n","            #self.actions.append((new_x,new_y))           # 경로 리스트에 최종 위치 추가. (주의) 좌표값이 그리드월드를 벗어나는 값 발생!\n","            ######################################################\n","        else:\n","            if self.grid[new_x][new_y] == obs_gridval:  #2. 장애물에 부딪히는 경우 종료\n","                # print('장애물')\n","                # new_x = cur_x  # 종료 처리할 경우 코멘트 처리\n","                # new_y = cur_y  # 종료 처리할 경우 코멘트 처리\n","                ###################################################종료 처리할 경우 실행\n","                self.done = True                           # while loop 종료\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.actions.append((new_x,new_y))         # 경로 리스트에 최종 위치 추가\n","                ###################################################\n","            elif self.grid[new_x][new_y] == rack_gridval:\n","                pass\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:  #3. 현재 목적지에 도착한 경우 다음 목적지 설정\n","                #print('목적지 도착')\n","                if [new_x, new_y] == [9, 4]:            #3-1. 최종 목적지에 도착한 경우 종료\n","                    self.done = True                       # while loop 종료\n","                    self.goal_ob_reward = True             # True = 1 (OB, 장애물 종료 처리 할 경우 self.done=True가 많아지므로 이때 사용)\n","                    self.actions.append((new_x,new_y))     # 경로 리스트에 최종 위치 추가\n","                ########################################################################################################\n","                '''\n","                writer 신휘정\n","                기존 : 아이템을 먹어도 에피소드 동안 목적지, 선반 숫자는 유지됨.\n","                현재 : 아이템을 먹으면 남은 아이템에 대해서 숫자가 새로 칠해짐\n","                '''\n","                if len(self.items) > 0:\n","                    self.items.remove(self.items[0])            # 아이템을 하나먹었으니, 아이템의 첫번째 값을 삭제해줌\n","                    self.wasGoal = True\n","                    self.move_track.pop()                       # 아이템을 들어가고 나오는 위치는 동일(주황색동그라미) => 이미 왔던경로라고 - 리워드 못받게 하기위해 마지막 요소 pop()\n","                ########################################################################################################                    \n","\n","                self.local_target.remove(self.local_target[0])  # 다음 목적지 설정. 최종 목적지에 도착한 경우에는 마지막 요소인 [9,4]를  제거\n","                self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치(목적지)에서 종료되었음을 표시. 원래는 목적지이었음\n","                self.curloc = [new_x, new_y]               # 새로 도착한 위치를 현재 위치로 변경\n","            \n","            else:                                       #4. 그냥 움직이는 경우\n","                #print('이동')\n","                ########################################################################################################\n","                '''\n","                writer 신휘정\n","                기존 : 아이템을 먹어도 에피소드 동안 목적지, 선반 숫자는 유지됨.\n","                현재 : 아이템을 먹으면 해당 선반은 빈선반으로 바뀜\n","                '''\n","                if self.wasGoal:\n","                    self.grid[cur_x][cur_y] = rack_gridval     # 현 위치의 그리드값을 선반값으로 초기화\n","                    self.wasGoal = False\n","                else:\n","                    self.grid[cur_x][cur_y] = default_gridval  # 현 위치의 그리드값을 기본값으로 초기화\n","                ########################################################################################################\n","                self.grid_box()                            # 선반, 목적지, 장애물 그리드값 초기화\n","                self.grid[new_x][new_y] = curloc_gridval   # 현 위치에서 종료되었음을 표시\n","                self.curloc = [new_x,new_y]\n","\n","        ############################### 보상받는 처리 시작 ###############################\n","        # 마지막 위치에서 위로 올라가는 행동을 한다면 - 보상받음 태양\n","        if len(self.local_target) == 1:                             # local_target이 remove되고 나면 길이가 1이되는 경우 = [9,4]만 있는 경우\n","            if not self.isReset:                                    # 처음으로 terminal_location == [9,4]가 되었다면,,\n","                self.move_track = []                                # 아이템을 찾으러 돌아다니는 경로 초기화\n","                self.isReset = True                                 # move_track이 초기화 되었으니 isReset = True\n","            reward = self.get_reward(new_x, new_y, out_of_boundary)\n","            if action == 0:\n","                reward = obs_reward                                # terminal_location == [9,4]로 갈 때 위로 올라가는거 금지\n","        else : \n","            reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        ############################### 보상받는 처리 끝 ###############################   \n","\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done\n","\n","    # >>> distance를 입력으로 받아 + 보상을 계산하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< \n","    def get_positive_distance_reward(self, distance):\n","        positive_reward = (20 - distance) / 200     # 0.095 ~ 0.001\n","        return positive_reward\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","    # >>> distance를 입력으로 받아 - 보상을 계산하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def get_negative_distance_reward(self, distance):\n","        negative_reward = -(distance / 200)\n","        return negative_reward\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    \n","\n","    # >>> 현재 위치에서 타겟까지의 거리를 구하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def get_distance(self): \n","        cur_x, cur_y, target_x, target_y = self.get_current_state()        \n","        distance = (abs(cur_x-target_x) + abs(cur_y-target_y))\n","        # print(cur_x, cur_y, target_x, target_y)\n","        # print('distance : ', distance)\n","        return distance\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<    \n","\n","    # >>> 현재 상황에 대한 정보를 구하는 메소드 (시작) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","    def get_current_state(self):\n","        # if len(self.local_target) != 1:\n","        #     state = (self.curloc[0], self.curloc[1], self.local_target[0][0], self.local_target[0][1])\n","        # else :\n","        state = (self.curloc[0], self.curloc[1], self.terminal_location[0], self.terminal_location[1])\n","        return np.array(state) #(cur_x, cur_y, tar_x, tar_y)\n","    # >>> 추가 (끝) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<        \n"]},{"cell_type":"code","execution_count":4,"id":"b29579a5","metadata":{"id":"b29579a5","executionInfo":{"status":"ok","timestamp":1654442551161,"user_tz":-540,"elapsed":1049,"user":{"displayName":"오강자","userId":"02252026995617554470"}}},"outputs":[],"source":["import collections\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ReplayBuffer():  # 리플레이 버퍼: 경험 저장소\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","    \n","    def put(self, transition):  # 리플레이 버퍼(메모리)를 경험(transition)으로 채우기\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):  # memory.sample(batch_size)로 사용\n","        mini_batch = random.sample(self.buffer, n)  # 미니배치 샘플링\n","        '''\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","        for transition in mini_batch:  # 경험의 각 요소들을 각각의 미니배치로 구성\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)\n","            a_lst.append([a])\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","        '''\n","        state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in mini_batch])\n","        action_batch = torch.Tensor([[a] for (s1,a,r,s2,d) in mini_batch])      # a를 tensor에 넣을 때는 [리스트]에 꼭 씌워주세요\n","        reward_batch = torch.Tensor([[r] for (s1,a,r,s2,d) in mini_batch])      # reward를 tensor에 넣을 때는 [리스트]에 꼭 씌워주세요\n","        state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in mini_batch])\n","        done_batch = torch.Tensor([[d] for (s1,a,r,s2,d) in mini_batch])        # done를 tensor에 넣을 때는 [리스트]에 꼭 씌워주세요\n","\n","        return state1_batch, action_batch, reward_batch, state2_batch, done_batch\n","    \n","    def size(self):  # 메모리 크기\n","        return len(self.buffer)\n","\n","class Qnet(nn.Module):\n","    '''##### Linear 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        in_size = 90  # input 크기\n","        L1 = 250\n","        L2 = 150\n","        out_size  = 4\n","\n","        self.fc1 = nn.Linear(in_size, L1)\n","        self.fc2 = nn.Linear(L1, L2)\n","        self.fc3 = nn.Linear(L2, out_size)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","    '''\n","    ##### Convolution 모델 #####\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(4, 32, 3, 1) #### 1이었던 부분을 4로 변경\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1) #### in_채널, out_channel, kernel_size, stride\n","        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n","        self.fc1 = nn.Linear(768, 512)\n","        self.fc2 = nn.Linear(512, 4)\n","\n","    def forward(self, x):\n","        x = x.to(device)               # (N, 4, 10, 9)\n","        x = F.relu(self.conv1(x))      # (N, 32, 8, 7)\n","        x = F.relu(self.conv2(x))      # (N, 64, 6, 5)\n","        x = F.relu(self.conv3(x))\n","        x = torch.flatten(x, 1)        # (N, 64, 30)\n","        x = F.relu(self.fc1(x))        # (N, 1920) -> (N, 128)\n","        x = self.fc2(x)                # (N, 128)  -> (N, 4)\n","        return x\n","\n","    def sample_action(self, obs, epsilon):  # Qnet()을 실행해서 모든 action에 대한 Qval을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        coin = random.random()  # e-greedy로 action 선택\n","        if coin < epsilon:\n","            return random.randint(0,3)\n","        else : \n","            return out.argmax().item()\n","    \n","    def test_action(self, obs):  # Qnet()을 실행해서 모든 action에 대한 Qvalue을 구한 후 action 선택\n","        out = self.forward(obs)  # out = Q_value, obs = state = 그리드맵\n","        return out.argmax().item()\n","\n","##################################################### DOUBLE _ DQN ######################################################       \n","def train(q, q_target, memory, optimizer):  # 메모리에 경험이 미니배치 크기 이상 쌓이면 미니배치 훈련 시작\n","    s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 메모리에서 미니배치를 뽑기: batch_size=1600\n","    \n","    q_out = q(s)  # Qnet의 Qvalue\n","    q_a = q_out.gather(dim=1,index=a.long().to(device)) # Qnet의 Qvalue 변환\n","\n","    with torch.no_grad():\n","        q_prime_idx = q(s_prime).max(1)[1]                                      # output : tensor (8)\n","        max_q_primes = q_target(s_prime)                                        # output : tensor (8, 4)\n","        max_q_prime = max_q_primes.gather(1, q_prime_idx.unsqueeze(1))          # output : tensor (8, 1)\n","\n","    Y = r.to(device) + gamma * max_q_prime * done_mask.to(device)               # output : tensor (8, 1)\n","\n","    loss = F.smooth_l1_loss(q_a, Y.detach())\n","    #loss = nn.MSELoss(X.to(torch.float32), Y.detach().to(torch.float32))  # 손실함수 변경. target.detach()로 변경\n","    optimizer.zero_grad()  # gradient값 초기화\n","    loss.backward()  # 자동미분\n","    optimizer.step()  # gradient 업데이트\n","    return loss.item()\n","##################################################### DOUBLE _ DQN ######################################################       \n","\n","########################################################## DQN ##########################################################\n","# def train(q, q_target, memory, optimizer):  # 메모리에 경험이 미니배치 크기 이상 쌓이면 미니배치 훈련 시작\n","#     '''\n","#     for i in range(10):\n","#         s, a, r, cr, s_prime, done_mask, gr = memory.sample(batch_size)\n","#         q_out = q(s)\n","#         q_a = q_out.gather(1,a.to(device))\n","        \n","#         max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n","#         target = r.to(device) + gamma * max_q_prime * done_mask.to(device)\n","        \n","#         loss = F.smooth_l1_loss(q_a, target)\n","        \n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","#     '''\n","    # s,a,r,s_prime,done_mask = memory.sample(batch_size)  # 메모리에서 미니배치를 뽑기: batch_size=1600\n","    # Q1 = q(s)  # Qnet의 Qvalue\n","    # X = Q1.gather(dim=1,index=a.long().unsqueeze(dim=1).to(device)).squeeze() # Qnet의 Qvalue 변환\n","    # with torch.no_grad():\n","    #     Q2 = q_target(s_prime)\n","    # Y = r.to(device) + gamma * done_mask.to(device) * torch.max(Q2,dim=1)[0]\n","    # loss = F.smooth_l1_loss(X, Y.detach())\n","    # #loss = nn.MSELoss(X.to(torch.float32), Y.detach().to(torch.float32))  # 손실함수 변경. target.detach()로 변경\n","    # optimizer.zero_grad()  # gradient값 초기화\n","    # loss.backward()  # 자동미분\n","    # optimizer.step()  # gradient 업데이트\n","    # return loss.item()\n","########################################################## DQN ##########################################################"]},{"cell_type":"code","execution_count":5,"id":"bc29ca14","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bc29ca14","executionInfo":{"status":"ok","timestamp":1654442551162,"user_tz":-540,"elapsed":6,"user":{"displayName":"오강자","userId":"02252026995617554470"}},"outputId":"d1c09067-9e08-4977-907d-77a1cbe04e5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor in cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU를 사용할 경우 ???\n","print('tensor in', device)  # GPU 사용 확인\n","## test 코드 #################################################################################\n","def test(epochs=1000, train_mode=False, display=True, model_load=True, PATH=PATH):  # train_mode = False 테스트 모드\n","\n","    test_env = Simulator()  # 데이터셋 읽기. 그리드월드 크기 지정\n","    test_q = Qnet()\n","    test_q.to(device)\n","    learning_rate = 0.00025    #3. 0.0005\n","    optimizer = optim.Adam(test_q.parameters(), lr=learning_rate)\n","\n","    if model_load:\n","        ###############################################################\n","        ## 학습한 모델 불러오기\n","        ###############################################################\n","        checkpoint = torch.load(PATH)\n","        test_q.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","        test_q.eval()\n","\n","    ### 액션 저장하는 txt파일 만들기 #######################\n","    f = open(__file__ + '/model_ho_v2_baseline_ddqn_TEST' + '.txt', 'w')\n","    ########################################################\n","        \n","    #epochs = len(test_env.files)\n","    wins = 0\n","\n","    for n_epi in range(epochs):\n","#         print('new_EPI #################################################')\n","        gridmap = test_env.reset(n_epi)  # 에피소드 시작시 12개 값 초기화\n","        history_ = np.stack((gridmap,gridmap,gridmap,gridmap), axis = 0) # 초기 state로 히스토리 초기화 (4,10,9)\n","        history = np.reshape([history_], (1,4,10,9)) # (배치, 채널, 가로, 세로) 로 reshape\n","        history = torch.from_numpy(history).float() # 텐서로 변환   \n","        done = False\n","        mov = 0\n","        while (not done):  # 최종 목적지 [9,4] 도착\n","            mov += 1\n","            # 상태 state1에서 action 결정\n","            #####################################################################################\n","            if test_env.curloc == [9,4]:  # 출발점에서는 무조건 위로 (action=0)\n","                action = 0\n","            elif test_env.curloc[0] == 0 and test_env.curloc[1] in [0,1,2,3,4,5,6,7,8]:  # (0,0)~(0,8)에서는 무조건 아래로\n","                action = 1\n","            elif test_env.curloc[0] in [2,3,4,5] and test_env.curloc[1] == 8:  # (2,8)~(5,8)에서는 무조건 왼쪽으로\n","                action = 2\n","            elif test_env.curloc[0] in [2,3,4,5] and test_env.curloc[1] == 0:  # (2,0)~(5,0)에서는 무조건 오른쪽으로\n","                action = 3\n","            else:  # 위 경우를 제외하곤 test 액션 선택\n","                action = test_q.test_action(history)\n","            #####################################################################################\n","            # action = test_q.test_action(history)  # 위 조건을 적용하지 않을 경우\n","            # make Move... action에 따른 이동\n","            gridmap, reward, cum_reward, done = test_env.step(action)\n","#             print(action, test_env.get_current_state(), reward, cum_reward)\n","\n","            next_gridmap_ = np.reshape([gridmap],(1,1,10,9)) # 다음 그리드를 히스토리 사이즈에 맞게 reshape\n","            next_gridmap = torch.from_numpy(next_gridmap_).float() # 텐서로 변환\n","            new_history = torch.cat([next_gridmap, history[:,:3,:,:]],dim=1) # (1,1+3,10,9)\n","            # 지금 있는 히스토리중 마지막 채널을 지우고 new를 제일 앞에 붙임\n","            history = new_history\n","\n","            if display:\n","                print('Move #%s; Taking action: %s' % (mov, action))\n","#                 print(gridmap.reshape(10,9))\n","\n","            if test_env.goal_ob_reward:\n","                wins += 1\n","                if display:\n","                    print(\"Game won! Reward: %s\" % (cum_reward))\n","                    \n","            if (mov > 250):  # 경로 길이가 너무 길어서 제외시키는 조건 추가 필요시 사용\n","                if display:\n","                    print(\"Game lost; too many moves.\")\n","                break\n","                \n","        ##### gif 생성을 위한 경로(env.actions) 저장 ####################################    \n","        f.write(str(n_epi)+'/'+str(test_env.local_target_original)+'\\n')\n","        f.write(str(test_env.actions))\n","        f.write('\\n')\n","        ######################################### \n","    f.close()\n","                \n","    win_perc = float(wins) / float(epochs)\n","    print(\"Test performed: {0}, # of wins: {1}\".format(epochs,wins))\n","    print(\"Win percentage: {}%\".format(100.0*win_perc))\n","    return win_perc"]},{"cell_type":"code","execution_count":6,"id":"1ae29afe","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"1ae29afe","executionInfo":{"status":"ok","timestamp":1654442906272,"user_tz":-540,"elapsed":355112,"user":{"displayName":"오강자","userId":"02252026995617554470"}},"outputId":"bb513eb6-4511-45fd-b182-be921247c2a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["data/factory_order_test.csv used\n","Test performed: 1226, # of wins: 1226\n","Win percentage: 100.0%\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":6}],"source":["test(epochs=1226, train_mode=False, display=False, model_load=True, PATH = PATH)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"TEST_multi_target.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
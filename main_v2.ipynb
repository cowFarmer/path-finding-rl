{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832a7f04",
   "metadata": {},
   "source": [
    "# 목표 Multi Q-table\n",
    "order box 타겟(A, B, ..., Q)에 대한 q-table을 q-learning으로 수렴시키게 만든다.   \n",
    "총 18개(A, B, ..., Q, [9,4] = start point)의 수렴된 table을 만들어야 한다.   \n",
    "order box를 얻은 경우 다음 order box를 향한다.    \n",
    "start -> A -> B -> end로 움직이는 알고리즘을 짠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde76393",
   "metadata": {},
   "source": [
    "### 바닥부터 배우는 강화학습 챕터 6 Q-learning\n",
    "책에 있는 코드를 참고해서 q-table이 제대로 수렴하는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28927ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.x = 9\n",
    "        self.y = 4\n",
    "\n",
    "    def step(self, a):\n",
    "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션 : 아래쪽\n",
    "        if a==0:\n",
    "            self.move_left()\n",
    "        elif a==1:\n",
    "            self.move_up()\n",
    "        elif a==2:\n",
    "            self.move_right()\n",
    "        elif a==3:\n",
    "            self.move_down()\n",
    "    \n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def move_left(self):\n",
    "        if self.y==0:\n",
    "            pass\n",
    "        # elif self.y==3 and self.x in [3, 4, 5, 6]:\n",
    "        #     pass\n",
    "        # elif self.y==5 and self.x in [3, 4, 5, 6]:\n",
    "        #     pass\n",
    "        # elif self.y==7 and self.x in [3, 4, 5, 6]:\n",
    "        #     pass\n",
    "        else:\n",
    "            self.y -=1\n",
    "\n",
    "    def move_right(self):\n",
    "        if self.y==9:\n",
    "            pass\n",
    "        # elif self.y==1 and self.x in [3, 4, 5, 6]:\n",
    "        #     pass\n",
    "        # elif self.y==3 and self.x in [3, 4, 5, 6]:\n",
    "        #     pass\n",
    "        # elif self.y==5 and self.x in [3, 4, 5, 6]:\n",
    "        #     pass\n",
    "        else:\n",
    "            self.y +=1\n",
    "\n",
    "    def move_up(self):\n",
    "        if self.x==0:\n",
    "            pass\n",
    "        # elif self.x==7 and self.y in [2, 4, 6]:\n",
    "        #     pass\n",
    "        else:\n",
    "            self.x -= 1\n",
    "\n",
    "    def move_down(self):\n",
    "        if self.x==10:\n",
    "            pass\n",
    "        # elif self.x==1 and self.y in [2, 4, 6]:\n",
    "        #     pass\n",
    "        else:\n",
    "            self.x+=1\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.x==5 and self.y==0: # 목표\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self): #시작 state로 돌아가는 함수\n",
    "        self.x = 9\n",
    "        self.y = 4\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((10, 9, 4))\n",
    "        self.eps = 0.5\n",
    "    \n",
    "    def select_action(self, s):\n",
    "        x, y = s\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            action_val = self.q_table[x,y,:]\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "\n",
    "    def update_table(self, transition):\n",
    "        s, a, r, s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        a_prime = self.select_action(s_prime) \n",
    "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1*(r + np.max(self.q_table[next_x, next_y, :]) - self.q_table[x,y,a])\n",
    "\n",
    "    # def anneal_eps(self):\n",
    "    #     self.eps -= 0.01 #q러닝에서는 epsilon이 더 천천히 줄어들도록 함(왜?)\n",
    "    #     self.eps = max(self.eps, 0.2) # 0.2도 더 크다(왜?)\n",
    "\n",
    "    def show_table(self):\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeros((10, 9))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "#             print(row_idx)\n",
    "            row = q_lst[row_idx]\n",
    "#             print(q_lst)\n",
    "            for col_idx in range(len(row)):\n",
    "#                 print(f\"col_idx = \\n{col_idx}\\n\")\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = QAgent()\n",
    "    episode = 10\n",
    "    action_list = ['left','up','right','down']\n",
    "    total_reward = 0\n",
    "\n",
    "    for n_epi in range(episode): #10번 에피소드\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "\n",
    "        print(f\"episode = {n_epi}\")\n",
    "        while not done: # 한 에피소드가 끝날때 까지\n",
    "            a = agent.select_action(s)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            agent.update_table((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "            total_reward += r\n",
    "\n",
    "            print(f\"a={action_list[a]}, s={s}, r={r}, total_reward={total_reward}\\n\")\n",
    "            # print(agent.q_table)\n",
    "            print(\"==================================\")\n",
    "        # agent.anneal_eps()\n",
    "    agent.show_table()\n",
    "    \n",
    "# 좌, 상, 우, 하 0 1 2 3 \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c700850",
   "metadata": {},
   "source": [
    "수렴시킨 q-table의 값 저장 작업이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11caedc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9d254",
   "metadata": {},
   "source": [
    "# 위의 수렴 과정 이후에 현재 task에 적용하기 아직 진행하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5cf9e",
   "metadata": {},
   "source": [
    "### Sim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "from draw_utils import *\n",
    "from pyglet.gl import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# reward\n",
    "# move_reward = 0.1\n",
    "# obs_reward = 0.1\n",
    "# goal_reward = 10\n",
    "\n",
    "move_reward = -1\n",
    "obs_reward = -10\n",
    "goal_reward = 50\n",
    "print('reward:' , move_reward, obs_reward, goal_reward)\n",
    "\n",
    "local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        height : 그리드 높이\n",
    "        width : 그리드 너비 \n",
    "        inds : A ~ Q alphabet list\n",
    "        '''\n",
    "        # Load train data\n",
    "        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_train.csv\"))\n",
    "        self.height = 10\n",
    "        self.width = 9\n",
    "        self.inds = list(ascii_uppercase)[:17]\n",
    "\n",
    "    def set_box(self):\n",
    "        '''\n",
    "        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n",
    "        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n",
    "        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n",
    "        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n",
    "        '''\n",
    "        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n",
    "\n",
    "        # 물건이 들어있을 수 있는 경우\n",
    "        for box in box_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100\n",
    "\n",
    "        # 물건이 실제 들어있는 경우\n",
    "        order_item = list(set(self.inds) & set(self.items))\n",
    "        order_csv = box_data[box_data['item'].isin(order_item)]\n",
    "\n",
    "        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100\n",
    "            # local target에 가야 할 위치 좌표 넣기\n",
    "            self.local_target.append(\n",
    "                [getattr(order_box, \"row\"),\n",
    "                 getattr(order_box, \"col\")]\n",
    "                )\n",
    "\n",
    "        self.local_target.sort()\n",
    "        self.local_target.append([9,4]) \n",
    "        # 처음 부터 local_target을 [8,4]로 정해도 좋을듯하다.\n",
    "\n",
    "        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n",
    "\n",
    "    def set_obstacle(self):\n",
    "        '''\n",
    "        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n",
    "        '''\n",
    "        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n",
    "        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n",
    "\n",
    "    def reset(self, epi):\n",
    "        '''\n",
    "        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n",
    "\n",
    "        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n",
    "        :return: 초기셋팅 된 그리드\n",
    "        :rtype: numpy.ndarray\n",
    "        _____________________________________________________________________________________\n",
    "        items : 이번 에피소드에서 가져와야하는 아이템들\n",
    "        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n",
    "        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n",
    "        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n",
    "        curloc : 현재 위치\n",
    "        '''\n",
    "\n",
    "        # initial episode parameter setting\n",
    "        self.epi = epi # 0~39999\n",
    "        self.items = list(self.files.iloc[self.epi])[0] # item list\n",
    "        self.cumulative_reward = 0\n",
    "        self.terminal_location = None\n",
    "        self.local_target = [] # items list\n",
    "        self.actions = []\n",
    "\n",
    "        # initial grid setting\n",
    "        # 그림\n",
    "        self.grid = np.ones((self.height, self.width), dtype=\"float16\")\n",
    "\n",
    "        # set information about the gridworld\n",
    "        # set_box()는 local_target\n",
    "        self.set_box()\n",
    "        self.set_obstacle()\n",
    "\n",
    "        # start point를 grid에 표시\n",
    "        self.curloc = [9, 4]\n",
    "        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        return self.grid\n",
    "\n",
    "    def apply_action(self, action, cur_x, cur_y):\n",
    "        '''\n",
    "        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n",
    "        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n",
    "        \n",
    "        :param x: 에이전트의 현재 x 좌표\n",
    "        :param y: 에이전트의 현재 y 좌표\n",
    "        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n",
    "        :rtype: int, int\n",
    "        '''\n",
    "        new_x = cur_x\n",
    "        new_y = cur_y\n",
    "        # up\n",
    "        if action == 0:\n",
    "            new_x = cur_x - 1\n",
    "        # down\n",
    "        elif action == 1:\n",
    "            new_x = cur_x + 1\n",
    "        # left\n",
    "        elif action == 2:\n",
    "            new_y = cur_y - 1\n",
    "        # right\n",
    "        else:\n",
    "            new_y = cur_y + 1\n",
    "\n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "\n",
    "    def get_reward(self, new_x, new_y, out_of_boundary):\n",
    "        '''\n",
    "        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n",
    "\n",
    "        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n",
    "        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n",
    "        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n",
    "        :return: action에 따른 리워드\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # 바깥으로 나가는 경우\n",
    "        if any(out_of_boundary):\n",
    "            reward = obs_reward\n",
    "                       \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 \n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                reward = obs_reward  \n",
    "\n",
    "            # 현재 목표에 도달한 경우\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "                reward = goal_reward\n",
    "\n",
    "            # 그냥 움직이는 경우 \n",
    "            else:\n",
    "                reward = move_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' \n",
    "        에이전트의 action에 따라 step을 진행한다.\n",
    "        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n",
    "        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n",
    "\n",
    "        :param action: 에이전트 행동\n",
    "        :return:\n",
    "            grid, 그리드\n",
    "            reward, 리워드\n",
    "            cumulative_reward, 누적 리워드\n",
    "            done, 종료 여부\n",
    "            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n",
    "\n",
    "        :rtype: numpy.ndarray, float, float, bool, bool/str\n",
    "\n",
    "        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n",
    "        '''\n",
    "\n",
    "        self.terminal_location = self.local_target[0]\n",
    "        cur_x,cur_y = self.curloc\n",
    "        self.actions.append((cur_x, cur_y))\n",
    "\n",
    "        goal_ob_reward = False\n",
    "        \n",
    "        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n",
    "\n",
    "        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n",
    "\n",
    "        # 바깥으로 나가는 경우 종료\n",
    "        if any(out_of_boundary):\n",
    "            self.done = True\n",
    "            goal_ob_reward = True\n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 종료\n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                self.done = True\n",
    "                goal_ob_reward = True\n",
    "\n",
    "            # 현재 목표에 도달한 경우, 다음 목표설정\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "\n",
    "                # end point 일 때\n",
    "                if [new_x, new_y] == [9,4]:\n",
    "                    self.done = True\n",
    "\n",
    "                self.local_target.remove(self.local_target[0])\n",
    "                self.grid[cur_x][cur_y] = 1\n",
    "                self.grid[new_x][new_y] = -5\n",
    "                goal_ob_reward = True\n",
    "                self.curloc = [new_x, new_y]\n",
    "            else:\n",
    "                # 그냥 움직이는 경우 \n",
    "                self.grid[cur_x][cur_y] = 1\n",
    "                self.grid[new_x][new_y] = -5\n",
    "                self.curloc = [new_x,new_y]\n",
    "                \n",
    "        reward = self.get_reward(new_x, new_y, out_of_boundary)\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        if self.done == True:\n",
    "            if [new_x, new_y] == [9, 4]:\n",
    "                if self.terminal_location == [9, 4]:\n",
    "                    # 완료되면 GIFS 저장\n",
    "                    goal_ob_reward = 'finish'\n",
    "                    height = 10\n",
    "                    width = 9 \n",
    "                    display = Display(visible=False, size=(width, height))\n",
    "                    display.start()\n",
    "\n",
    "                    start_point = (9, 4)\n",
    "                    unit = 50\n",
    "                    screen_height = height * unit\n",
    "                    screen_width = width * unit\n",
    "                    log_path = \"./logs\"\n",
    "                    data_path = \"./data\"\n",
    "                    render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "                    for idx, new_pos in enumerate(self.actions):\n",
    "                        render_cls.update_movement(new_pos, idx+1)\n",
    "                    \n",
    "                    render_cls.save_gif(self.epi, '_성공')\n",
    "                    render_cls.viewer.close()\n",
    "                    display.stop()\n",
    "            else:     \n",
    "                # 완료되면 GIFS 저장\n",
    "                goal_ob_reward = 'finish'\n",
    "                height = 10\n",
    "                width = 9 \n",
    "                display = Display(visible=False, size=(width, height))\n",
    "                display.start()\n",
    "\n",
    "                start_point = (9, 4)\n",
    "                unit = 50\n",
    "                screen_height = height * unit\n",
    "                screen_width = width * unit\n",
    "                log_path = \"./logs\"\n",
    "                data_path = \"./data\"\n",
    "                render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "                for idx, new_pos in enumerate(self.actions):\n",
    "                    render_cls.update_movement(new_pos, idx+1)\n",
    "\n",
    "                render_cls.save_gif(self.epi, '_실패')\n",
    "                render_cls.viewer.close()\n",
    "                display.stop()\n",
    "        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sim = Simulator()\n",
    "    files = pd.read_csv(\"./data/factory_order_train.csv\")\n",
    "   \n",
    "    for epi in range(2): # len(files)):\n",
    "        items = list(files.iloc[epi])[0]\n",
    "        done = False\n",
    "        i = 0\n",
    "        obs = sim.reset(epi)\n",
    "        actions = [0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1]\n",
    "\n",
    "        while done == False:\n",
    "            \n",
    "            i += 1\n",
    "            next_obs, reward, cumul ,done, goal_reward = sim.step(actions[i])\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if (done == True) or (i == (len(actions)-1)):\n",
    "                i =0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24966fcb",
   "metadata": {},
   "source": [
    "### policy, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from Sim import Simulator\n",
    "\n",
    "class Policy():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        pass\n",
    "    \n",
    "#     def predict_pi(self):\n",
    "#         pass\n",
    "    \n",
    "#     def predict_value(self):\n",
    "#         pass\n",
    "    \n",
    "#     def train_net(self):\n",
    "#         pass\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        coin = random.randint(0,3)\n",
    "        return coin\n",
    "\n",
    "# off-policy q-learning\n",
    "class QLearning(Policy):\n",
    "    def __init__(self):\n",
    "#         self.q_table = np.zeros((sim.height, sim.width, 4), dtype=\"float16\")\n",
    "        self.q_table = np.zeros((10, 9, 4), dtype=\"float16\")\n",
    "        self.eps = 0.9\n",
    "        self.alpha = 0.01\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        x, y = state\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            action_val = self.q_table[x,y,:]\n",
    "            action = np.argmax(action_val)\n",
    "            print(f\"policy action = {action}\")\n",
    "        return action\n",
    "    \n",
    "    def update_table(self, history):\n",
    "        s,a,r,s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        # Q-learning update\n",
    "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n",
    "        \n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.01 # epsilon\n",
    "        self.eps = max(self.eps, 0.2)\n",
    "        \n",
    "    def show_table(self):\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeors((10,9,4))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "            row = q_lst[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        print(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15314d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Policy import *\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.target_list=None\n",
    "        self.policy = None\n",
    "        \n",
    "    def set_target_list(self, local_target):\n",
    "        self.target_list = local_target\n",
    "        print(\"Agent's local target = {}\".format(local_target))\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def update_target(self):\n",
    "        pass\n",
    "    \n",
    "    # 현재 state를 받고 policy의 action하기\n",
    "    # action_list = ['up','down','left','right'] = 0,1,2,3\n",
    "    def select_action(self, state):\n",
    "        # 진열대 array rack(좌 상 우, 세 종류) 만들어서 isin으로 체크하고 위치에 따라 행동 return 만들기\n",
    "        left_rack = [5,0], [4,0], [3,0], [2,0]\n",
    "        top_rack = [0,0], [0,1], [0,2], [0,3], [0,4], [0,5], [0,6], [0,7], [0,8]\n",
    "        right_rack = [2,8], [3,8], [4,8], [5,8]\n",
    "        if state == [9,4]:\n",
    "            return 0 # up\n",
    "        elif state in top_rack:\n",
    "            return 1 # down\n",
    "        elif state in right_rack:\n",
    "            return 2 # left\n",
    "        elif state in left_rack:\n",
    "            return 3 # right\n",
    "        else:\n",
    "            action = self.policy.select_action(state)\n",
    "            return action\n",
    "    \n",
    "    def set_policy(self, mode):\n",
    "        if mode==\"random\":\n",
    "            self.policy = RandomPolicy()\n",
    "        elif mode==\"q_learning\":\n",
    "            self.policy = QLearning()\n",
    "        else:\n",
    "            print(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d8b9c",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ee3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Sim import Simulator\n",
    "from Agent import Agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = pd.read_csv(\"./data/factory_order_train.csv\")\n",
    "    \n",
    "    sim = Simulator()\n",
    "    agent = Agent()\n",
    "    agent.set_policy('random')\n",
    "   \n",
    "    for epi in range(5): # len(files)):\n",
    "        # env 초기화\n",
    "        obs = sim.reset(epi)\n",
    "        print('\\n\\nenv 초기화 완료, {}번째 에피소드 시작'.format(epi))\n",
    "        \n",
    "        # agent에게 local_target 주기\n",
    "        items = list(files.iloc[epi])[0]\n",
    "        agent.set_target_list(items)\n",
    "        done = False\n",
    "        \n",
    "        while done == False:\n",
    "            # agent 랜덤 행동 취하기\n",
    "            action = agent.select_action()\n",
    "            print(action)\n",
    "            next_obs, reward, cumul ,done, goal_reward = sim.step(action)\n",
    "            print('done', done)\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ad016",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957de68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7ac61",
   "metadata": {},
   "source": [
    "### 바닥부터 배우는 강화학습 챕터 6 Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a64465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97f562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(): # 그리드월드 클래스 생성\n",
    "    def __init__(self): # 초기 변수 x, y (그리드 월드 시작 state)\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "    def step(self, a):\n",
    "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션 : 아래쪽\n",
    "        if a==0:\n",
    "            self.move_left() # 각 이동함수를 통해서 현재 state의 좌표를 변환한다.\n",
    "        elif a==1:\n",
    "            self.move_up()\n",
    "        elif a==2:\n",
    "            self.move_right()\n",
    "        elif a==3:\n",
    "            self.move_down()\n",
    "    \n",
    "        reward = -1 #보상은 1로 고정 (MDP를 모르므로 실제 관측하기 전까지는 모른다고 가정)\n",
    "        done = self.is_done() # done을 불리언으로 판정 terminal state 판단을 위해\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def move_left(self):\n",
    "        if self.y==0: #벽(테두리)지정\n",
    "            pass\n",
    "        elif self.y==3 and self.x in [0, 1, 2]: #장애물도 마찬가지로 지정해준다.\n",
    "            pass\n",
    "        elif self.y==5 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        else:\n",
    "            self.y -=1\n",
    "\n",
    "    def move_right(self):\n",
    "        if self.y==1 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y==3 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        elif self.y==6:\n",
    "            pass\n",
    "        else:\n",
    "            self.y +=1\n",
    "\n",
    "    def move_up(self):\n",
    "        if self.x==0:\n",
    "            pass\n",
    "        elif self.x==3 and self.y==2:\n",
    "            pass\n",
    "        else:\n",
    "            self.x -= 1\n",
    "\n",
    "    def move_down(self):\n",
    "        if self.x==4:\n",
    "            pass\n",
    "        elif self.x==1 and self.y==4:\n",
    "            pass\n",
    "        else:\n",
    "            self.x+=1\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.x==4 and self.y==6: # 목표 state인 (4, 6)에 도달하면 끝난다.\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self): #시작 state로 돌아가는 함수\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f738500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((5, 7, 4)) #q밸류를 저장하는 변수, 모두 0이고 3차원임\n",
    "        self.eps = 0.9 \n",
    "    \n",
    "    def select_action(self, s):\n",
    "        # 엡실론-그리디로 액션 선택\n",
    "        x, y = s\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            print(self.q_table[x,y,:])\n",
    "            action_val = self.q_table[x,y,:]\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "\n",
    "    def update_table(self, transition):\n",
    "        s, a, r , s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        a_prime = self.select_action(s_prime) \n",
    "        #SARSA대신 Q러닝 업데이트 식을 이용\n",
    "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1*(r + np.max(self.q_table[next_x, next_y, :]) - self.q_table[x,y,a])\n",
    "        #공식에 따라서 SARSA에서는 그냥 다음 액션의 Q_table을 가져오던 것에서 다음 Q_table의 max를 가져오는것으로 바뀜\n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.01 #q러닝에서는 epsilon이 더 천천히 줄어들도록 함(왜?)\n",
    "        self.eps = max(self.eps, 0.2) # 0.2도 더 크다(왜?)\n",
    "\n",
    "    def show_table(self):\n",
    "        q_lst = self.q_table.tolist()\n",
    "        print(self.q_table)\n",
    "        data = np.zeros((5, 7))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "#             print(row_idx)\n",
    "            row = q_lst[row_idx]\n",
    "#             print(q_lst)\n",
    "            for col_idx in range(len(row)):\n",
    "#                 print(f\"col_idx = \\n{col_idx}\\n\")\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "951e830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-14.93967852 -14.99143922 -13.99999999 -13.99999999]\n",
      "  [-14.44717042 -13.79653821 -13.82424545 -13.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ -6.93439226  -6.93736874  -6.82738185  -6.83894806]\n",
      "  [ -6.08550143  -6.24658731  -5.98089047  -5.98016746]\n",
      "  [ -5.79734961  -5.29701372  -4.99502375  -4.99482916]\n",
      "  [ -4.60161566  -4.19239284  -4.11643606  -3.99970726]]\n",
      "\n",
      " [[-13.94981503 -14.94345962 -12.99999999 -12.99999999]\n",
      "  [-13.74811209 -13.90794335 -12.89308869 -12.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ -6.99487715  -7.68089704  -6.          -7.99149948]\n",
      "  [ -6.99029617  -6.94796811  -5.          -5.99268867]\n",
      "  [ -5.99427936  -5.94492815  -4.          -4.        ]\n",
      "  [ -4.6964982   -4.59569735  -3.79804845  -3.        ]]\n",
      "\n",
      " [[-12.87278989 -13.84571914 -12.         -12.        ]\n",
      "  [-12.94861796 -12.95377784 -11.98798441 -11.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ -7.99378792  -7.          -7.99928517  -8.99163165]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ -3.98345421  -4.9651486   -3.          -3.        ]\n",
      "  [ -3.91875209  -3.95182609  -2.95924179  -2.        ]]\n",
      "\n",
      " [[-11.8029901  -12.33677465 -11.         -12.06395037]\n",
      "  [-11.96891254 -11.98927218 -10.         -11.90249589]\n",
      "  [-10.99515693  -9.98652219  -9.         -10.92472698]\n",
      "  [ -9.9757363   -8.          -8.9927981   -9.9868314 ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ -2.91997811  -3.90399438  -2.          -2.        ]\n",
      "  [ -2.96896841  -2.97122073  -1.98874157  -1.        ]]\n",
      "\n",
      " [[-11.50688132 -11.46977898 -11.50793472 -11.5210711 ]\n",
      "  [-10.97325137 -10.95617447 -10.9575582  -11.19202441]\n",
      "  [-10.41811848  -9.99277953  -9.9931667  -10.45757816]\n",
      "  [-10.31892534  -8.99995104  -9.40827532  -9.47893069]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [ -1.72356153  -2.57865297  -1.          -1.70246648]\n",
      "  [  0.           0.           0.           0.        ]]]\n",
      "[[3. 3. 0. 2. 3. 3. 3.]\n",
      " [3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 1. 0. 3. 3.]\n",
      " [2. 2. 2. 1. 0. 3. 3.]\n",
      " [1. 1. 1. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = QAgent()\n",
    "    episode = 1000\n",
    "\n",
    "    for n_epi in range(episode): #1000번 에피소드\n",
    "        done = False\n",
    "\n",
    "        s = env.reset()\n",
    "        while not done: # 한 에피소드가 끝날때 까지\n",
    "            a = agent.select_action(s)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            agent.update_table((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "        agent.anneal_eps()\n",
    "\n",
    "    agent.show_table()\n",
    "    \n",
    "# 좌, 상, 우, 하\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa22f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 1. 0. 3. 3.]\n",
      " [2. 2. 2. 1. 0. 3. 3.]\n",
      " [3. 1. 2. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
